{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita2002/LLMS/blob/main/Practice_Session_04_Student_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49823642",
      "metadata": {
        "id": "49823642"
      },
      "source": [
        "# PS 04: Word Representation II (Statistical Methods)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this practice session, you will be able to:\n",
        "\n",
        "1. **Construct one-hot vectors** and reason about **sparsity** and **dimensionality**.\n",
        "2. **Compare cosine similarity** and understand how it captures vector similarity.\n",
        "3. **Review TF–IDF & LSA** and relate these to dense word embeddings.\n",
        "4. **Train Word2Vec** models (Skip-gram & CBOW) with **Gensim** on a toy corpus.\n",
        "5. **Visualize embeddings** with **t-SNE** and interpret clusters.\n",
        "6. **Use embeddings for analogies** (e.g., *king – man + woman ≈ queen*).\n",
        "7. **Implement a tiny GloVe** optimizer on global co-occurrence statistics.\n",
        "8. **Compare neighbors across models** (Skip-gram, CBOW, GloVe) and discuss differences.\n",
        "\n",
        "---\n",
        "\n",
        "## Practice Tasks Overview\n",
        "\n",
        "This session contains **2 main tasks**, each with two subtasks:\n",
        "\n",
        "- **Task 1**\n",
        "  - **1.1**: *Manual One-Hot Construction (+ Pairwise Similarities)* — Build sentence one-hot vectors by hand from a tiny vocabulary.\n",
        "  - **1.2**: *Neighbors & Analogies (Skip-gram)* — Top-5 similar words for a target using Word2Vec (Skip-gram), and solve *king – man + woman = ?* with Skip-gram embeddings.\n",
        "\n",
        "- **Task 2**\n",
        "  - **2.1**: *CBOW Neighbors **and** CBOW vs Skip-gram Comparison* — Compare neighbors for example target words and discuss coherence.\n",
        "  - **2.2**: *Optimize GloVe Hyperparameters to Hit Expected Neighbors* — Tune GloVe so that, for each target word, at least one word from an expected set appears in its top-k neighbors.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gensim"
      ],
      "metadata": {
        "id": "wNGbsCrrs73V"
      },
      "id": "wNGbsCrrs73V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83f222bb",
      "metadata": {
        "id": "83f222bb"
      },
      "source": [
        "## 1. One-Hot Encoding and Cosine Similarity\n",
        "\n",
        "### One-Hot Example (queen vs. princess)\n",
        "Let a tiny vocabulary be  \n",
        "$\\text{V} = [\\text{king},\\, \\text{queen},\\, \\text{man},\\, \\text{woman},\\, \\text{princess}]$.\n",
        "\n",
        "Then the one-hot vectors are:\n",
        "$$\n",
        "\\mathbf{e}_{\\text{queen}} = [0,\\,1,\\,0,\\,0,\\,0],\\qquad\n",
        "\\mathbf{e}_{\\text{princess}} = [0,\\,0,\\,0,\\,0,\\,1].\n",
        "$$\n",
        "They are **orthogonal**: $\\mathbf{e}_{\\text{queen}}^\\top \\mathbf{e}_{\\text{princess}} = 0$.\n",
        "\n",
        "### Why One-Hot is Limited\n",
        "- Cannot generalize: \"queen\" and \"princess\" are orthogonal.\n",
        "- Distance does not reflect semantic similarity.\n",
        "- Motivation for **dense** representations (embeddings).\n",
        "\n",
        "### Cosine Similarity on Simple Vectors\n",
        "- Cosine: invariant to scaling and good for semantic direction.\n",
        "\n",
        "**Formula**\n",
        "$$\n",
        "\\text{Cosine}(\\mathbf{a},\\mathbf{b}) \\;=\\;\n",
        "\\frac{\\mathbf{a}^\\top \\mathbf{b}}{\\lVert \\mathbf{a} \\rVert_2 \\, \\lVert \\mathbf{b} \\rVert_2}\n",
        "$$\n",
        "\n",
        "**For the one-hot example above**  \n",
        "$$\n",
        "\\text{Cosine}(\\mathbf{e}_{\\text{queen}}, \\mathbf{e}_{\\text{princess}}) = 0\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4970b77f",
      "metadata": {
        "id": "4970b77f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Tiny toy corpus (3 short sentences)\n",
        "tiny_corpus_sentences = [\n",
        "    \"king and queen rule the kingdom\",\n",
        "    \"the man and the woman walk\",\n",
        "    \"the river flows by the old kingdom\"\n",
        "]\n",
        "\n",
        "# Tokenization & Vocabulary\n",
        "def tokenize_words(text: str):\n",
        "    \"\"\"Lowercase, whitespace-split tokenization (toy; no punctuation handling).\"\"\"\n",
        "    return [w.lower() for w in text.split()]\n",
        "\n",
        "all_tokens = [token for sent in tiny_corpus_sentences for token in tokenize_words(sent)]\n",
        "vocabulary = sorted(set(all_tokens))\n",
        "token_to_index = {token: idx for idx, token in enumerate(vocabulary)}\n",
        "index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
        "\n",
        "print(\"Vocabulary (sorted):\")\n",
        "print(\" \", vocabulary)\n",
        "print(f\"→ Vocabulary size: {len(vocabulary)} token types\\n\")\n",
        "\n",
        "print(\"Token → Index mapping:\")\n",
        "for i, (tok, idx) in enumerate(token_to_index.items()):\n",
        "    print(f\"  {tok:>10} → {idx}\")\n",
        "print()\n",
        "\n",
        "# One-hot encoders\n",
        "def one_hot_word_vector(word: str, token_to_index: dict) -> np.ndarray:\n",
        "    \"\"\"Return a one-hot vector for a single word using the given vocabulary mapping.\"\"\"\n",
        "    vec = np.zeros(len(token_to_index), dtype=int)\n",
        "    if word in token_to_index:\n",
        "        word_index = token_to_index[word]\n",
        "        vec[word_index] = 1\n",
        "    return vec\n",
        "\n",
        "def one_hot_sentence_union_vector(sentence: str, token_to_index: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return a sentence-level one-hot vector (set/union).\n",
        "    A position is 1 if the corresponding token appears at least once in the sentence.\n",
        "    \"\"\"\n",
        "    vec = np.zeros(len(token_to_index), dtype=int)\n",
        "    for word in tokenize_words(sentence):\n",
        "        if word in token_to_index:\n",
        "            word_index = token_to_index[word]\n",
        "            vec[word_index] = 1\n",
        "    return vec\n",
        "\n",
        "# Demonstration: word-level one-hots\n",
        "print(\"Example one-hot vectors (word-level):\")\n",
        "for word in [\"king\", \"queen\", \"river\", \"cat\"]:  # 'cat' is intentionally OOV\n",
        "    vec = one_hot_word_vector(word, token_to_index)\n",
        "    status = \"\" if word in token_to_index else \" (OOV — out of vocabulary)\"\n",
        "    print(f\"     vector: {vec.tolist()}\")\n",
        "print()\n",
        "\n",
        "# Demonstration: sentence-level one-hots (union)\n",
        "print(\"Sentence-level one-hot vectors (union of tokens in sentence):\")\n",
        "for i, sentence in enumerate(tiny_corpus_sentences):\n",
        "    vec = one_hot_sentence_union_vector(sentence, token_to_index)\n",
        "    print(f\"  S{i}: '{sentence}'\")\n",
        "    print(f\"     vector: {vec.tolist()}\")\n",
        "print()\n",
        "\n",
        "# Cosine similarity demo\n",
        "A_vec = np.array([1.0, 1.0])\n",
        "B_vec = np.array([2.0, 2.0])  # Same direction as A, larger magnitude\n",
        "C_vec = np.array([2.0, 0.0])  # Different direction\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"Cosine similarity between vectors a and b.\"\"\"\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "    return float(a @ b) / denom if denom > 0 else 0.0\n",
        "\n",
        "print(\"Cosine similarity on simple vectors:\")\n",
        "print(f\"  A = {A_vec.tolist()}, B = {B_vec.tolist()}, C = {C_vec.tolist()}\")\n",
        "print(f\"  cos(A, B) = {cosine_similarity(A_vec, B_vec):.4f}\")\n",
        "print(f\"  cos(A, C) = {cosine_similarity(A_vec, C_vec):.4f}\")\n",
        "print(f\"  cos(B, C) = {cosine_similarity(B_vec, C_vec):.4f}\")\n",
        "print(\"  → Cosine judges A and B as identical in direction.\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9137370",
      "metadata": {
        "id": "a9137370"
      },
      "source": [
        "## 2. Shared Toy Corpus (for Skip-gram, CBOW, and GloVe)\n",
        "\n",
        "We’ll use a small, curated corpus that mixes several topical clusters so the embeddings have enough signal to learn simple relationships:\n",
        "\n",
        "- **Royalty & gender**: e.g., *king, queen, prince, princess, palace, kingdom*  \n",
        "- **Tech**: e.g., *computer, software, data, algorithms, models*  \n",
        "- **Emotions**: e.g., *happy, sad, joyful, cheerful, angry*  \n",
        "- **Nature / water**: e.g., *river, valley, bank, bridge, boats*\n",
        "\n",
        "### What this section does\n",
        "1. **Tokenize** each sentence with a simple lowercase + whitespace split.\n",
        "2. **Build the vocabulary** from all tokens.\n",
        "\n",
        "### Variables defined (used by later sections)\n",
        "- raw_corpus → the list of corpus sentences (List[str])  \n",
        "- tokenized_corpus → the tokenized sentences (List[List[str]])  \n",
        "- vocab_all → the sorted list of unique tokens in the corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f74c2e2f",
      "metadata": {
        "id": "f74c2e2f"
      },
      "outputs": [],
      "source": [
        "# Shared Toy Corpus for Skip-gram, CBOW, and GloVe\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Corpus sentences (grouped by theme)\n",
        "toy_corpus_sentences = [\n",
        "    # royalty / gender\n",
        "    \"king rules the kingdom\",\n",
        "    \"queen rules the kingdom\",\n",
        "    \"the king is a man\",\n",
        "    \"the queen is a woman\",\n",
        "    \"a princess and a prince live in the palace\",\n",
        "    \"a man and a woman walk to the palace\",\n",
        "    \"the royal family visits the city\",\n",
        "    \"the kingdom has a strong army\",\n",
        "    # tech\n",
        "    \"a computer runs software and processes data\",\n",
        "    \"programmers write code in a computer laboratory\",\n",
        "    \"deep learning models train on data\",\n",
        "    \"algorithms improve computer performance\",\n",
        "    \"hardware and software form a computer system\",\n",
        "    \"laptops and desktops are kinds of computer\",\n",
        "    # emotions\n",
        "    \"he feels happy and joyful today\",\n",
        "    \"she is very happy with the results\",\n",
        "    \"the movie made everyone sad\",\n",
        "    \"a cheerful smile can make people happy\",\n",
        "    \"angry voices faded after a happy resolution\",\n",
        "    # nature / water\n",
        "    \"the river flows through the valley\",\n",
        "    \"boats travel along the river\",\n",
        "    \"fish live in the river and lake\",\n",
        "    \"the river bank is covered with trees\",\n",
        "    \"a bridge crosses the wide river\"\n",
        "]\n",
        "\n",
        "# Tokenization\n",
        "def tokenize_simple(text: str):\n",
        "    \"\"\"Lowercase + whitespace-split.\"\"\"\n",
        "    return [w.lower() for w in text.split()]\n",
        "\n",
        "tokenized_sentences = [tokenize_simple(s) for s in toy_corpus_sentences]\n",
        "\n",
        "# Vocabulary\n",
        "all_tokens_flat = [tok for sent in tokenized_sentences for tok in sent]\n",
        "vocabulary_all = sorted(set(all_tokens_flat))\n",
        "word_to_index = {w: i for i, w in enumerate(vocabulary_all)}\n",
        "index_to_word = {i: w for w, i in word_to_index.items()}\n",
        "\n",
        "# Statistics\n",
        "num_sents = len(tokenized_sentences)\n",
        "vocab_size = len(vocabulary_all)\n",
        "sent_lengths = [len(s) for s in tokenized_sentences]\n",
        "avg_len = float(np.mean(sent_lengths)) if sent_lengths else 0.0\n",
        "\n",
        "print(f\"\\nToy corpus prepared:\")\n",
        "print(f\"  • #sentences = {num_sents}\")\n",
        "print(f\"  • vocab size = {vocab_size}\")\n",
        "print(f\"  • avg tokens per sentence ≈ {avg_len:.2f}\")\n",
        "print()\n",
        "\n",
        "# Peek at a few sentences with their tokens\n",
        "print(\"Sample sentences (with tokens):\")\n",
        "for i in range(min(3, num_sents)):\n",
        "    print(f\"  S{i}: '{toy_corpus_sentences[i]}'\")\n",
        "    print(f\"     tokens: {tokenized_sentences[i]}\")\n",
        "print()\n",
        "\n",
        "# Peek at a few vocabulary terms and mappings\n",
        "print(\"Sample vocabulary terms (first 10):\")\n",
        "print(\" \", vocabulary_all[:10])\n",
        "print(\"Sample word→index pairs (first 10):\")\n",
        "for i, (w, idx) in enumerate(word_to_index.items()):\n",
        "    if i >= 10:\n",
        "        print(\"   ...\")\n",
        "        break\n",
        "    print(f\"   {w:>12} → {idx}\")\n",
        "print()\n",
        "\n",
        "# Variable names (to stay aligned with later cells)\n",
        "raw_corpus = toy_corpus_sentences\n",
        "tokenized_corpus = tokenized_sentences\n",
        "vocab_all = vocabulary_all\n",
        "\n",
        "print(f\"\\nToy corpus ready: {len(tokenized_corpus)} sentences | Vocab size: {len(vocab_all)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Word2Vec (Skip-gram) — PyTorch on the Toy Corpus\n",
        "\n",
        "We train a **Skip-gram** model (with **naïve softmax**) on the **shared toy corpus** prepared earlier.\n",
        "\n",
        "**Corpus variables used (from the previous cell):**\n",
        "- `tokenized_corpus` (list of token lists)\n",
        "- `word_to_index` (w→i)\n",
        "- `index_to_word` (i→w)\n",
        "\n",
        "---\n",
        "\n",
        "### Data preparation (with few examples)\n",
        "\n",
        "We use a symmetric window of size **$W$** on each side.\n",
        "\n",
        "- **Robust (used in code below):** the window is **clipped at sentence boundaries**.  \n",
        "  For a sentence of length $n$, and a target word at position $p$ (token $t$), its **context indices** are\n",
        "  $$\n",
        "  \\{\\max(0,p\\!-\\!W),\\ldots,p\\!-\\!1\\}\\;\\cup\\;\\{p\\!+\\!1,\\ldots,\\min(n\\!-\\!1,p\\!+\\!W)\\}.\n",
        "  $$\n",
        "  We emit one training pair **$(t \\rightarrow c)$** for **each** context token $c$ in that set.\n",
        "\n",
        "- **Strict (HW style):** require a **full** window of exactly $W$ on **both** sides; otherwise **skip** the position.\n",
        "\n",
        "Each eligible target produces up to $2W$ pairs $(\\text{target } t \\rightarrow \\text{context } c)$.\n",
        "\n",
        "**Example (from the provided corpus, $W=2$):**\n",
        "\n",
        "Sentence: “**a bridge crosses the wide river**”  \n",
        "Tokens: $[a,\\; bridge,\\; crosses,\\; the,\\; wide,\\; river]$\n",
        "\n",
        "- Target $t=$ “bridge” at position $p=1$  \n",
        "  Context window (clipped): positions $\\{0\\}\\cup\\{2,3\\}$ → contexts $c \\in \\{a,\\; crosses,\\; the\\}$  \n",
        "\n",
        "  **Strict:** **skip** (because we don’t have $W=2$ tokens on the left)\n",
        "\n",
        "  Emitted pairs for robust mode: $(\\text{bridge}\\!\\to\\!a),\\;(\\text{bridge}\\!\\to\\!\\text{crosses}),\\;(\\text{bridge}\\!\\to\\!\\text{the})$\n",
        "\n",
        "- Target $t=$ “wide” at position $p=4$  \n",
        "  Context window (clipped): positions $\\{2,3\\}\\cup\\{5\\}$ → contexts $c \\in \\{\\text{crosses},\\;\\text{the},\\;\\text{river}\\}$  \n",
        "\n",
        "  **Strict:** **skip** (because we don’t have $W=2$ tokens on the right)\n",
        "\n",
        "  Emitted pairs for robust mode: $(\\text{wide}\\!\\to\\!\\text{crosses}),\\;(\\text{wide}\\!\\to\\!\\text{the}),\\;(\\text{wide}\\!\\to\\!\\text{river})$\n",
        "\n",
        "(These words are mapped to integer ids via `word_to_index` before training.)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5Aoq-Yx38F63"
      },
      "id": "5Aoq-Yx38F63"
    },
    {
      "cell_type": "markdown",
      "id": "a22c1bf1",
      "metadata": {
        "id": "a22c1bf1"
      },
      "source": [
        "![Skip-gram Example](https://maelfabien.github.io/assets/images/skp_gr.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Notation (using $t$=target, $c$=context)\n",
        "\n",
        "Let $|V|$ be the vocabulary size and $d$ the embedding dimension.\n",
        "\n",
        "- $W_{\\text{in}} \\in \\mathbb{R}^{|V|\\times d}$: **target/center** embedding table; for word index $t$,\n",
        "  $\\;\\mathbf{v}_t = W_{\\text{in}}[t,:]$.\n",
        "- $W_{\\text{out}} \\in \\mathbb{R}^{|V|\\times d}$: **context** embedding table; for word index $c$,\n",
        "  $\\;\\mathbf{u}_c = W_{\\text{out}}[c,:]$.\n",
        "- $b_{\\text{out}} \\in \\mathbb{R}^{|V|}$: output bias vector; component $b_{\\text{out},j}$ for class $j$.\n",
        "\n",
        "---\n",
        "\n",
        "### Model (naïve softmax)\n",
        "\n",
        "Given a **target** index $t$ (selects $\\mathbf{v}_t$ from $W_{\\text{in}}$), we score **every** vocabulary item and predict **one** context index $c$:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = W_{\\text{out}}\\,\\mathbf{v}_t + b_{\\text{out}},\n",
        "\\qquad\n",
        "\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{z}),\n",
        "\\quad\n",
        "\\hat{y}_j = \\frac{e^{z_j}}{\\sum_{k=1}^{|V|} e^{z_k}} .\n",
        "$$\n",
        "\n",
        "**Cross-entropy loss (target $\\to$ context)** with a one-hot target $\\mathbf{y}$ for the true context $c$:\n",
        "\n",
        "$$\n",
        "\\boxed{\\;{L}(t,c) = -\\log \\hat{y}_c\\;}\n",
        "$$\n",
        "\n",
        "**Equivalent log-sum-exp form** (expanding the softmax):\n",
        "\n",
        "$$\n",
        "\\boxed{\\;\n",
        "{L}(t,c)\n",
        "= -\\big(\\mathbf{u}_c^\\top \\mathbf{v}_t + b_{\\text{out},c}\\big)\n",
        "\\;+\\; \\log \\sum_{j=1}^{|V|} \\exp\\!\\big(\\mathbf{u}_j^\\top \\mathbf{v}_t + b_{\\text{out},j}\\big)\n",
        "\\;}\n",
        "$$\n",
        "\n",
        "> **Efficiency note:** In this demo we use **naïve softmax** (summing over the **entire vocabulary**). In practice, **negative sampling** replaces the full softmax with $1$ positive $+\\;K$ negatives to avoid the $\\text{O}(|V|)$ normalization.\n",
        "\n",
        "---\n",
        "\n",
        "### Training setup (this demo)\n",
        "\n",
        "- **Embedding dim** $d$: 50  \n",
        "- **Window**: 5 (words on each side)  \n",
        "- **Batch size**: 8  \n",
        "- **Epochs**: 20  \n",
        "- **Optimizer**: Adam (learning rate $= 0.05$)  \n",
        "- **Loss**: `nn.CrossEntropyLoss()` (applies softmax + CE)\n",
        "\n",
        "---\n",
        "\n",
        "### After training\n",
        "\n",
        "- Keep $W_{\\text{in}}$ as the word embeddings (often discard/ignore $W_{\\text{out}}$ downstream).\n",
        "\n",
        "- We use the **input embeddings** $W_{\\text{in}}$ as word vectors.\n",
        "\n",
        "\n",
        "- To embed a word (token) with index \\(w\\), its embedding is\n",
        "  $$\n",
        "  \\mathbf{v}_w = W_{\\text{in}}[w,:] \\in \\mathbb{R}^d.\n",
        "  $$\n",
        "\n",
        "- To embed a sentence, let a sentence be a sequence of word (token) indices\n",
        "  $$\n",
        "  S = (w_1, w_2, \\dots, w_m).\n",
        "  $$\n",
        "\n",
        "  We define its embedding as the **mean of its word embeddings**:\n",
        "  $$\n",
        "  \\mathrm{embed}(S)\n",
        "  \\;=\\; \\frac{1}{m} \\sum_{r=1}^{m} \\mathbf{v}_{w_r}\n",
        "  \\;=\\; \\frac{1}{m} \\sum_{r=1}^{m} W_{\\text{in}}[w_r,:].\n",
        "  $$\n",
        "\n",
        "- Nearest neighbors are computed with **cosine similarity** on rows of $W_{\\text{in}}$:\n",
        "\n",
        "$$\n",
        "\\cos(\\mathbf{v}_a,\\mathbf{v}_b) \\;=\\;\n",
        "\\frac{\\mathbf{v}_a^\\top \\mathbf{v}_b}{\\lVert \\mathbf{v}_a\\rVert\\,\\lVert \\mathbf{v}_b\\rVert}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "x-CTcj3SUDei"
      },
      "id": "x-CTcj3SUDei"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Skip-gram (naïve softmax) — PyTorch on toy corpus\n",
        "# ================================\n",
        "import numpy as np\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -----------------------------\n",
        "# Reproducibility & device\n",
        "# -----------------------------\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Skip-gram data maker (robust windowing by handling boundaries via clipping)\n",
        "# -----------------------------\n",
        "def skipgram_pairs_with_edges(tokenized: List[List[str]],\n",
        "                              w2i: Dict[str,int],\n",
        "                              window: int = 2\n",
        "                             ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Create Skip-gram pairs with a symmetric window of size `window`,\n",
        "    **clipping the window to sentence boundaries** (handles edges).\n",
        "\n",
        "    For each position c, we take context indices in:\n",
        "        [max(0, c-window) ... c-1]  ∪  [c+1 ... min(n-1, c+window)]\n",
        "\n",
        "    Returns:\n",
        "        centers : (N,) np.int64\n",
        "        contexts: (N,) np.int64\n",
        "    \"\"\"\n",
        "    centers, contexts = [], []\n",
        "    W = window\n",
        "    for sent in tokenized:\n",
        "        idxs = [w2i[w] for w in sent if w in w2i]\n",
        "        n = len(idxs)\n",
        "        for c in range(n):\n",
        "            left  = max(0, c - W)\n",
        "            right = min(n, c + W + 1)\n",
        "            for j in range(left, right):\n",
        "                if j == c:\n",
        "                    continue\n",
        "                centers.append(idxs[c])\n",
        "                contexts.append(idxs[j])\n",
        "    return np.array(centers, dtype=np.int64), np.array(contexts, dtype=np.int64)\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset\n",
        "# -----------------------------\n",
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, centers: np.ndarray, contexts: np.ndarray):\n",
        "        self.centers  = centers\n",
        "        self.contexts = contexts\n",
        "    def __len__(self) -> int:\n",
        "        return self.centers.shape[0]\n",
        "    def __getitem__(self, idx: int):\n",
        "        x = torch.tensor(self.centers[idx],  dtype=torch.long)\n",
        "        y = torch.tensor(self.contexts[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# -----------------------------\n",
        "# Model (Embedding -> Linear -> CE)\n",
        "# -----------------------------\n",
        "class SkipGramTorch(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.in_embed = nn.Embedding(vocab_size, dim)  # W_in: (|V|, d)\n",
        "        self.out      = nn.Linear(dim, vocab_size)     # logits: (d -> |V|)\n",
        "        # Small Gaussian init, like CBOW task\n",
        "        nn.init.normal_(self.in_embed.weight, mean=0.0, std=0.01)\n",
        "        nn.init.normal_(self.out.weight,      mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.out.bias)\n",
        "\n",
        "    def forward(self, center_idx: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        center_idx: (B,) indices of center words\n",
        "        returns:\n",
        "            logits: (B, |V|)\n",
        "        \"\"\"\n",
        "        v_c = self.in_embed(center_idx)  # (B, d)\n",
        "        logits = self.out(v_c)           # (B, |V|)\n",
        "        return logits\n",
        "\n",
        "@dataclass\n",
        "class SkipGramModelToy:\n",
        "    \"\"\"Light wrapper to carry the trained model & metadata.\"\"\"\n",
        "    w2i: Dict[str,int]\n",
        "    i2w: Dict[int,str]\n",
        "    model: nn.Module\n",
        "    dim: int\n",
        "    window: int\n",
        "\n",
        "# -----------------------------\n",
        "# Training function (mini-batch + Adam)\n",
        "# -----------------------------\n",
        "def skipgram_train_toy(tokenized: List[List[str]],\n",
        "                       w2i: Dict[str,int],\n",
        "                       i2w: Dict[int,str],\n",
        "                       dim: int        = 100,\n",
        "                       window: int     = 2,\n",
        "                       batch_size: int = 256,\n",
        "                       epochs: int     = 10,\n",
        "                       lr: float       = 1e-3,\n",
        "                      ) -> SkipGramModelToy:\n",
        "\n",
        "    centers, contexts = skipgram_pairs_with_edges(tokenized, w2i, window=window)\n",
        "\n",
        "    print(f\"[Skip-gram] data: {len(contexts)} pairs | V={len(w2i)} | d={dim} | window={window}\")\n",
        "\n",
        "    ds = SkipGramDataset(centers, contexts)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = SkipGramTorch(vocab_size=len(w2i), dim=dim).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss, total_n = 0.0, 0\n",
        "        for xb, yb in dl:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)           # (B, |V|)\n",
        "            loss = criterion(logits, yb) # targets are context indices\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            total_n += xb.size(0)\n",
        "        avg_loss = total_loss / max(1, total_n)\n",
        "        if ep % max(1, epochs//5) == 0:\n",
        "            print(f\"  epoch {ep:02d}/{epochs} | avg_loss={avg_loss:.4f}\")\n",
        "\n",
        "    return SkipGramModelToy(w2i=w2i, i2w=i2w, model=model, dim=dim, window=window)\n",
        "\n",
        "def cosine_similarity_numpy(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Cosine(v1, v2) = (v1 · v2) / (||v1|| * ||v2||).\n",
        "    If either norm is zero, return 0.0.\n",
        "    \"\"\"\n",
        "    dot_product = float(np.dot(vec1, vec2))\n",
        "    norm1 = float(np.linalg.norm(vec1))\n",
        "    norm2 = float(np.linalg.norm(vec2))\n",
        "    if norm1 == 0.0 or norm2 == 0.0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "# -----------------------------\n",
        "# Nearest neighbors from W_in\n",
        "# -----------------------------\n",
        "def most_similar_from_Win(word: str, skg: SkipGramModelToy, topn: int = 5) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Top-n neighbors by cosine using rows of W_in (input embeddings).\n",
        "    \"\"\"\n",
        "    if word not in skg.w2i:\n",
        "        return []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        M = skg.model.in_embed.weight.detach().cpu().numpy()  # (|V|, d)\n",
        "\n",
        "    q_idx = skg.w2i[word]\n",
        "    v_q = M[q_idx]\n",
        "\n",
        "    # Cosine similarities against all vocab rows\n",
        "    sims = np.array([cosine_similarity_numpy(M[i], v_q) for i in range(M.shape[0])], dtype=float)\n",
        "\n",
        "    # Exclude the query itself\n",
        "    sims[q_idx] = -np.inf\n",
        "\n",
        "    # Top-n\n",
        "    k = topn\n",
        "    order = np.argsort(-sims)[:k]\n",
        "    return [(skg.i2w[i], float(sims[i])) for i in order]\n",
        "\n",
        "# -----------------------------\n",
        "# Train on the toy corpus\n",
        "# -----------------------------\n",
        "sg_model_pytorch = skipgram_train_toy(\n",
        "    tokenized=tokenized_corpus,\n",
        "    w2i=word_to_index,\n",
        "    i2w=index_to_word,\n",
        "    dim=50,         # embedding dim\n",
        "    window=5,       # context window on each side\n",
        "    batch_size=8,\n",
        "    epochs=20,\n",
        "    lr=0.05,\n",
        ")\n",
        "\n",
        "# Probe neighbors\n",
        "probe_words = [\"king\", \"queen\", \"computer\", \"happy\", \"river\"]\n",
        "print(\"\\nNearest neighbors (Skip-gram PyTorch, cosine on W_in):\")\n",
        "for w in probe_words:\n",
        "    nns = most_similar_from_Win(w, sg_model_pytorch, topn=5)\n",
        "    pretty = \", \".join([f\"{a} ({b:.3f})\" for a, b in nns]) if nns else \"[OOV]\"\n",
        "    print(f\"  • {w:<9}: {pretty}\")\n"
      ],
      "metadata": {
        "id": "bxyWqIUI04SU"
      },
      "id": "bxyWqIUI04SU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-gram (gensim) + t-SNE visualization\n",
        "\n",
        "We now train a **Skip-gram** Word2Vec model using the **gensim** library (to mirror our toy PyTorch implementation) and then visualize a subset of the learned embeddings with **t-SNE**.\n",
        "\n",
        "---\n",
        "\n",
        "### t-SNE visualization notes\n",
        "After training, we project a **subset of words** into 2D with **t-SNE** to visually inspect clusters.\n",
        "\n",
        "- **What t-SNE does:** preserves **local neighborhoods** (who is near whom), not global distances.\n",
        "- **Parameters used:** `n_components=2`; **perplexity** is chosen adaptively from the number of plotted words.\n",
        "- **Caveats:** t-SNE is **stochastic**; absolute distances/axes are **not** directly interpretable—use it to spot **clusters** and **neighbors**, not to make metric claims.\n"
      ],
      "metadata": {
        "id": "CEGrc3qmJhAc"
      },
      "id": "CEGrc3qmJhAc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Skip-gram Word2Vec (gensim) on toy corpus and visualize via t-SNE\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration (mirror toy setup)\n",
        "# -----------------------------\n",
        "sg_config = {\n",
        "    \"architecture\": \"Skip-gram\",\n",
        "    \"vector_size\": 50,           # embedding dim (d)\n",
        "    \"window\": 5,                 # context window (each side)\n",
        "    \"min_count\": 1,              # keep everything in this demo\n",
        "    \"sample\": 0.0,               # disable subsampling (tiny corpus)\n",
        "    \"negative\": 0,               # no negative sampling\n",
        "    \"hs\": 1,                     # use heirarchical softmax\n",
        "    \"epochs\": 20,                # training epochs\n",
        "    \"seed\": RANDOM_SEED,\n",
        "    \"alpha\": 0.05,\n",
        "    \"workers\": 1,\n",
        "}\n",
        "\n",
        "print(\"Training Word2Vec (Skip-gram, gensim)\")\n",
        "print(f\"  • sentences           : {len(tokenized_corpus)}\")\n",
        "print(f\"  • vector_size (dims)  : {sg_config['vector_size']}\")\n",
        "print(f\"  • window              : {sg_config['window']}\")\n",
        "print(f\"  • min_count           : {sg_config['min_count']}\")\n",
        "print(f\"  • negative samples    : {sg_config['negative']}\")\n",
        "print(f\"  • epochs              : {sg_config['epochs']}\")\n",
        "print()\n",
        "\n",
        "# Build the model with parameters on the constructor and train\n",
        "sg_model_gensim = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=sg_config[\"vector_size\"],\n",
        "    window=sg_config[\"window\"],\n",
        "    min_count=sg_config[\"min_count\"],\n",
        "    sample=sg_config[\"sample\"],\n",
        "    sg=1,                                 # 1 for Skip-gram and 0 for CBOW\n",
        "    negative=sg_config[\"negative\"],\n",
        "    hs=sg_config[\"hs\"],\n",
        "    seed=sg_config[\"seed\"],\n",
        "    alpha=sg_config[\"alpha\"],\n",
        "    epochs=sg_config[\"epochs\"],\n",
        "    workers=sg_config[\"workers\"]\n",
        ")\n",
        "\n",
        "# Post-training summary\n",
        "vocab_size = len(sg_model_gensim.wv)\n",
        "print(\"Skip-gram model trained (gensim).\")\n",
        "print(f\"  • vocabulary size     : {vocab_size}\")\n",
        "print(f\"  • embedding dim       : {sg_model_gensim.wv.vector_size}\")\n",
        "print(f\"  • #training sentences : {sg_model_gensim.corpus_count}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "wh_2TOjzJewa"
      },
      "id": "wh_2TOjzJewa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# t-SNE visualization for (man, woman) and (king, queen)\n",
        "# -----------------------------\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Words/pairs to visualize and connect\n",
        "pairs = [(\"man\", \"woman\"), (\"king\", \"queen\")]\n",
        "words = sorted({w for a, b in pairs for w in (a, b)})\n",
        "\n",
        "# In-vocab filtering\n",
        "in_vocab = [w for w in words if w in sg_model_gensim.wv]\n",
        "oov = [w for w in words if w not in sg_model_gensim.wv]\n",
        "if oov:\n",
        "  print(f\"Skipping OOV words: {oov}\")\n",
        "\n",
        "if len(in_vocab) < 2:\n",
        "  print(\"Not enough in-vocab words to plot (need at least 2).\")\n",
        "else:\n",
        "  # Collect embeddings\n",
        "  X = np.array([sg_model_gensim.wv[w] for w in in_vocab])\n",
        "\n",
        "  # Choose a valid perplexity for tiny sets: must be < n_samples\n",
        "  n = len(in_vocab)\n",
        "  perplexity = max(1.0, min(30.0, n - 1.0))\n",
        "\n",
        "  tsne = TSNE(\n",
        "      n_components=2,\n",
        "      perplexity=perplexity,\n",
        "      random_state=RANDOM_SEED\n",
        "  )\n",
        "  coords = tsne.fit_transform(X)\n",
        "  pos = {w: coords[i] for i, w in enumerate(in_vocab)}\n",
        "\n",
        "  # Plot\n",
        "  plt.figure(figsize=(7, 6))\n",
        "  for w in in_vocab:\n",
        "      x, y = pos[w]\n",
        "      plt.scatter(x, y, s=70)\n",
        "      plt.annotate(w, (x, y), fontsize=11, xytext=(6, 3), textcoords=\"offset points\")\n",
        "\n",
        "  # Helper to draw a line between two words if both are present\n",
        "  def connect(a: str, b: str):\n",
        "      if a in pos and b in pos:\n",
        "          xs = [pos[a][0], pos[b][0]]\n",
        "          ys = [pos[a][1], pos[b][1]]\n",
        "          plt.plot(xs, ys, linewidth=2.2, alpha=0.85)\n",
        "\n",
        "  # Draw the requested connections\n",
        "  connect(\"man\", \"woman\")\n",
        "  connect(\"king\", \"queen\")\n",
        "\n",
        "  plt.title(\"t-SNE of Skip-gram embeddings — connections: man–woman, king–queen\")\n",
        "  plt.xlabel(\"t-SNE dim 1\"); plt.ylabel(\"t-SNE dim 2\")\n",
        "  plt.grid(True, alpha=0.3)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "vpLltZl8fJRX"
      },
      "id": "vpLltZl8fJRX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7821f279",
      "metadata": {
        "id": "7821f279"
      },
      "source": [
        "## TASK 1.1: Manual One-Hot Construction (+ Pairwise Similarities)\n",
        "\n",
        "**Objective:**  \n",
        "1) Given a tiny vocabulary and 3 sentences, construct sentence one-hot vectors *(union form)* by hand.  \n",
        "2) **Compute** the **cosine similarity** **between each pair of the three sentence vectors**, and **compare** the results briefly.\n",
        "\n",
        "**Tiny vocabulary:**  \n",
        "$$\n",
        "\\text{V} = [\\text{\"king\"},\\ \\text{\"queen\"},\\ \\text{\"man\"},\\ \\text{\"woman\"},\\ \\text{\"river\"},\\ \\text{\"walk\"}]\n",
        "$$\n",
        "\n",
        "**Sentences:**\n",
        "1. \"king and queen\"  \n",
        "2. \"man and woman walk\"  \n",
        "3. \"the river and the queen\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf48e88",
      "metadata": {
        "id": "8cf48e88"
      },
      "outputs": [],
      "source": [
        "# Task 1.1: Manual One-Hot Construction & Pairwise Similarities\n",
        "\n",
        "# Tiny task vocabulary and sentences\n",
        "task_vocab = [\"king\",\"queen\",\"man\",\"woman\",\"river\",\"walk\"]\n",
        "w2i = {t:i for i,t in enumerate(task_vocab)}\n",
        "task_sents = [\n",
        "    \"king and queen\",\n",
        "    \"man and woman walk\",\n",
        "    \"the river and the queen\"\n",
        "]\n",
        "\n",
        "# Utilities\n",
        "def _tokenize_simple(text: str):\n",
        "    \"\"\"Lowercase + whitespace split.\"\"\"\n",
        "    return [w.lower() for w in text.split()]\n",
        "\n",
        "# TODO: Implement one-hot vector construction for each sentence\n",
        "def hand_one_hot(sentence: str, w2i: dict) -> np.ndarray:\n",
        "    \"\"\"Union one-hot vector for a sentence based on the provided vocabulary mapping.\"\"\"\n",
        "    vec = np.___(___(w2i), dtype=int)\n",
        "    for word in ___(sentence):\n",
        "        if word in w2i:\n",
        "            index = w2i[___]\n",
        "            vec[___] = ___\n",
        "    return vec\n",
        "\n",
        "def cosine_similarity_vec(a, b):\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "    return float(a @ b) / denom if denom > 0 else 0.0\n",
        "\n",
        "# Build vectors\n",
        "task_vecs = []\n",
        "for i, sentence in enumerate(task_sents):\n",
        "    vec = hand_one_hot(sentence, w2i)\n",
        "    task_vecs.append(vec)\n",
        "    print(f\"  Sent {i}: '{sentence}'\")\n",
        "    print(f\"    one-hot -> {vec.tolist()} (number_active_dimension={int(vec.sum())})\")\n",
        "\n",
        "# TODO: Compute pairwise similarities among the 3 sentence vectors\n",
        "pairs = [(0,1), (0,2), (1,2)]\n",
        "print(\"\\nPairwise Cosine similarity (sentence one-hots, union):\")\n",
        "for i, j in pairs:\n",
        "    vi, vj = ___[i], ___[j]\n",
        "    cos_ij = ___(vi, vj)\n",
        "    print(f\"  S{i} ↔ S{j}:  cos={cos_ij:.4f}\")\n",
        "\n",
        "# Insights\n",
        "print(\"\\nCosine reflects pattern overlap (shared active indices),\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3824f3a",
      "metadata": {
        "id": "d3824f3a"
      },
      "source": [
        "## TASK 1.2: Neighbors & Analogies (Skip-gram)\n",
        "\n",
        "**Goal:** Use the trained Skip-gram model **(gensim)** to (1) inspect **top-k nearest neighbors** for several target words and (2) solve **vector analogies** (e.g., *king − man + woman ≈ queen*), then **evaluate** the results.\n",
        "\n",
        "**Targets**: `[\"king\", \"queen\", \"computer\", \"happy\", \"river\"]`\n",
        "\n",
        "**What you’ll do**\n",
        "1. **Neighbors:** For each target $t$, list the **top-5 neighbors** with cosine similarities.  \n",
        "\n",
        "   - This reveals local structure: topical clusters, gender pairs, tech terms, etc.\n",
        "\n",
        "2. **Analogies:** For each analogy $(\\text{positive} - \\text{negative})$, compute the vector  \n",
        "\n",
        "   $\\mathbf{v} = \\sum_{p\\in \\text{positive}} \\mathbf{v}_p \\;-\\; \\sum_{n\\in \\text{negative}} \\mathbf{v}_n$\n",
        "\n",
        "   and retrieve the **top-5** closest words to $\\mathbf{v}$.\n",
        "\n",
        "3. **Evaluation:** If an **expected** answer is given (e.g., *queen*), report\n",
        "   - **Hit@5** (whether expected is in top-5),\n",
        "   - **Rank** of the expected word if present\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61995438",
      "metadata": {
        "id": "61995438"
      },
      "outputs": [],
      "source": [
        "# TASK 1.2: Neighbors & Analogies\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Helpers\n",
        "def topn(model, word, n=5):\n",
        "    \"\"\"Top-n most similar words (cosine) for a single query word.\"\"\"\n",
        "    if (model is None) or (word not in model.wv):\n",
        "        return []\n",
        "    return model.wv.most_similar(word, topn=n)\n",
        "\n",
        "def most_similar_vec(model, positive=None, negative=None, topn_k=5):\n",
        "    \"\"\"\n",
        "    Top-n most similar words to a vector formed by (+) positives and (-) negatives.\n",
        "    \"\"\"\n",
        "    positive = positive or []\n",
        "    negative = negative or []\n",
        "    if model is None:\n",
        "        return []\n",
        "    # Ensure all are in-vocab\n",
        "    # TODO: Prepare positive and negative vector lists\n",
        "    pos = [___ for ___ in ___ if w in model.___]\n",
        "    neg = [___ for ___ in ___ if w in model.___]\n",
        "    if not pos and not neg:\n",
        "        return []\n",
        "    return model.wv.most_similar(positive=pos, negative=neg, topn=topn_k)\n",
        "\n",
        "def rank_of_word(results_list, target_word):\n",
        "    \"\"\"Return 1-based rank of target_word within (word, score) results; None if absent.\"\"\"\n",
        "    for i, (w, _) in enumerate(results_list, start=1):\n",
        "        if w == target_word:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "# Configuration\n",
        "neighbor_targets = [\"king\", \"queen\", \"computer\", \"happy\", \"river\"]\n",
        "analogy_specs = [\n",
        "    # (positives, negatives, expected)\n",
        "    ([\"king\", \"woman\"], [\"man\"], \"queen\"),\n",
        "    ([\"queen\", \"man\"], [\"woman\"], \"king\"),\n",
        "    ([\"programming\", \"computer\"], [\"data\"], None),   # open-ended, no expected\n",
        "]\n",
        "\n",
        "# Neighbors\n",
        "print(\"Nearest neighbors (Skip-gram, cosine similarity)\")\n",
        "for t in neighbor_targets:\n",
        "    # TODO: Get top-5 neighbors\n",
        "    sims = ___(___, ___, n=5)\n",
        "    if not sims:\n",
        "        print(f\"  • {t:<10} → [OOV or no neighbors]\")\n",
        "        continue\n",
        "    pretty = \", \".join([f\"{w} ({s:.5f})\" for w, s in sims])\n",
        "    print(f\"  • {t:<10} → {pretty}\")\n",
        "print()\n",
        "\n",
        "# Analogies\n",
        "print(\"Vector analogies: top-5 results (+/- lists shown)\")\n",
        "for positives, negatives, expected in analogy_specs:\n",
        "    # Show the query\n",
        "    pos_str = \" + \".join(positives) if positives else \"∅\"\n",
        "    neg_str = \" + \".join(negatives) if negatives else \"∅\"\n",
        "    print(f\"  • Query: ({pos_str}) − ({neg_str})\", end=\"\")\n",
        "    if expected:\n",
        "        print(f\"  | expected ≈ {expected}\")\n",
        "    else:\n",
        "        print()\n",
        "\n",
        "    # Compute results\n",
        "    results = most_similar_vec(sg_model_gensim, positive=positives, negative=negatives, topn_k=5)\n",
        "    if not results:\n",
        "        print(\"      → No results (possibly OOV terms).\")\n",
        "        continue\n",
        "\n",
        "    # Pretty print top-5\n",
        "    print(\"      top-5:\", \", \".join([f\"{w} ({s:.5f})\" for w, s in results]))\n",
        "\n",
        "    # Evaluation if expected given\n",
        "    if expected:\n",
        "        r = rank_of_word(results, expected)\n",
        "        hit5 = (r is not None)\n",
        "        print(f\"      eval : Hit@5={hit5} | rank={r if r else '—'}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Word2Vec (CBOW)\n",
        "\n",
        "We train a **CBOW** model (with **naïve softmax**) on the **shared toy corpus** prepared earlier.\n",
        "\n",
        "**Corpus variables used (from the previous cell):**\n",
        "- `tokenized_corpus` (list of token lists)\n",
        "- `word_to_index` (w→i)\n",
        "- `index_to_word` (i→w)\n",
        "\n",
        "---\n",
        "\n",
        "### Data preparation (with few examples)\n",
        "\n",
        "We use a symmetric window of size **$W$** on each side.\n",
        "\n",
        "- **Strict (HW style):** require a **full** window of exactly $W$ tokens on **both** sides; boundary positions are **skipped** (no padding/masking).\n",
        "- **Robust (alternative):** clip the window at sentence boundaries and average whatever context exists (can be fewer than $2W$).\n",
        "\n",
        "Each eligible position yields **one** training example: **(mean of context embeddings) → center**.\n",
        "\n",
        "**Example (from the corpus, $W=2$):**\n",
        "\n",
        "Sentence: “**a bridge crosses the wide river**”  \n",
        "Tokens: $[a,\\; bridge,\\; crosses,\\; the,\\; wide,\\; river]$\n",
        "\n",
        "- Center (target) $t=$ “crosses” at position $p=2$  \n",
        "  Context indices (clipped): $\\{0,1\\}\\cup\\{3,4\\}$ → contexts $c \\in \\{a,\\; bridge,\\; the,\\; wide\\}$\n",
        "  \n",
        "  Emitted example for both strict and robust modes: **mean($a, bridge, the, wide$) → $t=$ crosses**\n",
        "\n",
        "- Center (target) $t=$ “a” at position $p=0$  \n",
        "  Context indices (clipped) $\\{1,2\\}$ → contexts $c \\in \\{bridge,\\; crosses\\}$\n",
        "  \n",
        "  **Strict:** **skip** (because we don’t have $W=2$ tokens on the left)\n",
        "  \n",
        "  Emitted example for robust mode: **mean$(bridge, crosses$) → $t=$ a**\n",
        "\n",
        "(These tokens are mapped to integer ids via `word_to_index` before batching.)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "sGittnKqNs32"
      },
      "id": "sGittnKqNs32"
    },
    {
      "cell_type": "markdown",
      "id": "4e9800ac",
      "metadata": {
        "id": "4e9800ac"
      },
      "source": [
        "![CBOW Example](https://sp-ao.shortpixel.ai/client/to_auto,q_glossy,ret_img,w_1177,h_824/https://towardsmachinelearning.org/wp-content/uploads/2022/04/CBOW1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notation ($t$=target/center, $c$=context)\n",
        "\n",
        "Let $|V|$ be the vocabulary size and $d$ the embedding dimension.\n",
        "\n",
        "- $W_{\\text{in}} \\in \\mathbb{R}^{|V|\\times d}$: **input/context** embedding table; for word index $c$,  \n",
        "  $\\mathbf{v}_c = W_{\\text{in}}[c,:]$.\n",
        "- $W_{\\text{out}} \\in \\mathbb{R}^{|V|\\times d}$: **output/target** embedding table; for word index $t$,  \n",
        "  $\\mathbf{u}_t = W_{\\text{out}}[t,:]$.\n",
        "- $b_{\\text{out}} \\in \\mathbb{R}^{|V|}$: output bias; component $b_{\\text{out},j}$ for class $j$.\n",
        "\n",
        "---\n",
        "\n",
        "### Model (naïve softmax)\n",
        "\n",
        "Given the set of context indices $\\text{C}$ (the $W$ tokens to the left and $W$ to the right) around a center:\n",
        "\n",
        "1) **Aggregate context** (mean of input embeddings):\n",
        "\n",
        "$$\n",
        "\\mathbf{h} \\;=\\; \\frac{1}{|\\text{C}|}\\sum_{c \\in \\text{C}} \\mathbf{v}_c .\n",
        "$$\n",
        "\n",
        "2) **Score** every vocabulary item and **predict the center index** $t$:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} \\;=\\; W_{\\text{out}}\\,\\mathbf{h} + b_{\\text{out}},\n",
        "\\qquad\n",
        "\\hat{\\mathbf{y}} \\;=\\; \\mathrm{softmax}(\\mathbf{z}),\n",
        "\\quad\n",
        "\\hat{y}_j \\;=\\; \\frac{e^{z_j}}{\\sum_{k=1}^{|V|} e^{z_k}} .\n",
        "$$\n",
        "\n",
        "**Cross-entropy loss (context → center)** with a one-hot target for the true center $t$:\n",
        "\n",
        "$$\n",
        "\\boxed{\\,L(\\text{C},t) = -\\log \\hat{y}_t\\,}\n",
        "$$\n",
        "\n",
        "**Equivalent log-sum-exp form** (expanding the softmax):\n",
        "\n",
        "$$\n",
        "\\boxed{\\,L(\\text{C},t)\n",
        "= -\\big(\\mathbf{u}_t^\\top \\mathbf{h} + b_{\\text{out},t}\\big)\n",
        "\\;+\\; \\log \\sum_{j=1}^{|V|} \\exp\\!\\big(\\mathbf{u}_j^\\top \\mathbf{h} + b_{\\text{out},j}\\big)\\,}\n",
        "$$\n",
        "\n",
        "> The $+\\log \\sum \\exp$ term is the **normalizer** over all classes $j$ (including $t$). Intuition: make the aggregated context $\\mathbf{h}$ align with the correct center vector $\\mathbf{u}_t$, while the normalization distributes probability mass away from incorrect centers.\n",
        "\n",
        "> **Efficiency note:** We use **naïve softmax** here (summing over the **entire vocabulary**). In practice, **negative sampling** often replaces the full softmax with $1$ positive $+\\;K$ negatives to avoid the $\\text{O}(|V|)$ normalization.\n",
        "\n",
        "---\n",
        "\n",
        "### Training setup (this demo)\n",
        "\n",
        "- **Embedding dim** $d$: 50  \n",
        "- **Window**: 5 (words on each side)  \n",
        "- **Batch size**: 8  \n",
        "- **Epochs**: 20  \n",
        "- **Optimizer**: Adam (learning rate $= 0.05$)  \n",
        "- **Loss**: `nn.CrossEntropyLoss()` (applies softmax + CE)\n",
        "\n",
        "---\n",
        "\n",
        "### After training\n",
        "\n",
        "- Keep $W_{\\text{in}}$ as the word embeddings (often discard/ignore $W_{\\text{out}}$ downstream).\n",
        "\n",
        "- We use the **input embeddings** $W_{\\text{in}}$ as word vectors.\n",
        "\n",
        "\n",
        "- To embed a word (token) with index \\(w\\), its embedding is\n",
        "  $$\n",
        "  \\mathbf{v}_w = W_{\\text{in}}[w,:] \\in \\mathbb{R}^d.\n",
        "  $$\n",
        "\n",
        "- To embed a sentence, let a sentence be a sequence of word (token) indices\n",
        "  $$\n",
        "  S = (w_1, w_2, \\dots, w_m).\n",
        "  $$\n",
        "\n",
        "  We define its embedding as the **mean of its word embeddings**:\n",
        "  $$\n",
        "  \\mathrm{embed}(S)\n",
        "  \\;=\\; \\frac{1}{m} \\sum_{r=1}^{m} \\mathbf{v}_{w_r}\n",
        "  \\;=\\; \\frac{1}{m} \\sum_{r=1}^{m} W_{\\text{in}}[w_r,:].\n",
        "  $$\n",
        "\n",
        "- Nearest neighbors are computed with **cosine similarity** on rows of $W_{\\text{in}}$:\n",
        "\n",
        "$$\n",
        "\\cos(\\mathbf{v}_a,\\mathbf{v}_b) \\;=\\;\n",
        "\\frac{\\mathbf{v}_a^\\top \\mathbf{v}_b}{\\lVert \\mathbf{v}_a\\rVert\\,\\lVert \\mathbf{v}_b\\rVert}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "9Ia4LMJciB2N"
      },
      "id": "9Ia4LMJciB2N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBOW (gensim) + t-SNE visualization\n",
        "\n",
        "We now train a **CBOW** Word2Vec model using the **gensim** library and then visualize a subset of the learned embeddings with **t-SNE**.\n",
        "\n",
        "**Training setup difference compared with Skip-gram**  \n",
        "- Architecture: **CBOW** (`sg=0`, `cbow_mean=1`)  \n"
      ],
      "metadata": {
        "id": "p1mOsmGSArko"
      },
      "id": "p1mOsmGSArko"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CBOW Word2Vec using gensim and visualize with t-SNE\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "cbow_config = {\n",
        "    \"architecture\": \"CBOW\",\n",
        "    \"vector_size\": 50,           # embedding dim (d)\n",
        "    \"window\": 5,                 # context window (each side)\n",
        "    \"min_count\": 1,              # keep everything in this demo\n",
        "    \"sample\": 0.0,               # disable subsampling (tiny corpus)\n",
        "    \"negative\": 0,               # no negative sampling\n",
        "    \"hs\": 1,                     # use heirarchical softmax\n",
        "    \"epochs\": 20,                # training epochs\n",
        "    \"seed\": RANDOM_SEED,\n",
        "    \"alpha\": 0.05,\n",
        "    \"workers\": 1,\n",
        "    \"cbow_mean\": 1,              # average the context vectors\n",
        "}\n",
        "\n",
        "print(\"Training Word2Vec (CBOW, gensim)\")\n",
        "print(f\"  • sentences           : {len(tokenized_corpus)}\")\n",
        "print(f\"  • vector_size (dims)  : {cbow_config['vector_size']}\")\n",
        "print(f\"  • window              : {cbow_config['window']}\")\n",
        "print(f\"  • min_count           : {cbow_config['min_count']}\")\n",
        "print(f\"  • negative samples    : {cbow_config['negative']}\")\n",
        "print(f\"  • epochs              : {cbow_config['epochs']}\")\n",
        "print()\n",
        "\n",
        "# Build the model with specified parameters on the constructor and train\n",
        "cbow_model_gensim = Word2Vec(\n",
        "    sentences=tokenized_corpus,                 # corpus here\n",
        "    vector_size=cbow_config[\"vector_size\"],\n",
        "    window=cbow_config[\"window\"],\n",
        "    min_count=cbow_config[\"min_count\"],\n",
        "    sample=sg_config[\"sample\"],\n",
        "    sg=0,                                       # 0 for CBOW, 1 for Skip-gram\n",
        "    negative=cbow_config[\"negative\"],\n",
        "    hs=sg_config[\"hs\"],\n",
        "    seed=cbow_config[\"seed\"],\n",
        "    cbow_mean=cbow_config[\"cbow_mean\"],\n",
        "    alpha=sg_config[\"alpha\"],\n",
        "    epochs=cbow_config[\"epochs\"],\n",
        "    workers=sg_config[\"workers\"]\n",
        "\n",
        ")\n",
        "\n",
        "# Post-training summary\n",
        "vocab_size = len(cbow_model_gensim.wv)\n",
        "print(\"CBOW model trained (gensim).\")\n",
        "print(f\"  • vocabulary size     : {vocab_size}\")\n",
        "print(f\"  • embedding dim       : {cbow_model_gensim.wv.vector_size}\")\n",
        "print(f\"  • #training sentences : {cbow_model_gensim.corpus_count}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "WJGl7xxuAuma"
      },
      "id": "WJGl7xxuAuma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# t-SNE visualization for (man, woman) and (king, queen)\n",
        "# -----------------------------\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Words/pairs to visualize and connect\n",
        "pairs = [(\"man\", \"woman\"), (\"king\", \"queen\")]\n",
        "words = sorted({w for a, b in pairs for w in (a, b)})\n",
        "\n",
        "# In-vocab filtering\n",
        "in_vocab = [w for w in words if w in cbow_model_gensim.wv]\n",
        "oov = [w for w in words if w not in cbow_model_gensim.wv]\n",
        "if oov:\n",
        "  print(f\"Skipping OOV words: {oov}\")\n",
        "\n",
        "if len(in_vocab) < 2:\n",
        "  print(\"Not enough in-vocab words to plot (need at least 2).\")\n",
        "else:\n",
        "  # Collect embeddings\n",
        "  X = np.array([cbow_model_gensim.wv[w] for w in in_vocab])\n",
        "\n",
        "  # Choose a valid perplexity for tiny sets: must be < n_samples\n",
        "  n = len(in_vocab)\n",
        "  perplexity = max(1.0, min(30.0, n - 1.0))\n",
        "\n",
        "  tsne = TSNE(\n",
        "      n_components=2,\n",
        "      perplexity=perplexity,\n",
        "      random_state=RANDOM_SEED\n",
        "  )\n",
        "  coords = tsne.fit_transform(X)\n",
        "  pos = {w: coords[i] for i, w in enumerate(in_vocab)}\n",
        "\n",
        "  # Plot\n",
        "  plt.figure(figsize=(7, 6))\n",
        "  for w in in_vocab:\n",
        "      x, y = pos[w]\n",
        "      plt.scatter(x, y, s=70)\n",
        "      plt.annotate(w, (x, y), fontsize=11, xytext=(6, 3), textcoords=\"offset points\")\n",
        "\n",
        "  # Helper to draw a line between two words if both are present\n",
        "  def connect(a: str, b: str):\n",
        "      if a in pos and b in pos:\n",
        "          xs = [pos[a][0], pos[b][0]]\n",
        "          ys = [pos[a][1], pos[b][1]]\n",
        "          plt.plot(xs, ys, linewidth=2.2, alpha=0.85)\n",
        "\n",
        "  # Draw the requested connections\n",
        "  connect(\"man\", \"woman\")\n",
        "  connect(\"king\", \"queen\")\n",
        "\n",
        "  plt.title(\"t-SNE of CBOW embeddings — connections: man–woman, king–queen\")\n",
        "  plt.xlabel(\"t-SNE dim 1\"); plt.ylabel(\"t-SNE dim 2\")\n",
        "  plt.grid(True, alpha=0.3)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "lXXzMZo3eQPo"
      },
      "id": "lXXzMZo3eQPo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. GloVe (Global Vectors) — PyTorch on the Toy Corpus\n",
        "\n",
        "We fit **GloVe** with **PyTorch** on the same **toy corpus**.  \n",
        "Unlike Skip-gram/CBOW (predictive, **local** windows + softmax), GloVe is **regression** on the **global** co-occurrence matrix.\n",
        "\n",
        "**Corpus variables used (from the shared toy cell):**\n",
        "- `tokenized_corpus` (list of token lists)\n",
        "- `word_to_index` (w→i)\n",
        "- `index_to_word` (i→w)\n",
        "\n",
        "---\n",
        "\n",
        "### Data (how we build $X$ in this demo)\n",
        "- Symmetric **window** of size $W$ on each side (we clip at sentence boundaries).  \n",
        "- For each center position $i$ and each context $j$ in its window, add **$1/\\text{distance}$** to $X_{ij}$.\n",
        "- This captures **global** co-occurrence structure across the whole corpus.\n",
        "\n",
        "Each non-zero $(i,j)$ becomes a **training pair** carrying $(X_{ij}, f(X_{ij}))$."
      ],
      "metadata": {
        "id": "lbG0lbTHRJaf"
      },
      "id": "lbG0lbTHRJaf"
    },
    {
      "cell_type": "markdown",
      "id": "e89d5d5d",
      "metadata": {
        "id": "e89d5d5d"
      },
      "source": [
        "### Mini co-occurrence example for $X$\n",
        "\n",
        "**Tiny corpus**\n",
        "- “king rules the kingdom”\n",
        "- “the queen rules the kingdom”\n",
        "\n",
        "**Vocabulary**\n",
        "$[\\,\\text{king},\\ \\text{kingdom},\\ \\text{queen},\\ \\text{rules},\\ \\text{the}\\,]$\n",
        "\n",
        "**Settings**: symmetric window $w=2$; add $1/\\text{distance}$ for each context token.\n",
        "\n",
        "#### Directed matrix $X$ (rows = center $i$, columns = context $j$)\n",
        "|        | king | kingdom | queen | rules | the |\n",
        "|:------:|:----:|:-------:|:-----:|:-----:|:---:|\n",
        "| **king**    | 0.0 | 0.0 | 0.0 | 1.0 | 0.5 |\n",
        "| **kingdom** | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 |\n",
        "| **queen**   | 0.0 | 0.0 | 0.0 | 1.0 | 1.5 |\n",
        "| **rules**   | 1.0 | 1.0 | 1.0 | 0.0 | 2.5 |\n",
        "| **the**     | 0.5 | 2.0 | 1.5 | 2.5 | 0.0 |\n",
        "\n",
        "*Interpretation:* $X_{\\text{king},\\text{rules}}=1.0$ because **rules** is at distance 1 from **king**;  \n",
        "$X_{\\text{king},\\text{the}}=0.5$ because **the** is at distance 2 from **king**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc7d1f3",
      "metadata": {
        "id": "3bc7d1f3"
      },
      "source": [
        "![GloVe Example](https://www.researchgate.net/publication/337461648/figure/fig1/AS:11431281342676763@1743564919258/The-model-architecture-of-GloVe-The-input-is-a-one-hot-representation-of-a-word-The.tif)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notation ($i$=center word in a pair, $j$=context word in a pair)\n",
        "\n",
        "Let $|V|$ be vocab size and $d$ the embedding dim.\n",
        "- $W_{\\text{in}} \\in \\mathbb{R}^{|V|\\times d}$: **word** embedding table; $\\mathbf{v}_i = W_{\\text{in}}[i,:]$  \n",
        "- $W_{\\text{out}} \\in \\mathbb{R}^{|V|\\times d}$: **context** embedding table; $\\mathbf{u}_j = W_{\\text{out}}[j,:]$  \n",
        "- $b_{\\text{in}} \\in \\mathbb{R}^{|V|}$: word **biases** for center words; $b_i$ = $b_{\\text{in}}[i]$\n",
        "- $b_{\\text{out}} \\in \\mathbb{R}^{|V|}$: word **biases** for context words; $b_{\\text{ctx},j}$ = $b_{\\text{out}}[j]$\n",
        "- $X \\in \\mathbb{R}_{\\ge 0}^{|V|\\times|V|}$: **(directed) co-occurrence**; entry $X_{ij}$ counts how often context $j$ appears around word $i$  \n",
        "  (we build it with a symmetric window and **$1/\\text{distance}$** weighting).\n",
        "\n",
        "---\n",
        "\n",
        "### Objective (weighted least squares)\n",
        "For all pairs with $X_{ij}>0$, minimize\n",
        "$$\n",
        "J \\;=\\; \\sum_{i,j} f(X_{ij}) \\Big( \\underbrace{\\mathbf{v}_i^\\top \\mathbf{u}_j + b_i + b_{\\text{ctx},j}}_{\\text{prediction} s_{ij}} \\;-\\; \\log X_{ij} \\Big)^2,\n",
        "$$\n",
        "\n",
        "with the standard GloVe **weighting function**\n",
        "\n",
        "$$\n",
        "f(x) \\;=\\;\n",
        "\\begin{cases}\n",
        "\\left(\\dfrac{x}{x_{\\max}}\\right)^\\alpha, & x < x_{\\max},\\\\[6pt]\n",
        "1, & x \\ge x_{\\max}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Intuition.** Learn embeddings so that the score $s_{ij}$ **approximates** $\\log X_{ij}$.  \n",
        "\n",
        "$f(x)$ down-weights very rare pairs and caps the influence of very frequent pairs.\n",
        "\n",
        "**Role of $\\alpha$**\n",
        "\n",
        "$\\alpha$ controls how fast $f(x)$ grows for $x < x_{\\max}$:\n",
        "\n",
        "- If $\\alpha = 1$: $f(x) = x/x_{\\max}$ (linear growth).\n",
        "- If $0 < \\alpha < 1$ (usual case): sublinear growth → very small counts are down-weighted, mid-range counts are emphasized.\n",
        "\n",
        "In practice, GloVe typically uses $\\alpha = 0.75$.\n",
        "\n",
        "**Role of $x_{\\max}$**\n",
        "\n",
        "$x_{\\max}$ is the **co-occurrence cutoff**:\n",
        "\n",
        "- For $x < x_{\\max}$: $f(x) = (x/x_{\\max})^\\alpha$ increases with $x$.\n",
        "- For $x \\ge x_{\\max}$: $f(x) = 1$ (extra frequency doesn’t increase the weight).\n",
        "\n",
        "This prevents extremely frequent words (e.g. “the”, “of”) from dominating the loss.\n",
        "\n",
        "In practice, $x_{\\max} \\in [50, 100]$ (often $x_{\\max} = 100$; $50$ is also reasonable).\n",
        "\n",
        "---\n",
        "\n",
        "### Training setup (this demo)\n",
        "- **Embedding dim** $d$: 100  \n",
        "- **Window**: 5 (distance-weighted $1/\\text{dist}$)  \n",
        "- **Weighting**: $x_{\\max}=50$, $\\alpha=0.75$  \n",
        "- **Optimizer**: Adam (learning rate $= 0.05$)\n",
        "- **Epochs**: 100\n",
        "- **Batch size**: 256  \n",
        "- **Init**: small Gaussian for $W_{\\text{in}}, W_{\\text{out}}$ and zeros for biases\n",
        "\n",
        "---\n",
        "\n",
        "### After training\n",
        "\n",
        "- Common practice is to use the **sum** of word and context embeddings as the final word vector:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{e}_i \\;=\\; \\mathbf{v}_i \\;+\\; \\mathbf{u}_i,\n",
        "  $$\n",
        "\n",
        "  where $\\mathbf{v}_i = W_{\\text{in}}[i,:]$ and $\\mathbf{u}_i = W_{\\text{out}}[i,:]$.\n",
        "\n",
        "- Collect these into a single embedding matrix\n",
        "\n",
        "  $$\n",
        "  E \\;=\\; W_{\\text{in}} + W_{\\text{out}} \\in \\mathbb{R}^{|V|\\times d},\n",
        "  $$\n",
        "\n",
        "  and use the $i$-th row $\\mathbf{e}_i = E[i,:]$ as the embedding for word index $i$.\n",
        "\n",
        "- To embed a word (token) with index $w$, its embedding is\n",
        "\n",
        "  $$\n",
        "  \\mathbf{e}_w = E[w,:] \\in \\mathbb{R}^d.\n",
        "  $$\n",
        "\n",
        "- To embed a sentence, let the sentence be a sequence of word (token) indices\n",
        "\n",
        "  $$\n",
        "  S = (w_1, w_2, \\dots, w_m).\n",
        "  $$\n",
        "\n",
        "  We define its embedding as the **mean of its word embeddings**:\n",
        "\n",
        "  $$\n",
        "  \\mathrm{embed}(S)\n",
        "  \\;=\\; \\frac{1}{m} \\sum_{r=1}^{m} \\mathbf{e}_{w_r}\n",
        "  \\;=\\; \\frac{1}{m} \\sum_{r=1}^{m} E[w_r,:]\n",
        "  $$\n",
        "\n",
        "- Nearest neighbors are computed with **cosine similarity** on rows of \\(E\\):\n",
        "\n",
        "  $$\n",
        "  \\cos(\\mathbf{e}_a,\\mathbf{e}_b) \\;=\\;\n",
        "  \\frac{\\mathbf{e}_a^\\top \\mathbf{e}_b}{\\lVert \\mathbf{e}_a\\rVert\\,\\lVert \\mathbf{e}_b\\rVert}\n",
        "  $$\n",
        "\n"
      ],
      "metadata": {
        "id": "XJ-c-Jx3cNc8"
      },
      "id": "XJ-c-Jx3cNc8"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# GloVe (weighted least squares) — PyTorch on the toy corpus\n",
        "# ================================\n",
        "import math, random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -----------------------------\n",
        "# Reproducibility & device\n",
        "# -----------------------------\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Build directed co-occurrence X with distance weighting\n",
        "# -----------------------------\n",
        "def build_cooccurrence(tokenized: List[List[str]],\n",
        "                       w2i: Dict[str,int],\n",
        "                       window: int = 5) -> Dict[Tuple[int,int], float]:\n",
        "    \"\"\"\n",
        "    X[(i,j)] = sum over occurrences of j in the symmetric window around i of (1 / distance)\n",
        "    Window is clipped at sentence boundaries.\n",
        "    \"\"\"\n",
        "    X = defaultdict(float)\n",
        "    for sent in tokenized:\n",
        "        ids = [w2i[w] for w in sent if w in w2i]\n",
        "        n = len(ids)\n",
        "        for c in range(n):\n",
        "            i_idx = ids[c]  # center word index\n",
        "            left  = max(0, c - window)\n",
        "            right = min(n, c + window + 1)\n",
        "            for pos in range(left, right):\n",
        "                if pos == c:\n",
        "                    continue\n",
        "                j_idx = ids[pos]\n",
        "                dist = abs(pos - c)\n",
        "                X[(i_idx, j_idx)] += 1.0 / dist\n",
        "    return X\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset over non-zero co-occurrence pairs\n",
        "# -----------------------------\n",
        "class GloVePairs(Dataset):\n",
        "    def __init__(self, pairs_ij, x_vals, weights):\n",
        "        self.i = torch.tensor([p[0] for p in pairs_ij], dtype=torch.long)\n",
        "        self.j = torch.tensor([p[1] for p in pairs_ij], dtype=torch.long)\n",
        "        self.x = torch.tensor(x_vals, dtype=torch.float32)\n",
        "        self.w = torch.tensor(weights, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return self.i.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.i[idx], self.j[idx], self.x[idx], self.w[idx]\n",
        "\n",
        "# -----------------------------\n",
        "# GloVe model: embeddings + biases\n",
        "# -----------------------------\n",
        "class GloVeTorch(nn.Module):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      W_in   (|V|, d)   → word embeddings  v_i\n",
        "      W_out  (|V|, d)   → context embeddings u_j\n",
        "      b_word (|V|,)     → word bias b_i\n",
        "      b_ctx  (|V|,)     → context bias b_ctx,j\n",
        "    Prediction: s_ij = v_i^T u_j + b_i + b_ctx,j\n",
        "    Loss per pair: w_ij * (s_ij - log X_ij)^2\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.in_embed  = nn.Embedding(vocab_size, dim)\n",
        "        self.out_embed = nn.Embedding(vocab_size, dim)\n",
        "        self.b_word    = nn.Embedding(vocab_size, 1)\n",
        "        self.b_ctx     = nn.Embedding(vocab_size, 1)\n",
        "\n",
        "        # Initialization (small Gaussian for embeddings, zeros for biases)\n",
        "        nn.init.normal_(self.in_embed.weight,  mean=0.0, std=0.01)\n",
        "        nn.init.normal_(self.out_embed.weight, mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.b_word.weight)\n",
        "        nn.init.zeros_(self.b_ctx.weight)\n",
        "\n",
        "    def forward(self, i_idx: torch.LongTensor, j_idx: torch.LongTensor):\n",
        "        v_i = self.in_embed(i_idx)            # (B, d)\n",
        "        u_j = self.out_embed(j_idx)           # (B, d)\n",
        "        b_i = self.b_word(i_idx).squeeze(-1)  # (B,)\n",
        "        b_j = self.b_ctx(j_idx).squeeze(-1)   # (B,)\n",
        "        s_ij = (v_i * u_j).sum(dim=1) + b_i + b_j  # (B,)\n",
        "        return s_ij\n",
        "\n",
        "    def combined_matrix(self):\n",
        "        \"\"\"E = W_in + W_out (|V|, d) for downstream similarity.\"\"\"\n",
        "        return self.in_embed.weight + self.out_embed.weight\n",
        "\n",
        "# -----------------------------\n",
        "# Weighting function f(x) for GloVe\n",
        "# -----------------------------\n",
        "def glove_weight(x: np.ndarray, x_max: float, alpha: float) -> np.ndarray:\n",
        "    w = (x / x_max) ** alpha\n",
        "    w = np.where(x < x_max, w, 1.0)\n",
        "    return w.astype(np.float32)\n",
        "\n",
        "# -----------------------------\n",
        "# Training function (prints prep & co-occurrence tables)\n",
        "# -----------------------------\n",
        "def glove_train_torch(tokenized: List[List[str]],\n",
        "                      w2i: Dict[str,int],\n",
        "                      i2w: Dict[int,str],\n",
        "                      dim: int        = 50,\n",
        "                      window: int     = 5,\n",
        "                      x_max: float    = 50.0,\n",
        "                      alpha: float    = 0.75,\n",
        "                      batch_size: int = 256,\n",
        "                      epochs: int     = 30,\n",
        "                      lr: float       = 0.05):\n",
        "    # Build directed co-occurrence\n",
        "    X = build_cooccurrence(tokenized, w2i, window=window)\n",
        "\n",
        "    # ---- Prints: prep + dense tables ----\n",
        "    V = len(i2w)\n",
        "    vocab_list = [i2w[i] for i in range(V)]\n",
        "    print(\"\\nGloVe prep:\")\n",
        "    print(f\"  • vocab size               : {V}\")\n",
        "    print(f\"  • nonzero co-occurrences   : {len(X)}\")\n",
        "\n",
        "    # Dense table for readability\n",
        "    Xd = np.zeros((V, V), dtype=float)\n",
        "    for (i, j), val in X.items():\n",
        "        Xd[i, j] = val\n",
        "\n",
        "    df_directed  = pd.DataFrame(Xd, index=vocab_list, columns=vocab_list)\n",
        "\n",
        "    print(\"\\nCo-occurrence matrix X (directed): values rounded to 2 decimals\")\n",
        "    display(df_directed.round(2))\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Convert to training tensors\n",
        "    pairs = list(X.keys())\n",
        "    xvals = np.array([X[p] for p in pairs], dtype=np.float32)\n",
        "    wvals = glove_weight(xvals, x_max=x_max, alpha=alpha)\n",
        "\n",
        "    print(\"\\nGloVe training (PyTorch):\")\n",
        "    print(f\"  • window                 : {window}\")\n",
        "    print(f\"  • x_max, alpha           : {x_max}, {alpha}\")\n",
        "    print(f\"  • d, batch, epochs       : {dim}, {batch_size}, {epochs}\")\n",
        "    print(f\"  • optimizer              : Adam (lr={lr})\")\n",
        "\n",
        "    ds = GloVePairs(pairs, xvals, wvals)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = GloVeTorch(vocab_size=V, dim=dim).to(DEVICE)\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss, total_n = 0.0, 0\n",
        "        for i_idx, j_idx, x_ij, w_ij in dl:\n",
        "            i_idx = i_idx.to(DEVICE); j_idx = j_idx.to(DEVICE)\n",
        "            x_ij  = x_ij.to(DEVICE);  w_ij  = w_ij.to(DEVICE)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            s_ij = model(i_idx, j_idx)           # (B,)\n",
        "            r    = s_ij - torch.log(x_ij + 1e-12) # Added tiny epsilon for stability to avoid log(0)\n",
        "            loss = (w_ij * (r ** 2)).mean()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += float(loss.item()) * i_idx.size(0)\n",
        "            total_n    += i_idx.size(0)\n",
        "        avg = total_loss / max(1, total_n)\n",
        "        if ep % max(1, epochs//5) == 0:\n",
        "            print(f\"  epoch {ep:02d}/{epochs} | avg_weighted_MSE≈{avg:.6f}\")\n",
        "\n",
        "    return model, X  # Return model and the co-occurrence dict for any later use\n",
        "\n",
        "def cosine_similarity_numpy(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Cosine(v1, v2) = (v1 · v2) / (||v1|| * ||v2||).\n",
        "    If either norm is zero, return 0.0.\n",
        "    \"\"\"\n",
        "    dot_product = float(np.dot(vec1, vec2))\n",
        "    norm1 = float(np.linalg.norm(vec1))\n",
        "    norm2 = float(np.linalg.norm(vec2))\n",
        "    if norm1 == 0.0 or norm2 == 0.0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "# -----------------------------\n",
        "# Nearest neighbors on E = W_in + W_out\n",
        "# -----------------------------\n",
        "def glove_most_similar(word: str,\n",
        "                       model: GloVeTorch,\n",
        "                       w2i: Dict[str,int],\n",
        "                       i2w: Dict[int,str],\n",
        "                       topn: int = 5):\n",
        "    \"\"\"\n",
        "    Top-n neighbors by cosine similarity using E = W_in + W_out.\n",
        "    \"\"\"\n",
        "    if word not in w2i:\n",
        "        return []\n",
        "\n",
        "    # Combined embeddings (E = W_in + W_out)\n",
        "    with torch.no_grad():\n",
        "        E = model.combined_matrix().detach().cpu().numpy()  # shape: (|V|, d)\n",
        "\n",
        "    q_idx = w2i[word]\n",
        "    v_q = E[q_idx]  # (d,)\n",
        "\n",
        "    # Cosine similarities against all words\n",
        "    sims = np.array([cosine_similarity_numpy(E[i], v_q) for i in range(E.shape[0])], dtype=float)\n",
        "\n",
        "    # Exclude the query token itself\n",
        "    sims[q_idx] = -np.inf\n",
        "\n",
        "    # Top-n by cosine\n",
        "    k = topn\n",
        "    order = np.argsort(-sims)[:k]\n",
        "    return [(i2w[i], float(sims[i])) for i in order]\n",
        "\n",
        "# -----------------------------\n",
        "# Train on the toy corpus & probe (PyTorch GloVe)\n",
        "# -----------------------------\n",
        "glove_model, X_dict = glove_train_torch(\n",
        "    tokenized=tokenized_corpus,\n",
        "    w2i=word_to_index,\n",
        "    i2w=index_to_word,\n",
        "    dim=100,\n",
        "    window=5,\n",
        "    x_max=50,\n",
        "    alpha=0.75,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    lr=0.05\n",
        ")\n",
        "\n",
        "probe_words = [\"king\", \"queen\", \"computer\", \"happy\", \"river\", \"bridge\"]\n",
        "print(\"\\nNearest neighbors (GloVe PyTorch, cosine on W_in + W_out):\")\n",
        "for w in probe_words:\n",
        "    nns = glove_most_similar(w, glove_model, word_to_index, index_to_word, topn=5)\n",
        "    pretty = \", \".join([f\"{a} ({b:.3f})\" for a,b in nns]) if nns else \"[OOV]\"\n",
        "    print(f\"  • {w:<9}: {pretty}\")\n"
      ],
      "metadata": {
        "id": "6C83bx4dZMeP"
      },
      "id": "6C83bx4dZMeP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison: CBOW/Skip-gram vs GloVe\n",
        "\n",
        "- These are **static** word embeddings; they do not handle OOVs or context differences (no contextual embeddings).  \n",
        "- Differences reflect **training objective & statistics**: predictive (Skip-gram/CBOW) vs. global regression (GloVe).\n",
        "  - **Skip-gram/CBOW**: predictive, use **local** context windows with negative sampling.\n",
        "  - **GloVe**: regression on **global** counts, fits all (non-zero) pairs jointly.\n",
        "  - In practice, neighbor lists may be similar in general but can **differ**: GloVe tends to reflect **broader global statistics**, while Skip-gram may capture **finer local relations** (especially for rarer words), and CBOW may capture frequent-context neighbors better.\n"
      ],
      "metadata": {
        "id": "8l6Gp0qmXnkk"
      },
      "id": "8l6Gp0qmXnkk"
    },
    {
      "cell_type": "markdown",
      "id": "afad819b",
      "metadata": {
        "id": "afad819b"
      },
      "source": [
        "## TASK 2.1: CBOW vs Skip-gram Comparison\n",
        "\n",
        "**What you’ll implement**\n",
        "\n",
        "- **Model comparison.**  \n",
        "   For `[\"king\", \"computer\", \"happy\", \"river\", \"bridge\"]`:\n",
        "   - list **CBOW** and **Skip-gram** neighbors (top-5),\n",
        "   - compute **overlap** (intersection size and Jaccard index),\n",
        "   - report a simple **coherence** metric = average pairwise cosine **among the 5 neighbors** (higher ≈ tighter cluster).\n",
        "\n",
        "Let\n",
        "- $C_w$ = set of top-5 CBOW neighbors for word $w$,\n",
        "- $S_w$ = set of top-5 Skip-gram neighbors for word $w$.\n",
        "\n",
        "Then:\n",
        "\n",
        "- **Overlap size**:\n",
        "  $$\n",
        "  \\text{overlap}(w) = \\bigl| C_w \\cap S_w \\bigr|.\n",
        "  $$\n",
        "\n",
        "- **Jaccard index**:\n",
        "  $$\n",
        "  J(w) = \\frac{\\bigl| C_w \\cap S_w \\bigr|}{\\bigl| C_w \\cup S_w \\bigr|}.\n",
        "  $$\n",
        "\n",
        "Notes:\n",
        "- Training on tiny corpora can produce many near-ties, that's why the results scores are very close to each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a371a9",
      "metadata": {
        "id": "a4a371a9"
      },
      "outputs": [],
      "source": [
        "# TASK 2.1: CBOW vs Skip-gram comparison\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Gensim most_similar (baseline)\n",
        "def most_similar_gensim(model, query_word, topn=5):\n",
        "    \"\"\"\n",
        "    Thin wrapper around gensim's most_similar for side-by-side comparison.\n",
        "    Returns a list of (word, score). Returns [] if model/word missing.\n",
        "    \"\"\"\n",
        "    if (model is None) or (query_word not in model.wv):\n",
        "        return []\n",
        "    return [(w, float(s)) for (w, s) in model.wv.most_similar(query_word, topn=topn)]\n",
        "\n",
        "# Coherence: avg pairwise cosine among neighbors\n",
        "def neighbor_coherence(model, word, topn_k=5):\n",
        "    \"\"\"\n",
        "    Average pairwise cosine among the top-k neighbors (unordered pairs).\n",
        "    Returns np.nan if fewer than 2 neighbors are available.\n",
        "    \"\"\"\n",
        "    # Get neighbor words via our manual cosine function\n",
        "    neighbors = [w for w, _ in most_similar_gensim(model, word, topn=topn_k)]\n",
        "    if len(neighbors) < 2:\n",
        "        return np.nan\n",
        "\n",
        "    # Unit vectors for neighbors\n",
        "    kv = model.wv\n",
        "    vecs = np.array([kv[w] for w in neighbors], dtype=float) # (num_words, d)\n",
        "    vecs_norm = np.linalg.norm(vecs, axis=1, keepdims=True)  # (num_words, 1)\n",
        "    # We add a tiny epsilon to the denominator before normalization to avoid division by 0\n",
        "    vecs = vecs / (vecs_norm + 1e-12)\n",
        "\n",
        "    # Plain, clear average over all unordered pairs\n",
        "    total, count = 0.0, 0\n",
        "    n = len(neighbors)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            # TODO: Compute the cosine similarity between each pair of vectors (note that the vectors are already normalized)\n",
        "            total += float(np.___(___[___], ___[___]))\n",
        "            count += 1\n",
        "\n",
        "    return (total / count) if count else np.nan\n",
        "\n",
        "# CBOW vs Skip-gram comparison\n",
        "compare_words = [\"king\", \"queen\", \"computer\", \"happy\", \"river\"]\n",
        "print(\"CBOW vs Skip-gram — neighbors, overlap, and coherence\")\n",
        "\n",
        "for w in compare_words:\n",
        "    cb = most_similar_gensim(cbow_model_gensim, w, topn=5)\n",
        "    sg = most_similar_gensim(sg_model_gensim,  w, topn=5)\n",
        "\n",
        "    cb_words = [x for x, _ in cb]\n",
        "    sg_words = [x for x, _ in sg]\n",
        "    # TODO: Find the overlap between the manual and gensim sets of neighboring words\n",
        "    overlap = sorted(___(___) & ___(___))\n",
        "    jaccard = (len(overlap) / len(set(cb_words) | set(sg_words))) if (cb_words and sg_words) else 0.0\n",
        "\n",
        "    cb_coh = neighbor_coherence(cbow_model_gensim, w, topn_k=5)\n",
        "    sg_coh = neighbor_coherence(sg_model_gensim,  w, topn_k=5)\n",
        "\n",
        "    print(f\"\\n  ▶ Word: {w}\")\n",
        "    print(f\"    Skip-gram : {', '.join([f'{a} ({b:.5f})' for a,b in sg])}\")\n",
        "    print(f\"    CBOW      : {', '.join([f'{a} ({b:.5f})' for a,b in cb])}\")\n",
        "    print(f\"    Overlap   : {overlap}  |  Jaccard={jaccard:.5f}\")\n",
        "    print(f\"    Coherence : CBOW≈{cb_coh:.5f}  |  SG≈{sg_coh:.5f}\")\n",
        "\n",
        "print()\n",
        "print(\"Discussion and Insights\")\n",
        "print(\"- Skip-gram often yields sharper, specific neighbors (rare-word friendly).\")\n",
        "print(\"- CBOW often yields smoother, frequent-context neighbors.\")\n",
        "print(\"- These are tendencies; the exact behavior depends on corpus, params, etc.\")\n",
        "print(\"- Consider semantic coherence and topical tightness.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 2.2: Skip-gram vs GloVe — Neighbors and MRR\n",
        "\n",
        "We compare **top-5 nearest neighbors** (cosine) for several targets under two models, then quantify how well each model surfaces a small **expected set** of “reasonable neighbors” using **Reciprocal Rank (RR)** and **Mean Reciprocal Rank (MRR)**.\n",
        "\n",
        "**Models compared**\n",
        "- **Skip-gram (PyTorch)** — neighbors from the input table $W_{\\text{in}}$.\n",
        "- **GloVe (PyTorch)** — neighbors from $E = W_{\\text{in}} + W_{\\text{out}}$.\n",
        "\n",
        "**Targets**: `[\"king\", \"computer\", \"happy\", \"river\", \"bridge\"]`\n",
        "\n",
        "**Expected sets** (derived from our toy corpus; used for MRR)\n",
        "- `king` → {`queen`, `kingdom`, `palace`}\n",
        "- `computer` → {`software`, `hardware`, `system`}\n",
        "- `happy` → {`joyful`, `cheerful`, `smile`}\n",
        "- `river` → {`bank`, `bridge`, `lake`}\n",
        "- `bridge` → {`river`, `bank`, `boats`}\n",
        "\n",
        "### Metrics\n",
        "- **RR (per target)**: if any expected word appears in the model’s **top-5** list at rank $r$, then $\\text{RR}=1/r$; otherwise $\\text{RR}=0$.\n",
        "- **MRR (per model)**: mean of RR over all targets.\n",
        "\n",
        "> These sets are small, corpus-specific heuristics (not gold labels). They’re only for quick checks on this toy setup.\n"
      ],
      "metadata": {
        "id": "MRDjasQx7eTR"
      },
      "id": "MRDjasQx7eTR"
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2.2 — Neighbors + RR/MRR (SG vs GloVe)\n",
        "\n",
        "from typing import List, Tuple, Dict, Iterable\n",
        "import numpy as np\n",
        "\n",
        "# --- Helpers to pull neighbors from existing models --------------------------\n",
        "def neighbors_skipgram(word: str, topn: int = 5) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Top-n neighbors from the Skip-gram PyTorch model (W_in).\n",
        "    Expects `skg_toy` (SkipGramModelToy) and `most_similar_from_Win` to exist.\n",
        "    \"\"\"\n",
        "    if sg_model_pytorch is not None:\n",
        "        return most_similar_from_Win(word, sg_model_pytorch, topn=topn)\n",
        "    return []\n",
        "\n",
        "def neighbors_glove(word: str, topn: int = 5) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Top-n neighbors from the GloVe PyTorch model (E = W_in + W_out).\n",
        "    Expects `glove_model`, `word_to_index`, `index_to_word`, and `glove_most_similar` to exist.\n",
        "    \"\"\"\n",
        "    if (glove_model is not None and\n",
        "      word_to_index is not None and index_to_word is not None):\n",
        "        return glove_most_similar(word, glove_model, word_to_index, index_to_word, topn=topn)\n",
        "    return []\n",
        "\n",
        "targets = [\"king\", \"computer\", \"happy\", \"river\", \"bridge\"]\n",
        "\n",
        "# --- Expected sets for RR/MRR (toy, heuristic) --------------------------------\n",
        "expected: Dict[str, set] = {\n",
        "    \"king\":     [\"queen\", \"kingdom\", \"palace\"],\n",
        "    \"computer\": [\"software\", \"hardware\", \"system\"],\n",
        "    \"happy\":    [\"joyful\", \"cheerful\", \"smile\"],\n",
        "    \"river\":    [\"bank\", \"bridge\", \"lake\"],\n",
        "    \"bridge\":   [\"river\", \"bank\", \"boats\"],\n",
        "}\n",
        "\n",
        "# --- RR/MRR utilities ---------------------------------------------------------\n",
        "def reciprocal_rank(pred_names: Iterable[str], expected_set: set) -> Tuple[float, int, str]:\n",
        "    \"\"\"\n",
        "    Return (RR, rank, matched_word) where rank is 1-based on first hit in pred_names.\n",
        "    If no expected item is found, returns (0.0, 0, '').\n",
        "    \"\"\"\n",
        "    for r, w in enumerate(pred_names, start=1):\n",
        "        # TODO: Find the rank of the first match in the expected set for the target word\n",
        "        if ___ in ___:\n",
        "            return 1.0 / ___, r, w\n",
        "    return 0.0, 0, \"\"\n",
        "\n",
        "def pretty_list(pairs: List[Tuple[str, float]]) -> str:\n",
        "    return \", \".join([f\"{w} ({s:.3f})\" for w, s in pairs])\n",
        "\n",
        "# --- Run comparison -----------------------------------------------------------\n",
        "print(\"\\n=== Nearest neighbors + RR/MRR (Skip-gram vs GloVe) ===\")\n",
        "sg_rrs, gl_rrs = [], []\n",
        "\n",
        "for t in targets:\n",
        "    sg_neighbors = neighbors_skipgram(t, topn=5)\n",
        "    gl_neighbors = neighbors_glove(t, topn=5)\n",
        "\n",
        "    sg_preds = [w for w, _ in sg_neighbors]\n",
        "    gl_preds = [w for w, _ in gl_neighbors]\n",
        "\n",
        "    rr_sg, rank_sg, hit_sg = reciprocal_rank(sg_preds, expected.get(t, set()))\n",
        "    rr_gl, rank_gl, hit_gl = reciprocal_rank(gl_preds, expected.get(t, set()))\n",
        "\n",
        "    sg_rrs.append(rr_sg)\n",
        "    gl_rrs.append(rr_gl)\n",
        "\n",
        "    print(f\"\\nTarget: {t}\")\n",
        "    print(f\"  Skip-gram top-5: [{pretty_list(sg_neighbors)}]\")\n",
        "    print(f\"    RR = {rr_sg:.3f}\" + (f\"  (hit='{hit_sg}' at rank {rank_sg})\" if hit_sg else \"  (no expected hit)\"))\n",
        "    print(f\"  GloVe     top-5: [{pretty_list(gl_neighbors)}]\")\n",
        "    print(f\"    RR = {rr_gl:.3f}\" + (f\"  (hit='{hit_gl}' at rank {rank_gl})\" if hit_gl else \"  (no expected hit)\"))\n",
        "\n",
        "# TODO: Find the mean of reciprocal ranks previously calculated\n",
        "mrr_sg = float(np.___(___)) if ___ else 0.0\n",
        "mrr_gl = float(np.___(___)) if ___ else 0.0\n",
        "\n",
        "print(\"\\n--- MRR over all targets ---\")\n",
        "print(f\"  Skip-gram MRR: {mrr_sg:.3f}\")\n",
        "print(f\"  GloVe     MRR: {mrr_gl:.3f}\")\n",
        "\n",
        "# Quick comparison line\n",
        "better = \"Skip-gram\" if mrr_sg > mrr_gl else (\"GloVe\" if mrr_gl > mrr_sg else \"Tied\")\n",
        "print(f\"\\nResult: {better} has higher MRR on this toy dataset.\")\n"
      ],
      "metadata": {
        "id": "R9daMsZu7euX"
      },
      "id": "R9daMsZu7euX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8bf739af",
      "metadata": {
        "id": "8bf739af"
      },
      "source": [
        "## 7. Summary\n",
        "\n",
        "**What You Practiced**\n",
        "- Built one-hot vectors; compared and calculated **cosine similarity**.\n",
        "- Trained **Skip-gram** and **CBOW** Word2Vec; explored neighbors and **analogy arithmetic**.\n",
        "- Visualized embeddings with **t-SNE**.\n",
        "- Implemented a **tiny GloVe** on co-occurrences.\n",
        "- Compared neighbors across **Skip-gram / CBOW / GloVe** and discussed differences.\n",
        "\n",
        "**Key Takeaways**\n",
        "- One-hot is simple but lacks semantics → embeddings provide dense, shared statistics.\n",
        "- **Cosine** is the go-to similarity for embeddings (direction matters).\n",
        "- **Skip-gram** (rare words) vs **CBOW** (frequent words) trade-offs.\n",
        "- **GloVe** leverages **global** statistics; Word2Vec focuses on **local** contexts.\n",
        "- Visualization + nearest-neighbor inspection are useful sanity checks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}