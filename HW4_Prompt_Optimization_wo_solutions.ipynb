{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita2002/LLMS/blob/main/HW4_Prompt_Optimization_wo_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMbrp6HAlh4P"
      },
      "source": [
        "# Homework 4: Generative Automatic Prompt Optimization & Interpretability\n",
        "\n",
        "This homework guides you through building an \"Automatic Prompt Engineer\" (APE) — a system where an LLM iteratively improves its own prompts based on error analysis. You will then use white-box interpretability techniques to understand *why* the optimized prompt works better.\n",
        "\n",
        "**Total Points: 20**\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By completing this homework, you will:\n",
        "- Understand how to build an automatic prompt optimization system using LLMs\n",
        "- Learn to evaluate prompt performance systematically\n",
        "- Gain experience with white-box interpretability techniques (attention patterns, confidence analysis)\n",
        "- Analyze why certain prompts work better than others\n",
        "- Practice error analysis and iterative improvement\n",
        "\n",
        "## Expected Outcomes\n",
        "\n",
        "After completing this homework, you should be able to:\n",
        "1. Generate and evaluate prompts programmatically\n",
        "2. Use meta-prompting to improve prompts iteratively\n",
        "3. Extract and visualize attention patterns from transformer models\n",
        "4. Analyze model confidence through logit analysis\n",
        "5. Draw meaningful insights from interpretability tools\n",
        "\n",
        "**Instructions:**\n",
        "1. Complete all tasks in this notebook\n",
        "2. Ensure your code runs without errors\n",
        "3. Submit both this notebook and any additional files created\n",
        "4. Write clear explanations for your approach\n",
        "5. Fill in the `## YOUR_CODE_STARTS_HERE` / `## YOUR_CODE_ENDS_HERE` sections\n",
        "6. Run the answer checkpoints after each task to verify your implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQlxZylhlh4Q"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhfKx8mflh4R"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers torch datasets\n",
        "%pip install -q matplotlib seaborn numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1lkjw4hlh4R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import random\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKE4nwXXlh4S"
      },
      "outputs": [],
      "source": [
        "# Using Gemma-2B-IT - better at arithmetic tasks than Phi-2\n",
        "# If you prefer Phi-2, uncomment the alternative code below\n",
        "print(\"Loading google/gemma-2b-it (excellent for math, supports output_attentions)...\")\n",
        "model_name = \"google/gemma-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Gemma needs pad_token set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device.type == \"cuda\" else None,\n",
        "    # Force eager attention for output_attentions support\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "if device.type == \"cpu\":\n",
        "    model = model.to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3gZM0h_lh4S"
      },
      "source": [
        "## Dataset: Arithmetic Word Problems\n",
        "\n",
        "We'll use a synthetic dataset of simple arithmetic word problems. This task is perfect for prompt optimization because:\n",
        "1. Clear right/wrong answers (easy to evaluate)\n",
        "2. The way you phrase the instruction matters a lot\n",
        "3. Errors are interpretable (wrong numbers, wrong operations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAsb94Tflh4S"
      },
      "outputs": [],
      "source": [
        "# Synthetic dataset of arithmetic word problems\n",
        "def generate_arithmetic_dataset(n_samples: int = 100, seed: int = 42) -> List[Dict]:\n",
        "    \"\"\"Generate synthetic arithmetic word problems.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    templates = [\n",
        "        (\"Alice has {a} apples. Bob gives her {b} more apples. How many apples does Alice have now?\",\n",
        "         lambda a, b: a + b),\n",
        "        (\"There are {a} birds on a tree. {b} birds fly away. How many birds are left?\",\n",
        "         lambda a, b: a - b),\n",
        "        (\"A store has {a} boxes. Each box contains {b} items. How many items are there in total?\",\n",
        "         lambda a, b: a * b),\n",
        "        (\"Tom has {a} candies. He wants to share them equally among {b} friends. How many candies does each friend get?\",\n",
        "         lambda a, b: a // b),\n",
        "        (\"{a} students are in a class. {b} more students join. What is the total number of students?\",\n",
        "         lambda a, b: a + b),\n",
        "        (\"A farmer has {a} cows. He sells {b} of them. How many cows does he have left?\",\n",
        "         lambda a, b: a - b),\n",
        "    ]\n",
        "\n",
        "    dataset = []\n",
        "    for i in range(n_samples):\n",
        "        template, operation = random.choice(templates)\n",
        "\n",
        "        # Generate appropriate numbers based on operation\n",
        "        if 'fly away' in template or 'sells' in template or 'left' in template:\n",
        "            # Subtraction: ensure a >= b\n",
        "            a = random.randint(10, 50)\n",
        "            b = random.randint(1, a)\n",
        "        elif 'share' in template or 'equally' in template:\n",
        "            # Division: ensure clean division\n",
        "            b = random.randint(2, 10)\n",
        "            a = b * random.randint(2, 10)\n",
        "        elif 'boxes' in template or 'contains' in template:\n",
        "            # Multiplication: keep numbers small\n",
        "            a = random.randint(2, 12)\n",
        "            b = random.randint(2, 12)\n",
        "        else:\n",
        "            # Addition\n",
        "            a = random.randint(5, 50)\n",
        "            b = random.randint(5, 50)\n",
        "\n",
        "        question = template.format(a=a, b=b)\n",
        "        answer = str(operation(a, b))\n",
        "\n",
        "        dataset.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": answer\n",
        "        })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Generate dataset\n",
        "full_dataset = generate_arithmetic_dataset(n_samples=120, seed=42)\n",
        "\n",
        "# Split into train (for error analysis) and validation (for evaluation)\n",
        "train_data = full_dataset[:60]\n",
        "val_data = full_dataset[60:100]\n",
        "test_data = full_dataset[100:]\n",
        "\n",
        "print(f\"Train set: {len(train_data)} examples\")\n",
        "print(f\"Validation set: {len(val_data)} examples\")\n",
        "print(f\"Test set: {len(test_data)} examples\")\n",
        "print(\"\\nSample questions:\")\n",
        "for i in range(3):\n",
        "    print(f\"  Q: {train_data[i]['question']}\")\n",
        "    print(f\"  A: {train_data[i]['answer']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WovmPCxNlh4T"
      },
      "source": [
        "---\n",
        "\n",
        "## TASK 1: The Evaluation Framework (5 points)\n",
        "\n",
        "Build the infrastructure to evaluate how well a prompt performs on our arithmetic task.\n",
        "\n",
        "**1.1**: *Generate Response* — Implement a function to generate model responses given a prompt template  \n",
        "**1.2**: *Evaluate Prompt* — Implement evaluation that returns accuracy and failed cases  \n",
        "**1.3**: *Baseline Testing* — Establish baseline performance with a naive prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SwmREnRlh4T"
      },
      "source": [
        "### Task 1.1: Generate Response Function\n",
        "\n",
        "**Goal**: Implement a function that generates model responses given a prompt template and question.\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "- Format the prompt template by inserting the question into the `{question}` placeholder\n",
        "- Tokenize the formatted prompt and prepare it for the model (move to correct device)\n",
        "- Generate a response using the model with greedy decoding (deterministic)\n",
        "- Extract only the newly generated tokens (not the input prompt tokens)\n",
        "- Decode the generated tokens back to text\n",
        "- Return a dictionary with at least a `'response'` key containing the generated text\n",
        "- Optionally include `'logits'` and `'attentions'` in the dictionary if requested via function parameters\n",
        "\n",
        "**Important considerations**:\n",
        "- For decoder-only models, `outputs.sequences` contains both input and generated tokens - you need to extract only the new tokens\n",
        "- Handle `pad_token_id` appropriately (use `eos_token_id` if `pad_token_id` is None)\n",
        "- Clean up the response text (strip whitespace, handle newlines appropriately)\n",
        "\n",
        "**Expected return format**: A dictionary with at least `{'response': str}`. The response should be a non-empty string containing the model's generated text.\n",
        "\n",
        "**Success criteria**:\n",
        "- Function returns a dictionary\n",
        "- Dictionary contains a `'response'` key with a non-empty string\n",
        "- For arithmetic problems, the response should ideally contain a number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDNVyQNwlh4T"
      },
      "outputs": [],
      "source": [
        "def generate_response(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_template: str,\n",
        "    question: str,\n",
        "    max_new_tokens: int = 200,  # Reduced for arithmetic (just need a number)\n",
        "    return_logits: bool = False,\n",
        "    return_attentions: bool = False\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate a response from the model given a prompt template and question.\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        tokenizer: The tokenizer\n",
        "        prompt_template: A string with {question} placeholder (e.g., \"Solve: {question}\")\n",
        "        question: The actual question to insert\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        return_logits: Whether to return the logits\n",
        "        return_attentions: Whether to return attention weights\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'response' (str), and optionally 'logits' and 'attentions'\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6U2lpg-lh4U"
      },
      "outputs": [],
      "source": [
        "# Test the generate_response function\n",
        "test_prompt = \"Answer the math question: {question}\"\n",
        "test_question = \"Alice has 5 apples. Bob gives her 3 more apples. How many apples does Alice have now?\"\n",
        "\n",
        "result = generate_response(model, tokenizer, test_prompt, test_question)\n",
        "print(f\"Prompt template: {test_prompt}\")\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Response: {result['response']}\")\n",
        "## RESULT_CHECKING_POINT -> Should output a number (ideally '8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQSqf12cPq3h"
      },
      "outputs": [],
      "source": [
        "# Answer Checkpoint: Task 1.1 - Generate Response\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATING: generate_response function\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks_passed = 0\n",
        "checks_failed = 0\n",
        "\n",
        "# Test 1: Function returns dictionary\n",
        "try:\n",
        "    test_result = generate_response(model, tokenizer, \"Test: {question}\", \"What is 2+2?\")\n",
        "    if isinstance(test_result, dict):\n",
        "        print(\"✓ PASS: Function returns dictionary\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(f\"✗ FAIL: Function should return a dictionary, got {type(test_result)}\")\n",
        "        checks_failed += 1\n",
        "except Exception as e:\n",
        "    print(f\"✗ FAIL: Function raised an error: {e}\")\n",
        "    checks_failed += 1\n",
        "    test_result = None\n",
        "\n",
        "# Test 2: Dictionary has 'response' key\n",
        "if test_result is not None:\n",
        "    try:\n",
        "        if 'response' in test_result:\n",
        "            print(\"✓ PASS: Dictionary contains 'response' key\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"✗ FAIL: Dictionary should contain 'response' key. Keys found: {list(test_result.keys())}\")\n",
        "            checks_failed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAIL: Error checking 'response' key: {e}\")\n",
        "        checks_failed += 1\n",
        "\n",
        "# Test 3: Response is non-empty string\n",
        "if test_result is not None and 'response' in test_result:\n",
        "    try:\n",
        "        response = test_result['response']\n",
        "        if isinstance(response, str) and len(response) > 0:\n",
        "            print(\"✓ PASS: Response is a non-empty string\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"✗ FAIL: Response should be a non-empty string. Got: {type(response)}, length: {len(response) if isinstance(response, str) else 'N/A'}\")\n",
        "            checks_failed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAIL: Error checking response type: {e}\")\n",
        "        checks_failed += 1\n",
        "\n",
        "# Test 4: Response contains a number (for arithmetic problems)\n",
        "if test_result is not None and 'response' in test_result:\n",
        "    try:\n",
        "        response = test_result['response']\n",
        "        import re\n",
        "        has_number = bool(re.search(r'\\d+', response))\n",
        "        if has_number:\n",
        "            print(\"✓ PASS: Response contains a number\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(\"⚠ WARNING: Response doesn't contain a number - this might be okay for some prompts\")\n",
        "            print(f\"   Response was: '{response[:100]}...'\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ WARNING: Could not check for numbers: {e}\")\n",
        "\n",
        "# Test 5: Test with the actual test case from the notebook\n",
        "try:\n",
        "    test_prompt = \"Answer the math question: {question}\"\n",
        "    test_question = \"Alice has 5 apples. Bob gives her 3 more apples. How many apples does Alice have now?\"\n",
        "    result = generate_response(model, tokenizer, test_prompt, test_question)\n",
        "    if isinstance(result, dict) and 'response' in result:\n",
        "        print(\"✓ PASS: Function works with test case from notebook\")\n",
        "        checks_passed += 1\n",
        "        # Check if response contains the expected answer (8)\n",
        "        response = result['response']\n",
        "        if '8' in response or 'eight' in response.lower():\n",
        "            print(\"✓ PASS: Response contains expected answer (8)\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"⚠ INFO: Response doesn't clearly contain '8', but this may vary by model\")\n",
        "            print(f\"   Response: '{response[:100]}...'\")\n",
        "    else:\n",
        "        print(\"✗ FAIL: Function doesn't work with test case\")\n",
        "        checks_failed += 1\n",
        "except Exception as e:\n",
        "    print(f\"✗ FAIL: Error with test case: {e}\")\n",
        "    checks_failed += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if checks_failed == 0:\n",
        "    print(f\"All checks passed! ✓ ({checks_passed} checks)\")\n",
        "else:\n",
        "    print(f\"Some checks failed: {checks_passed} passed, {checks_failed} failed\")\n",
        "    print(\"Please review your implementation and try again.\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBb2riDLlh4U"
      },
      "source": [
        "### Task 1.2: Evaluate Prompt Function\n",
        "\n",
        "**Goal**: Implement functions to extract numbers from text and evaluate prompt performance on a dataset.\n",
        "\n",
        "#### Part A: `extract_number` function\n",
        "\n",
        "Extract the first numeric answer from a text response. The model might respond in various formats (e.g., `\"8\"`, `\"The answer is 8\"`, `\"8 apples\"`, `\"Alice has 8 apples\"`). The function should handle these cases and return the first number found as a string, or `None` if no number is present.\n",
        "\n",
        "**Expected return**: A string containing the number (e.g., `\"8\"`) or `None` if no number found.\n",
        "\n",
        "#### Part B: `evaluate_prompt` function\n",
        "\n",
        "Evaluate how well a prompt performs on a dataset by:\n",
        "- Running the prompt on each example using `generate_response()`\n",
        "- Extracting the predicted number from each response using `extract_number()`\n",
        "- Comparing predictions to target answers (exact string match)\n",
        "- Calculating accuracy as `correct_predictions / total_examples`\n",
        "- Collecting failed cases with information about each error\n",
        "\n",
        "**Expected return format**: A tuple `(float, List[Dict])` where:\n",
        "- First element is accuracy (0.0 to 1.0)\n",
        "- Second element is a list of dictionaries, each containing failed case information with keys: `'question'`, `'prediction'`, `'target'`, `'raw_response'`\n",
        "\n",
        "**Success criteria**:\n",
        "- `extract_number` handles various text formats correctly\n",
        "- `evaluate_prompt` returns a tuple of (float, list)\n",
        "- Accuracy is between 0.0 and 1.0\n",
        "- Failed cases list contains dictionaries with the required keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85tyhjUolh4U"
      },
      "outputs": [],
      "source": [
        "def extract_number(text: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extract the first number from a text response.\n",
        "    Handles cases like '8', 'The answer is 8', '8 apples', etc.\n",
        "    \"\"\"\n",
        "    # Clean up text - remove special characters that might confuse extraction\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove common prefixes\n",
        "    prefixes = [\"Answer:\", \"The answer is\", \"Answer is\", \"The answer:\", \"Result:\", \"Result is\"]\n",
        "    for prefix in prefixes:\n",
        "        if text.lower().startswith(prefix.lower()):\n",
        "            text = text[len(prefix):].strip()\n",
        "\n",
        "    # Find all numbers in the text\n",
        "    numbers = re.findall(r'-?\\d+', text)\n",
        "    if numbers:\n",
        "        # Return the first number that looks like an answer (not too large, positive for our problems)\n",
        "        for num in numbers:\n",
        "            num_int = int(num)\n",
        "            # For arithmetic word problems, answers are typically reasonable positive numbers\n",
        "            if 0 <= num_int <= 10000:  # Reasonable range for our problems\n",
        "                return num\n",
        "        # If no reasonable number found, return first number anyway\n",
        "        return numbers[0]\n",
        "    return None\n",
        "\n",
        "\n",
        "def evaluate_prompt(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_template: str,\n",
        "    dataset: List[Dict],\n",
        "    verbose: bool = False\n",
        ") -> Tuple[float, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Evaluate a prompt template on a dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        tokenizer: The tokenizer\n",
        "        prompt_template: Template with {question} placeholder\n",
        "        dataset: List of {'question': str, 'answer': str} dicts\n",
        "        verbose: Whether to print progress\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (accuracy: float, failed_cases: List[Dict])\n",
        "        failed_cases contains {'question', 'prediction', 'target'} for each error\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    return accuracy, failed_cases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifx-f9Q8lh4U"
      },
      "outputs": [],
      "source": [
        "# Test the evaluate_prompt function on a small subset\n",
        "test_prompt = \"{question}\"\n",
        "accuracy, failures = evaluate_prompt(model, tokenizer, test_prompt, val_data[:10], verbose=True)\n",
        "print(f\"\\nAccuracy: {accuracy:.2%}\")\n",
        "print(f\"Number of failures: {len(failures)}\")\n",
        "if failures:\n",
        "    print(f\"\\nSample failure:\")\n",
        "    print(f\"  Question: {failures[0]['question']}\")\n",
        "    print(f\"  Predicted: {failures[0]['prediction']}\")\n",
        "    print(f\"  Target: {failures[0]['target']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lcCkeLIPq3h"
      },
      "outputs": [],
      "source": [
        "# Answer Checkpoint: Task 1.2 - Evaluate Prompt\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATING: extract_number and evaluate_prompt functions\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks_passed = 0\n",
        "checks_failed = 0\n",
        "\n",
        "# Test 1: extract_number handles various formats\n",
        "test_cases = [\n",
        "    (\"8\", \"8\"),\n",
        "    (\"The answer is 8\", \"8\"),\n",
        "    (\"8 apples\", \"8\"),\n",
        "    (\"Alice has 8 apples\", \"8\"),\n",
        "    (\"Result: 42\", \"42\"),\n",
        "    (\"No number here\", None),\n",
        "    (\"\", None),\n",
        "]\n",
        "\n",
        "print(\"\\nTesting extract_number function:\")\n",
        "for text, expected in test_cases:\n",
        "    try:\n",
        "        result = extract_number(text)\n",
        "        if result == expected:\n",
        "            print(f\"✓ PASS: extract_number('{text[:30]}...') = {result}\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"✗ FAIL: extract_number('{text[:30]}...') = {result}, expected {expected}\")\n",
        "            checks_failed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAIL: extract_number('{text[:30]}...') raised error: {e}\")\n",
        "        checks_failed += 1\n",
        "\n",
        "# Test 2: evaluate_prompt returns tuple\n",
        "print(\"\\nTesting evaluate_prompt function:\")\n",
        "try:\n",
        "    test_prompt = \"{question}\\nAnswer:\"\n",
        "    accuracy, failures = evaluate_prompt(model, tokenizer, test_prompt, val_data[:5], verbose=False)\n",
        "\n",
        "    # Check return type\n",
        "    if isinstance(accuracy, (int, float)) and isinstance(failures, list):\n",
        "        print(\"✓ PASS: evaluate_prompt returns tuple of (number, list)\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(f\"✗ FAIL: evaluate_prompt should return (float, list), got ({type(accuracy)}, {type(failures)})\")\n",
        "        checks_failed += 1\n",
        "\n",
        "    # Check accuracy range\n",
        "    if 0.0 <= accuracy <= 1.0:\n",
        "        print(f\"✓ PASS: Accuracy is in valid range [0.0, 1.0]: {accuracy:.2%}\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(f\"✗ FAIL: Accuracy should be between 0.0 and 1.0, got {accuracy}\")\n",
        "        checks_failed += 1\n",
        "\n",
        "    # Check failed cases structure\n",
        "    if len(failures) > 0:\n",
        "        first_failure = failures[0]\n",
        "        required_keys = ['question', 'prediction', 'target', 'raw_response']\n",
        "        if all(key in first_failure for key in required_keys):\n",
        "            print(\"✓ PASS: Failed cases have correct structure\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            missing = [k for k in required_keys if k not in first_failure]\n",
        "            print(f\"✗ FAIL: Failed cases missing keys: {missing}\")\n",
        "            checks_failed += 1\n",
        "    else:\n",
        "        print(\"⚠ INFO: No failures found (this is okay if accuracy is 100%)\")\n",
        "        checks_passed += 1\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ FAIL: evaluate_prompt raised error: {e}\")\n",
        "    checks_failed += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if checks_failed == 0:\n",
        "    print(f\"All checks passed! ✓ ({checks_passed} checks)\")\n",
        "else:\n",
        "    print(f\"Some checks failed: {checks_passed} passed, {checks_failed} failed\")\n",
        "    print(\"Please review your implementation and try again.\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH_viygUlh4V"
      },
      "source": [
        "### Task 1.3: Baseline Evaluation\n",
        "\n",
        "**Goal**: Test multiple baseline prompts to establish a starting point for optimization.\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "- Define a list of simple, naive prompt templates to test (at least 3-5 prompts)\n",
        "- Evaluate each prompt on the validation set using `evaluate_prompt()`\n",
        "- Store results (accuracy and failures) for each prompt in a dictionary\n",
        "- Select the prompt with the highest accuracy as the starting point for optimization\n",
        "\n",
        "**What to look for**:\n",
        "- Different prompts will have different accuracies\n",
        "- The best baseline gives us a reference point - we want to improve upon it\n",
        "- Typical baseline accuracies for arithmetic problems: 20-60% (varies by model and prompt quality)\n",
        "\n",
        "**Success criteria**:\n",
        "- At least 3-5 baseline prompts tested\n",
        "- Best baseline accuracy > 0% (model is working)\n",
        "- Results dictionary structure is correct with 'accuracy' and 'failures' keys\n",
        "- Best baseline prompt selected and stored\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW4hYpaylh4V"
      },
      "outputs": [],
      "source": [
        "# Define several baseline prompts to test (intentionally weak to show optimization improvement)\n",
        "baseline_prompts = [\n",
        "    \"{question}\",  # Just the question, no instruction\n",
        "    \"{question}?\",  # Just question mark\n",
        "    \"Here: {question}\",  # Ambiguous\n",
        "    \"{question} answer\",  # Unclear format\n",
        "    \"Math: {question}\",  # Minimal context\n",
        "]\n",
        "\n",
        "print(\"Evaluating baseline prompts on validation set...\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "baseline_results = {}\n",
        "for prompt in baseline_prompts:\n",
        "    accuracy, failures = evaluate_prompt(model, tokenizer, prompt, val_data)\n",
        "    baseline_results[prompt] = {'accuracy': accuracy, 'failures': failures}\n",
        "    print(f\"Prompt: '{prompt[:50]}...'\" if len(prompt) > 50 else f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Accuracy: {accuracy:.2%} ({int(accuracy * len(val_data))}/{len(val_data)})\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "# Select the best baseline as our starting point\n",
        "best_baseline_prompt = max(baseline_results.keys(), key=lambda p: baseline_results[p]['accuracy'])\n",
        "best_baseline_accuracy = baseline_results[best_baseline_prompt]['accuracy']\n",
        "best_baseline_failures = baseline_results[best_baseline_prompt]['failures']\n",
        "\n",
        "print(f\"\\nBest baseline prompt: '{best_baseline_prompt}'\")\n",
        "print(f\"Best baseline accuracy: {best_baseline_accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB0GIgNMPq3i"
      },
      "outputs": [],
      "source": [
        "# Answer Checkpoint: Task 1.3 - Baseline Evaluation\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATING: Baseline evaluation results\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks_passed = 0\n",
        "checks_failed = 0\n",
        "\n",
        "# Check if baseline_results exists and has correct structure\n",
        "try:\n",
        "    if 'baseline_results' in globals():\n",
        "        print(\"✓ PASS: baseline_results variable exists\")\n",
        "        checks_passed += 1\n",
        "\n",
        "        # Check that at least 3 prompts were tested\n",
        "        if len(baseline_results) >= 3:\n",
        "            print(f\"✓ PASS: At least 3 baseline prompts tested ({len(baseline_results)} prompts)\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"✗ FAIL: Should test at least 3 prompts, found {len(baseline_results)}\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check structure of results\n",
        "        for prompt, result in baseline_results.items():\n",
        "            if isinstance(result, dict) and 'accuracy' in result and 'failures' in result:\n",
        "                checks_passed += 1\n",
        "                break\n",
        "            else:\n",
        "                print(f\"✗ FAIL: baseline_results should contain dicts with 'accuracy' and 'failures' keys\")\n",
        "                checks_failed += 1\n",
        "                break\n",
        "\n",
        "        # Check best baseline variables\n",
        "        if 'best_baseline_prompt' in globals() and 'best_baseline_accuracy' in globals():\n",
        "            print(\"✓ PASS: best_baseline_prompt and best_baseline_accuracy variables exist\")\n",
        "            checks_passed += 1\n",
        "\n",
        "            # Check accuracy is valid\n",
        "            if isinstance(best_baseline_accuracy, (int, float)) and 0.0 <= best_baseline_accuracy <= 1.0:\n",
        "                print(f\"✓ PASS: Best baseline accuracy is valid: {best_baseline_accuracy:.2%}\")\n",
        "                checks_passed += 1\n",
        "\n",
        "                if best_baseline_accuracy > 0:\n",
        "                    print(\"✓ PASS: Best baseline accuracy > 0% (model is working)\")\n",
        "                    checks_passed += 1\n",
        "                else:\n",
        "                    print(\"⚠ WARNING: Best baseline accuracy is 0% - model may not be working correctly\")\n",
        "            else:\n",
        "                print(f\"✗ FAIL: best_baseline_accuracy should be between 0.0 and 1.0, got {best_baseline_accuracy}\")\n",
        "                checks_failed += 1\n",
        "        else:\n",
        "            print(\"✗ FAIL: best_baseline_prompt and/or best_baseline_accuracy not defined\")\n",
        "            checks_failed += 1\n",
        "    else:\n",
        "        print(\"✗ FAIL: baseline_results variable not found. Did you run the baseline evaluation cell?\")\n",
        "        checks_failed += 1\n",
        "except Exception as e:\n",
        "    print(f\"✗ FAIL: Error checking baseline results: {e}\")\n",
        "    checks_failed += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if checks_failed == 0:\n",
        "    print(f\"All checks passed! ✓ ({checks_passed} checks)\")\n",
        "else:\n",
        "    print(f\"Some checks failed: {checks_passed} passed, {checks_failed} failed\")\n",
        "    print(\"Please review your implementation and try again.\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1juwnxeilh4V"
      },
      "source": [
        "---\n",
        "\n",
        "## TASK 2: Generative Optimization Loop (7 points)\n",
        "\n",
        "Now we'll build the core APE system: a feedback loop where the model analyzes its errors and generates improved prompts.\n",
        "\n",
        "**2.1**: *Error Analysis Prompt* — Construct a meta-prompt that shows the model its failures  \n",
        "**2.2**: *Candidate Generation* — Generate multiple candidate prompts  \n",
        "**2.3**: *Optimization Loop* — Implement the full iterative optimization cycle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRh1OLivlh4V"
      },
      "source": [
        "### Task 2.1: Error Analysis Meta-Prompt\n",
        "\n",
        "**Goal**: Create a \"meta-prompt\" that instructs the model to analyze its errors and suggest an improved prompt.\n",
        "\n",
        "**What is a meta-prompt?**\n",
        "A meta-prompt is a prompt that asks the model to generate another prompt. We'll show the model its failures and ask it to write a better instruction prompt.\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "- Randomly sample `n_examples` from the `failed_cases` list\n",
        "- Format each failed case to show: the question, the model's prediction, and the correct answer\n",
        "- Construct a meta-prompt string that:\n",
        "  - Explains the context (using an AI to solve math word problems)\n",
        "  - Shows the current prompt being used\n",
        "  - Presents the failed examples in a clear format\n",
        "  - Asks the model to write a better prompt\n",
        "  - **Important**: Emphasize that the new prompt MUST contain `{question}` as a placeholder\n",
        "  - Instruct the model to output ONLY the new prompt, nothing else\n",
        "\n",
        "**Example meta-prompt structure**:\n",
        "```\n",
        "I am using an AI to solve math word problems. The current instruction prompt is:\n",
        "\n",
        "\"{current_prompt}\"\n",
        "\n",
        "However, the AI made mistakes on these examples:\n",
        "Example 1:\n",
        "  Question: [question]\n",
        "  Model predicted: [prediction]\n",
        "  Correct answer: [target]\n",
        "...\n",
        "\n",
        "Write a better instruction prompt that would help the AI answer correctly.\n",
        "The prompt must contain {question} as a placeholder.\n",
        "Output only the new prompt, nothing else.\n",
        "\n",
        "New prompt:\n",
        "```\n",
        "\n",
        "**Success criteria**:\n",
        "- Meta-prompt contains the current prompt\n",
        "- Meta-prompt contains at least one failed example\n",
        "- Meta-prompt asks for an improved prompt\n",
        "- Meta-prompt explicitly mentions the `{question}` placeholder requirement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRj7BQYelh4V"
      },
      "outputs": [],
      "source": [
        "def create_meta_prompt(\n",
        "    current_prompt: str,\n",
        "    failed_cases: List[Dict],\n",
        "    n_examples: int = 3\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Create a meta-prompt for the model to analyze errors and suggest a better prompt.\n",
        "\n",
        "    Args:\n",
        "        current_prompt: The prompt template currently being used\n",
        "        failed_cases: List of failed examples with 'question', 'prediction', 'target'\n",
        "        n_examples: Number of failed examples to include\n",
        "\n",
        "    Returns:\n",
        "        A meta-prompt string asking the model to improve the prompt\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-3ZnMmePq3i"
      },
      "outputs": [],
      "source": [
        "# Answer Checkpoint: Task 2.1 - Meta-Prompt Creation\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATING: create_meta_prompt function\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks_passed = 0\n",
        "checks_failed = 0\n",
        "\n",
        "try:\n",
        "    # Test with sample data\n",
        "    if 'best_baseline_prompt' in globals() and 'best_baseline_failures' in globals():\n",
        "        test_meta = create_meta_prompt(best_baseline_prompt, best_baseline_failures[:5], n_examples=2)\n",
        "\n",
        "        # Check 1: Returns a string\n",
        "        if isinstance(test_meta, str):\n",
        "            print(\"✓ PASS: Function returns a string\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"✗ FAIL: Function should return string, got {type(test_meta)}\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 2: Contains current prompt\n",
        "        if best_baseline_prompt in test_meta or best_baseline_prompt.replace('\\n', ' ') in test_meta:\n",
        "            print(\"✓ PASS: Meta-prompt contains current prompt\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(\"✗ FAIL: Meta-prompt should contain the current prompt\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 3: Contains failed examples\n",
        "        if len(best_baseline_failures) > 0:\n",
        "            first_failure_q = best_baseline_failures[0].get('question', '')\n",
        "            if first_failure_q and first_failure_q in test_meta:\n",
        "                print(\"✓ PASS: Meta-prompt contains failed examples\")\n",
        "                checks_passed += 1\n",
        "            else:\n",
        "                print(\"⚠ WARNING: Meta-prompt may not contain failed examples\")\n",
        "\n",
        "        # Check 4: Mentions {question} placeholder requirement\n",
        "        if '{question}' in test_meta or 'question' in test_meta.lower():\n",
        "            print(\"✓ PASS: Meta-prompt mentions question placeholder\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(\"⚠ WARNING: Meta-prompt should mention {question} placeholder requirement\")\n",
        "\n",
        "        # Check 5: Asks for improved prompt\n",
        "        if any(word in test_meta.lower() for word in ['better', 'improve', 'new prompt', 'write']):\n",
        "            print(\"✓ PASS: Meta-prompt asks for improved prompt\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(\"⚠ WARNING: Meta-prompt should ask for an improved prompt\")\n",
        "    else:\n",
        "        print(\"⚠ SKIP: best_baseline_prompt or best_baseline_failures not found. Run Task 1.3 first.\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ FAIL: Error testing create_meta_prompt: {e}\")\n",
        "    checks_failed += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if checks_failed == 0:\n",
        "    print(f\"All checks passed! ✓ ({checks_passed} checks)\")\n",
        "else:\n",
        "    print(f\"Some checks failed: {checks_passed} passed, {checks_failed} failed\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba7cw6NWlh4V"
      },
      "outputs": [],
      "source": [
        "# Test the meta-prompt creation\n",
        "sample_meta_prompt = create_meta_prompt(best_baseline_prompt, best_baseline_failures, n_examples=2)\n",
        "print(\"Sample Meta-Prompt:\")\n",
        "print(\"=\" * 70)\n",
        "print(sample_meta_prompt)\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plj11Iu2lh4V"
      },
      "source": [
        "### Task 2.2: Candidate Prompt Generation\n",
        "\n",
        "**Goal**: Generate multiple diverse candidate prompts from the meta-prompt using sampling.\n",
        "\n",
        "**Step-by-step instructions**:\n",
        "\n",
        "1. **Tokenize the meta-prompt**: Convert the meta-prompt string to token IDs, similar to `generate_response()`\n",
        "\n",
        "2. **Generate multiple candidates**: Loop to generate `n_candidates` valid prompts:\n",
        "   - Use `model.generate()` with `do_sample=True` for diversity\n",
        "   - Set `temperature` (e.g., 0.7) - higher = more diverse, lower = more focused\n",
        "   - Use `top_p=0.9` for nucleus sampling (helps with quality)\n",
        "   - Generate extra candidates (e.g., `n_candidates * 5`) since some may be invalid\n",
        "\n",
        "3. **Clean and normalize candidates**:\n",
        "   - Decode only the newly generated tokens (not the input)\n",
        "   - Strip whitespace\n",
        "   - Remove surrounding quotes if present (`\"...\"` or `'...'`)\n",
        "   - Normalize placeholder: replace variations like `{example_question}` or `{the_question}` with `{question}`\n",
        "   - If candidate mentions \"question\" but has no placeholder, try to add `{question}`\n",
        "\n",
        "4. **Validate candidates**:\n",
        "   - Must contain `{question}` placeholder\n",
        "   - Must be reasonable length (< 200 characters)\n",
        "   - Must be able to be formatted: `candidate.format(question=\"test\")` should work\n",
        "   - Should not be duplicate of existing candidates\n",
        "   - Should be meaningfully different from current prompt (optional but helpful)\n",
        "\n",
        "5. **Return valid candidates**: Return up to `n_candidates` valid prompts\n",
        "\n",
        "**Key parameters**:\n",
        "- `temperature`: Controls randomness (0.7-0.9 good for diversity)\n",
        "- `top_p`: Nucleus sampling threshold (0.9 is standard)\n",
        "- `max_new_tokens`: Limit generation length (150-200 tokens recommended for complete prompts)\n",
        "\n",
        "**Success criteria**:\n",
        "- Function returns a list of strings\n",
        "- All candidates contain `{question}` placeholder\n",
        "- All candidates can be formatted without KeyError\n",
        "- At least 1 candidate generated (even if fewer than requested)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl_53wnzPq3i"
      },
      "outputs": [],
      "source": [
        "# Answer Checkpoint: Task 2.2 - Candidate Generation\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATING: generate_candidate_prompts function\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks_passed = 0\n",
        "checks_failed = 0\n",
        "\n",
        "try:\n",
        "    if 'sample_meta_prompt' in globals():\n",
        "        test_candidates = generate_candidate_prompts(model, tokenizer, sample_meta_prompt, n_candidates=3, temperature=0.7)\n",
        "\n",
        "        # Check 1: Returns a list\n",
        "        if isinstance(test_candidates, list):\n",
        "            print(\"✓ PASS: Function returns a list\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"✗ FAIL: Function should return list, got {type(test_candidates)}\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 2: All items are strings\n",
        "        if all(isinstance(c, str) for c in test_candidates):\n",
        "            print(\"✓ PASS: All candidates are strings\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(\"✗ FAIL: All candidates should be strings\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 3: All contain {question}\n",
        "        if len(test_candidates) > 0:\n",
        "            all_have_placeholder = all('{question}' in c for c in test_candidates)\n",
        "            if all_have_placeholder:\n",
        "                print(\"✓ PASS: All candidates contain {question} placeholder\")\n",
        "                checks_passed += 1\n",
        "            else:\n",
        "                missing = [i for i, c in enumerate(test_candidates) if '{question}' not in c]\n",
        "                print(f\"✗ FAIL: Candidates {missing} missing {{question}} placeholder\")\n",
        "                checks_failed += 1\n",
        "\n",
        "            # Check 4: All can be formatted\n",
        "            format_errors = []\n",
        "            for i, c in enumerate(test_candidates):\n",
        "                try:\n",
        "                    c.format(question=\"test\")\n",
        "                except KeyError as e:\n",
        "                    format_errors.append(i)\n",
        "\n",
        "            if len(format_errors) == 0:\n",
        "                print(\"✓ PASS: All candidates can be formatted\")\n",
        "                checks_passed += 1\n",
        "            else:\n",
        "                print(f\"✗ FAIL: Candidates {format_errors} cannot be formatted (KeyError)\")\n",
        "                checks_failed += 1\n",
        "\n",
        "            # Check 5: At least 1 candidate\n",
        "            if len(test_candidates) >= 1:\n",
        "                print(f\"✓ PASS: Generated at least 1 candidate ({len(test_candidates)} candidates)\")\n",
        "                checks_passed += 1\n",
        "            else:\n",
        "                print(\"⚠ WARNING: No candidates generated (this may be okay if model is struggling)\")\n",
        "        else:\n",
        "            print(\"⚠ WARNING: No candidates generated\")\n",
        "    else:\n",
        "        print(\"⚠ SKIP: sample_meta_prompt not found. Run Task 2.1 test cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ FAIL: Error testing generate_candidate_prompts: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    checks_failed += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if checks_failed == 0:\n",
        "    print(f\"All checks passed! ✓ ({checks_passed} checks)\")\n",
        "else:\n",
        "    print(f\"Some checks failed: {checks_passed} passed, {checks_failed} failed\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fAqBD-lh4V"
      },
      "outputs": [],
      "source": [
        "def generate_candidate_prompts(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    meta_prompt: str,\n",
        "    n_candidates: int = 5,\n",
        "    temperature: float = 0.7\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate multiple candidate prompts using the meta-prompt.\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        tokenizer: The tokenizer\n",
        "        meta_prompt: The meta-prompt asking for improved prompts\n",
        "        n_candidates: Number of candidates to generate\n",
        "        temperature: Sampling temperature for diversity\n",
        "\n",
        "    Returns:\n",
        "        List of valid candidate prompt strings (containing {question})\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C16y_DQslh4V"
      },
      "outputs": [],
      "source": [
        "# Test candidate generation\n",
        "print(\"Generating candidate prompts...\")\n",
        "candidates = generate_candidate_prompts(model, tokenizer, sample_meta_prompt, n_candidates=3)\n",
        "print(f\"\\nGenerated {len(candidates)} valid candidates:\")\n",
        "for i, c in enumerate(candidates, 1):\n",
        "    print(f\"  {i}. {c}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjAg8W29Pq3j"
      },
      "outputs": [],
      "source": [
        "# Answer Checkpoint: Task 2.3 - Optimization Loop\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATING: optimize_prompt function\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks_passed = 0\n",
        "checks_failed = 0\n",
        "\n",
        "try:\n",
        "    if 'optimization_result' in globals():\n",
        "        result = optimization_result\n",
        "\n",
        "        # Check 1: Returns a dictionary\n",
        "        if isinstance(result, dict):\n",
        "            print(\"✓ PASS: Function returns a dictionary\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"✗ FAIL: Function should return dict, got {type(result)}\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 2: Has required keys\n",
        "        required_keys = ['iterations', 'prompts', 'accuracies', 'best_prompt', 'best_accuracy']\n",
        "        missing_keys = [k for k in required_keys if k not in result]\n",
        "        if len(missing_keys) == 0:\n",
        "            print(\"✓ PASS: History dictionary has all required keys\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(f\"✗ FAIL: Missing keys: {missing_keys}\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 3: At least 1 iteration completed\n",
        "        if len(result.get('iterations', [])) >= 1:\n",
        "            print(f\"✓ PASS: At least 1 iteration completed ({len(result['iterations'])} iterations)\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(\"✗ FAIL: Should complete at least 1 iteration\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 4: Prompts list contains initial prompt\n",
        "        if len(result.get('prompts', [])) >= 1:\n",
        "            print(\"✓ PASS: Prompts list contains at least initial prompt\")\n",
        "            checks_passed += 1\n",
        "        else:\n",
        "            print(\"✗ FAIL: Prompts list should contain at least initial prompt\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 5: Best accuracy is valid\n",
        "        best_acc = result.get('best_accuracy', -1)\n",
        "        if isinstance(best_acc, (int, float)) and 0.0 <= best_acc <= 1.0:\n",
        "            print(f\"✓ PASS: Best accuracy is valid: {best_acc:.2%}\")\n",
        "            checks_passed += 1\n",
        "            # Check if improved (or at least maintained)\n",
        "            if 'best_baseline_accuracy' in globals():\n",
        "                if best_acc >= best_baseline_accuracy:\n",
        "                    print(f\"✓ PASS: Best accuracy ({best_acc:.2%}) >= baseline ({best_baseline_accuracy:.2%})\")\n",
        "                    checks_passed += 1\n",
        "                else:\n",
        "                    print(f\"⚠ INFO: Best accuracy ({best_acc:.2%}) < baseline ({best_baseline_accuracy:.2%})\")\n",
        "                    print(\"   This is okay - optimization doesn't always improve immediately\")\n",
        "        else:\n",
        "            print(f\"✗ FAIL: best_accuracy should be between 0.0 and 1.0, got {best_acc}\")\n",
        "            checks_failed += 1\n",
        "\n",
        "        # Check 6: Iterations structure\n",
        "        if len(result.get('iterations', [])) > 0:\n",
        "            first_iter = result['iterations'][0]\n",
        "            iter_keys = ['prompt', 'train_accuracy', 'val_accuracy', 'n_failures']\n",
        "            if all(k in first_iter for k in iter_keys):\n",
        "                print(\"✓ PASS: Iteration data has correct structure\")\n",
        "                checks_passed += 1\n",
        "            else:\n",
        "                missing = [k for k in iter_keys if k not in first_iter]\n",
        "                print(f\"⚠ WARNING: Iteration data missing keys: {missing}\")\n",
        "    else:\n",
        "        print(\"⚠ SKIP: optimization_result not found. Run the optimization loop cell first.\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ FAIL: Error testing optimize_prompt: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    checks_failed += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if checks_failed == 0:\n",
        "    print(f\"All checks passed! ✓ ({checks_passed} checks)\")\n",
        "else:\n",
        "    print(f\"Some checks failed: {checks_passed} passed, {checks_failed} failed\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW0oDaXQlh4W"
      },
      "source": [
        "### Task 2.3: The Full Optimization Loop\n",
        "\n",
        "**Goal**: Implement the complete Automatic Prompt Engineering (APE) algorithm that iteratively improves prompts.\n",
        "\n",
        "**Algorithm Overview**:\n",
        "\n",
        "The APE algorithm follows this iterative process:\n",
        "1. **Evaluate** current prompt on training and validation data\n",
        "2. **Analyze** errors to understand failure modes\n",
        "3. **Generate** candidate improved prompts using meta-prompting\n",
        "4. **Evaluate** candidates on validation data\n",
        "5. **Select** best candidate (if better than current)\n",
        "6. **Repeat** until convergence or max iterations\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "- **Initialize history**: Create a dictionary to track iterations, prompts tried, accuracies, best prompt, and best accuracy\n",
        "\n",
        "- **Main optimization loop** (for each iteration):\n",
        "  - Evaluate current prompt on both training data (for error analysis) and validation data (for selection)\n",
        "  - Update history with current iteration results\n",
        "  - Check stopping conditions (target accuracy reached, no failures, max iterations, no valid candidates)\n",
        "  - Generate candidate prompts using meta-prompting from training failures\n",
        "  - Evaluate all candidates on validation data and track the best one\n",
        "  - Update prompt if best candidate is better than current\n",
        "\n",
        "- **Return history**: Dictionary with all optimization information\n",
        "\n",
        "**Stopping conditions**:\n",
        "- Target accuracy reached (e.g., 95%)\n",
        "- No failures to analyze (perfect accuracy on training)\n",
        "- Maximum iterations reached\n",
        "- No valid candidates generated\n",
        "\n",
        "**Success criteria**:\n",
        "- History dictionary has correct structure with all required keys\n",
        "- At least 1 iteration completed\n",
        "- Best accuracy >= initial accuracy (or explanation if not improved)\n",
        "- Prompts list contains at least the initial prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zE6QAlZlh4W"
      },
      "outputs": [],
      "source": [
        "def optimize_prompt(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    initial_prompt: str,\n",
        "    train_data: List[Dict],\n",
        "    val_data: List[Dict],\n",
        "    n_iterations: int = 3,\n",
        "    n_candidates: int = 3,\n",
        "    target_accuracy: float = 0.95,\n",
        "    verbose: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Run the full prompt optimization loop.\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        tokenizer: The tokenizer\n",
        "        initial_prompt: Starting prompt template\n",
        "        train_data: Data for error analysis\n",
        "        val_data: Data for evaluation\n",
        "        n_iterations: Maximum optimization iterations\n",
        "        n_candidates: Candidates to generate per iteration\n",
        "        target_accuracy: Stop if this accuracy is reached\n",
        "        verbose: Print progress\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with optimization history and best prompt\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bK1pxq6lh4W"
      },
      "outputs": [],
      "source": [
        "# Run the optimization loop\n",
        "print(\"Starting Automatic Prompt Optimization...\")\n",
        "print(f\"Initial prompt: '{best_baseline_prompt}'\")\n",
        "print(f\"Initial accuracy: {best_baseline_accuracy:.2%}\")\n",
        "\n",
        "optimization_result = optimize_prompt(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    initial_prompt=best_baseline_prompt,\n",
        "    train_data=train_data,\n",
        "    val_data=val_data,\n",
        "    n_iterations=3,\n",
        "    n_candidates=3,\n",
        "    target_accuracy=0.95,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"OPTIMIZATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best prompt found: '{optimization_result['best_prompt']}'\")\n",
        "print(f\"Best accuracy: {optimization_result['best_accuracy']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lot3WoSBlh4W"
      },
      "outputs": [],
      "source": [
        "# Visualize optimization progress\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(optimization_result['accuracies']) + 1),\n",
        "         [a * 100 for a in optimization_result['accuracies']],\n",
        "         'bo-', linewidth=2, markersize=10)\n",
        "plt.axhline(y=best_baseline_accuracy * 100, color='r', linestyle='--', label='Baseline')\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Validation Accuracy (%)', fontsize=12)\n",
        "plt.title('Prompt Optimization Progress', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(0, 100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjQLGvDblh4W"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on held-out test set\n",
        "print(\"Final Evaluation on Test Set\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Baseline on test\n",
        "baseline_test_acc, baseline_test_failures = evaluate_prompt(\n",
        "    model, tokenizer, best_baseline_prompt, test_data\n",
        ")\n",
        "print(f\"Baseline prompt: '{best_baseline_prompt}'\")\n",
        "print(f\"Baseline test accuracy: {baseline_test_acc:.2%}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Optimized on test\n",
        "optimized_prompt = optimization_result['best_prompt']\n",
        "optimized_test_acc, optimized_test_failures = evaluate_prompt(\n",
        "    model, tokenizer, optimized_prompt, test_data\n",
        ")\n",
        "print(f\"Optimized prompt: '{optimized_prompt}'\")\n",
        "print(f\"Optimized test accuracy: {optimized_test_acc:.2%}\")\n",
        "\n",
        "print()\n",
        "improvement = (optimized_test_acc - baseline_test_acc) * 100\n",
        "print(f\"Improvement: {improvement:+.1f} percentage points\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA-NuSaLlh4W"
      },
      "source": [
        "---\n",
        "\n",
        "## TASK 3: Interpretability Analysis (6 points)\n",
        "\n",
        "Now we'll analyze *why* the optimized prompt performs better using white-box techniques.\n",
        "\n",
        "**3.1**: *Confidence Analysis* — Compare model confidence (logit values) between prompts  \n",
        "**3.2**: *Attention Analysis* — Visualize and compare attention patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAxFq3m_lh4W"
      },
      "source": [
        "### Task 3.1: Confidence Analysis\n",
        "\n",
        "Compare how \"confident\" the model is when using the baseline vs optimized prompt:\n",
        "1. Extract logits for the generated answer tokens\n",
        "2. Convert to probabilities\n",
        "3. Compare the probability assigned to correct tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-04qfF_lh4W"
      },
      "outputs": [],
      "source": [
        "def analyze_confidence(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_template: str,\n",
        "    question: str,\n",
        "    target_answer: str\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Analyze model confidence for a given prompt and question.\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        tokenizer: The tokenizer\n",
        "        prompt_template: The prompt template to use\n",
        "        question: The question to answer\n",
        "        target_answer: The correct answer\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with confidence metrics\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dpp_gljlh4W"
      },
      "outputs": [],
      "source": [
        "# Compare confidence between baseline and optimized prompts\n",
        "print(\"Confidence Analysis: Baseline vs Optimized Prompt\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Select examples to analyze\n",
        "analysis_examples = test_data[:10]\n",
        "\n",
        "baseline_confidences = []\n",
        "optimized_confidences = []\n",
        "baseline_entropies = []\n",
        "optimized_entropies = []\n",
        "\n",
        "for example in analysis_examples:\n",
        "    question = example['question']\n",
        "    target = example['answer']\n",
        "\n",
        "    # Analyze with baseline prompt\n",
        "    baseline_result = analyze_confidence(model, tokenizer, best_baseline_prompt, question, target)\n",
        "    baseline_confidences.append(baseline_result['first_token_confidence'])\n",
        "    baseline_entropies.append(baseline_result['entropy'])\n",
        "\n",
        "    # Analyze with optimized prompt\n",
        "    optimized_result = analyze_confidence(model, tokenizer, optimized_prompt, question, target)\n",
        "    optimized_confidences.append(optimized_result['first_token_confidence'])\n",
        "    optimized_entropies.append(optimized_result['entropy'])\n",
        "\n",
        "print(f\"Average First Token Confidence:\")\n",
        "print(f\"  Baseline:  {np.mean(baseline_confidences):.4f}\")\n",
        "print(f\"  Optimized: {np.mean(optimized_confidences):.4f}\")\n",
        "print()\n",
        "print(f\"Average Entropy (lower = more confident):\")\n",
        "print(f\"  Baseline:  {np.mean(baseline_entropies):.4f}\")\n",
        "print(f\"  Optimized: {np.mean(optimized_entropies):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn_PRpNglh4X"
      },
      "outputs": [],
      "source": [
        "# Visualize confidence comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Confidence bar chart\n",
        "ax1 = axes[0]\n",
        "x = np.arange(len(analysis_examples))\n",
        "width = 0.35\n",
        "ax1.bar(x - width/2, baseline_confidences, width, label='Baseline', alpha=0.8)\n",
        "ax1.bar(x + width/2, optimized_confidences, width, label='Optimized', alpha=0.8)\n",
        "ax1.set_xlabel('Example Index')\n",
        "ax1.set_ylabel('First Token Confidence')\n",
        "ax1.set_title('Model Confidence Comparison')\n",
        "ax1.legend()\n",
        "ax1.set_xticks(x)\n",
        "\n",
        "# Entropy comparison\n",
        "ax2 = axes[1]\n",
        "ax2.bar(x - width/2, baseline_entropies, width, label='Baseline', alpha=0.8, color='coral')\n",
        "ax2.bar(x + width/2, optimized_entropies, width, label='Optimized', alpha=0.8, color='seagreen')\n",
        "ax2.set_xlabel('Example Index')\n",
        "ax2.set_ylabel('Entropy (lower = more confident)')\n",
        "ax2.set_title('Prediction Entropy Comparison')\n",
        "ax2.legend()\n",
        "ax2.set_xticks(x)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENnqRjJTlh4X"
      },
      "source": [
        "### Task 3.2: Attention Analysis\n",
        "\n",
        "Visualize attention patterns to understand what the model \"looks at\" with different prompts:\n",
        "1. Extract attention weights from the model (decoder attention for decoder-only models)\n",
        "2. Create attention heatmaps\n",
        "3. Compare attention concentration between prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aPJGv9blh4X"
      },
      "outputs": [],
      "source": [
        "def get_encoder_attention(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_template: str,\n",
        "    question: str\n",
        ") -> Tuple[torch.Tensor, List[str]]:\n",
        "    \"\"\"\n",
        "    Extract attention weights for analysis (decoder attention for decoder-only models).\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        tokenizer: The tokenizer\n",
        "        prompt_template: The prompt template\n",
        "        question: The question\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (attention_weights, token_labels)\n",
        "        attention_weights: Tensor of shape (n_layers, n_heads, seq_len, seq_len)\n",
        "        token_labels: List of token strings for labeling\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nMPuCCDlh4X"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(\n",
        "    attention: torch.Tensor,\n",
        "    tokens: List[str],\n",
        "    layer: int = -1,\n",
        "    head: int = 0,\n",
        "    title: str = \"Attention Heatmap\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize attention weights as a heatmap.\n",
        "\n",
        "    Args:\n",
        "        attention: Tensor of shape (n_layers, n_heads, seq_len, seq_len)\n",
        "        tokens: List of token strings\n",
        "        layer: Which layer to visualize (-1 for last)\n",
        "        head: Which attention head to visualize\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU018m70lh4X"
      },
      "outputs": [],
      "source": [
        "# Compare attention patterns for a specific example\n",
        "example = test_data[0]\n",
        "print(f\"Analyzing attention for question:\")\n",
        "print(f\"  {example['question']}\")\n",
        "print(f\"  Answer: {example['answer']}\")\n",
        "print()\n",
        "\n",
        "# Get attention for baseline prompt\n",
        "baseline_attn, baseline_tokens = get_encoder_attention(\n",
        "    model, tokenizer, best_baseline_prompt, example['question']\n",
        ")\n",
        "print(f\"Baseline prompt: '{best_baseline_prompt}'\")\n",
        "visualize_attention(\n",
        "    baseline_attn, baseline_tokens,\n",
        "    layer=-1, head=0,\n",
        "    title=\"Baseline Prompt - Last Layer, Head 0\"\n",
        ")\n",
        "\n",
        "# Get attention for optimized prompt\n",
        "optimized_attn, optimized_tokens = get_encoder_attention(\n",
        "    model, tokenizer, optimized_prompt, example['question']\n",
        ")\n",
        "print(f\"Optimized prompt: '{optimized_prompt}'\")\n",
        "visualize_attention(\n",
        "    optimized_attn, optimized_tokens,\n",
        "    layer=-1, head=0,\n",
        "    title=\"Optimized Prompt - Last Layer, Head 0\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85qMn52glh4X"
      },
      "outputs": [],
      "source": [
        "def compute_attention_concentration(\n",
        "    attention: torch.Tensor,\n",
        "    tokens: List[str],\n",
        "    number_pattern: str = r'\\d+'\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Compute how much attention is concentrated on number tokens.\n",
        "\n",
        "    Args:\n",
        "        attention: Attention tensor (layers, heads, seq, seq)\n",
        "        tokens: List of token strings\n",
        "        number_pattern: Regex pattern for number tokens\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with attention concentration metrics\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7JNc_-wlh4X"
      },
      "outputs": [],
      "source": [
        "# Compare attention concentration across multiple examples\n",
        "print(\"Attention Concentration Analysis\")\n",
        "print(\"=\"*70)\n",
        "print(\"Measuring how much attention is directed toward number tokens...\\n\")\n",
        "\n",
        "baseline_number_attention = []\n",
        "optimized_number_attention = []\n",
        "\n",
        "for i, example in enumerate(test_data[:10]):\n",
        "    # Baseline\n",
        "    b_attn, b_tokens = get_encoder_attention(model, tokenizer, best_baseline_prompt, example['question'])\n",
        "    b_conc = compute_attention_concentration(b_attn, b_tokens)\n",
        "    baseline_number_attention.append(b_conc['avg_attention_per_number'])\n",
        "    # Optimized\n",
        "    o_attn, o_tokens = get_encoder_attention(model, tokenizer, optimized_prompt, example['question'])\n",
        "    o_conc = compute_attention_concentration(o_attn, o_tokens)\n",
        "    optimized_number_attention.append(o_conc['avg_attention_per_number'])\n",
        "print(f\"Average Attention per Number Token:\")\n",
        "print(f\"  Baseline:  {np.mean(baseline_number_attention):.4f}\")\n",
        "print(f\"  Optimized: {np.mean(optimized_number_attention):.4f}\")\n",
        "print()\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "x = np.arange(10)\n",
        "width = 0.35\n",
        "ax.bar(x - width/2, baseline_number_attention, width, label='Baseline', alpha=0.8)\n",
        "ax.bar(x + width/2, optimized_number_attention, width, label='Optimized', alpha=0.8)\n",
        "ax.set_xlabel('Example Index')\n",
        "ax.set_ylabel('Avg Attention per Number Token (normalized)')\n",
        "ax.set_title('Attention Concentration on Numbers: Baseline vs Optimized')\n",
        "ax.legend()\n",
        "ax.set_xticks(x)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqRX0cfvlh4Y"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary and Analysis\n",
        "\n",
        "Write your analysis of why the optimized prompt performed better based on the interpretability findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSm_igEQlh4Y"
      },
      "source": [
        "### Your Analysis (Fill this in)\n",
        "\n",
        "**1. Prompt Optimization Results:**\n",
        "- Initial baseline prompt: [YOUR ANSWER]\n",
        "- Final optimized prompt: [YOUR ANSWER]\n",
        "- Accuracy improvement: [YOUR ANSWER]\n",
        "\n",
        "**2. Confidence Analysis Findings:**\n",
        "- How did model confidence change with the optimized prompt?\n",
        "- What does the entropy comparison tell us?\n",
        "\n",
        "[YOUR ANSWER]\n",
        "\n",
        "**3. Attention Analysis Findings:**\n",
        "- Did the optimized prompt change what the model attends to?\n",
        "- Was there more attention on important tokens (numbers)?\n",
        "\n",
        "[YOUR ANSWER]\n",
        "\n",
        "**4. Why Did Optimization Work?**\n",
        "- Based on your analysis, explain why the optimized prompt performs better.\n",
        "\n",
        "[YOUR ANSWER]\n",
        "\n",
        "**5. Limitations:**\n",
        "- What are the limitations of this approach?\n",
        "- When might automatic prompt optimization fail?\n",
        "\n",
        "[YOUR ANSWER]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkA00rjDlh4Y"
      },
      "outputs": [],
      "source": [
        "# Final summary statistics\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBaseline Prompt: '{best_baseline_prompt}'\")\n",
        "print(f\"Optimized Prompt: '{optimized_prompt}'\")\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Baseline Accuracy:  {baseline_test_acc:.2%}\")\n",
        "print(f\"  Optimized Accuracy: {optimized_test_acc:.2%}\")\n",
        "print(f\"  Improvement: {(optimized_test_acc - baseline_test_acc)*100:+.1f}%\")\n",
        "print(f\"\\nInterpretability Metrics (averaged over test examples):\")\n",
        "print(f\"  Confidence - Baseline:  {np.mean(baseline_confidences):.4f}\")\n",
        "print(f\"  Confidence - Optimized: {np.mean(optimized_confidences):.4f}\")\n",
        "print(f\"  Avg Attention per Number Token - Baseline:  {np.mean(baseline_number_attention):.4f}\")\n",
        "print(f\"  Avg Attention per Number Token - Optimized: {np.mean(optimized_number_attention):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS (2 points): Solve any task with an LLM\n",
        "\n",
        "**Goal.**  \n",
        "Pick **one** of the homework tasks (Task 1, Task 2, or Task 3) and solve it using an **LLM**. Provide the **LLM** with the **task description** and any **starter/prerequisite code** it depends on, ask it to generate a complete **code solution** first, then run that **generated code** here in the code notebook yourself. Finally, document what you did and **compare** the LLM’s result to your own pipeline.\n",
        "\n",
        "**What to deliver below.**\n",
        "1) **LLM used** (name + version, e.g., “Llama-3-8B-Instruct”, “GPT-x”, “Claude-x”, “Mistral-x”, etc.).  \n",
        "2) **Prompt(s)** you used.  \n",
        "3) **LLM output** — copy and paste the generated code.  \n",
        "4) **Comparison** to your solution: what matches or differs (quantitative or qualitative).  \n",
        "5) **Reflection**: what the LLM was **good at** vs **bad at**, what it got **right** vs **wrong**.\n",
        "\n",
        "> **No code required.** You do **not** need to run, share, or submit any code used for the LLM generation. Provide only the deliverables listed above.\n",
        "> You may use any LLMs through any interface (API, web UI, local inference).\n"
      ],
      "metadata": {
        "id": "IfD8mGz1oc9A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwVLk82Moe-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}