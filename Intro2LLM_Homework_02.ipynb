{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita2002/LLMS/blob/main/Intro2LLM_Homework_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "2Hovllpn7lRB"
      },
      "source": [
        "# HW 02: Retrieval with TF-IDF / BM25 / LSA, Word2Vec (CBOW), and MAP Evaluation\n",
        "\n",
        "### This homework has three tasks:\n",
        "1) **Ranked retrieval with TF-IDF, BM25, and LSA (Truncated SVD)** on a sampled subset of 20 Newsgroups — **6 pts**\n",
        "2) **Word2Vec (CBOW) with PyTorch** trained on the same sampled subset — **7 pts**\n",
        "3) **Mean Average Precision (MAP)** implementation + evaluate LSA vs CBOW on the test split — **7 pts**\n",
        "\n",
        "**BONUS:** Solve any task with an LLM — **2 pts**\n",
        "\n",
        "#### Total: 20 points (+2 bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILeyL6VG7lRB"
      },
      "source": [
        "## Pre-requisite code\n",
        "\n",
        "The code in this section will be used in the tasks.\n",
        "**Do not change these lines in your submission.**\n",
        "\n",
        "We:\n",
        "- load a **small, fixed subset** of the 20 Newsgroups dataset (4 topics),\n",
        "- split it into TRAIN_DOCS and TEST_DOCS,\n",
        "- build queries and relevance sets,\n",
        "- define helpers for tokenization, cosine similarity, etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these on Colab.\n",
        "!pip -q install rank_bm25 torch scikit-learn"
      ],
      "metadata": {
        "id": "FJs7DEekwAu8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random, collections, itertools\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import numpy as np\n",
        "\n",
        "# External libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Tokenization\n",
        "# -------------------------------------------------\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Lowercase, keep alphabetic tokens only, split on whitespace.\n",
        "    We'll also use this tokenizer inside scikit-learn's TfidfVectorizer.\n",
        "    \"\"\"\n",
        "    toks = []\n",
        "    for t in text.lower().strip().split():\n",
        "        # keep only alphabetic chars\n",
        "        t = \"\".join(ch for ch in t if ch.isalpha())\n",
        "        if t:\n",
        "            toks.append(t)\n",
        "    return toks\n",
        "\n",
        "@dataclass\n",
        "class Doc:\n",
        "    text: str\n",
        "    tokens: List[str]\n",
        "    label: str      # coarse topic label (used to derive relevance)\n",
        "\n",
        "def make_docs(lines: List[Tuple[str, str]]) -> List[Doc]:\n",
        "    \"\"\"\n",
        "    lines = [(text, label), ...]\n",
        "    We'll re-tokenize here, even though we already truncated earlier,\n",
        "    to keep the interface clean.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    for txt, lab in lines:\n",
        "        docs.append(Doc(text=txt, tokens=tokenize(txt), label=lab))\n",
        "    return docs\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Mini 20 Newsgroups subset\n",
        "# -------------------------------------------------\n",
        "# We'll sample 4 topical newsgroups as our \"topics\"\n",
        "CATEGORIES = [\n",
        "    \"comp.sys.mac.hardware\",  # Apple / Mac hardware issues\n",
        "    \"sci.space\",              # Space / NASA / missions\n",
        "    \"rec.autos\",              # Cars / engines / performance\n",
        "    \"rec.sport.hockey\",       # Hockey / teams / playoffs\n",
        "]\n",
        "\n",
        "# We'll map each full newsgroup name to a short label\n",
        "LABEL_MAP = {\n",
        "    \"comp.sys.mac.hardware\": \"mac\",\n",
        "    \"sci.space\": \"space\",\n",
        "    \"rec.autos\": \"autos\",\n",
        "    \"rec.sport.hockey\": \"hockey\",\n",
        "}\n",
        "\n",
        "# How many docs we keep per class for train/test,\n",
        "# and how aggressively we truncate each post.\n",
        "TRAIN_PER_CLASS = 100\n",
        "TEST_PER_CLASS  = 5\n",
        "MAX_TOKENS_PER_DOC = 50  # cap each doc to first N tokens for speed\n",
        "\n",
        "def collect_subset(dataset, per_class: int) -> List[Tuple[str,str]]:\n",
        "    \"\"\"\n",
        "    Build a list of (truncated_text, short_label) for up to per_class\n",
        "    examples per category in CATEGORIES.\n",
        "    We remove headers/footers/quotes via fetch_20newsgroups(... remove=...).\n",
        "    \"\"\"\n",
        "    by_class: Dict[str, List[Tuple[str,str]]] = {cat: [] for cat in CATEGORIES}\n",
        "\n",
        "    for text, tgt in zip(dataset.data, dataset.target):\n",
        "        cat = dataset.target_names[tgt]\n",
        "        if cat not in by_class:\n",
        "            continue\n",
        "        if len(by_class[cat]) >= per_class:\n",
        "            continue\n",
        "\n",
        "        toks = tokenize(text)\n",
        "        toks = toks[:MAX_TOKENS_PER_DOC]        # truncate\n",
        "        truncated_text = \" \".join(toks)         # rejoin to text form\n",
        "\n",
        "        by_class[cat].append((truncated_text, LABEL_MAP[cat]))\n",
        "\n",
        "    # flatten in fixed category order for determinism\n",
        "    lines = []\n",
        "    for cat in CATEGORIES:\n",
        "        lines.extend(by_class[cat])\n",
        "    return lines\n",
        "\n",
        "# Load raw 20NG subsets (without headers/footers/quotes to reduce noise)\n",
        "_20ng_train = fetch_20newsgroups(\n",
        "    subset=\"train\",\n",
        "    categories=CATEGORIES,\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "_20ng_test = fetch_20newsgroups(\n",
        "    subset=\"test\",\n",
        "    categories=CATEGORIES,\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# Build our small train/test corpora\n",
        "TRAIN_RAW = collect_subset(_20ng_train, per_class=TRAIN_PER_CLASS)\n",
        "TEST_RAW  = collect_subset(_20ng_test,  per_class=TEST_PER_CLASS)\n",
        "\n",
        "TRAIN_DOCS = make_docs(TRAIN_RAW)\n",
        "TEST_DOCS  = make_docs(TEST_RAW)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Queries & relevance\n",
        "# -------------------------------------------------\n",
        "# We craft one realistic search-style query per topic.\n",
        "# Relevant docs are those in TEST_DOCS whose label matches.\n",
        "QUERIES = [\n",
        "    (\n",
        "        \"how do i fix problems on my mac like disk failures, monitor issues and software failures\",\n",
        "        \"mac\"\n",
        "    ),\n",
        "    (\n",
        "        \"latest updates on nasa missions space shuttle launches and orbital science experiments\",\n",
        "        \"space\"\n",
        "    ),\n",
        "    (\n",
        "        \"advice on car performance engine reliability and buying a new vehicle\",\n",
        "        \"autos\"\n",
        "    ),\n",
        "    (\n",
        "        \"discussion about hockey teams playoffs goalies and physical play on the ice\",\n",
        "        \"hockey\"\n",
        "    )\n",
        "]\n",
        "\n",
        "def relevant_doc_indices(test_docs: List[Doc], label: str) -> List[int]:\n",
        "    \"\"\"Return indices in test_docs whose label matches the given topic label.\"\"\"\n",
        "    return [i for i, d in enumerate(test_docs) if d.label == label]\n",
        "\n",
        "print(f\"✓ Loaded {len(TRAIN_DOCS)} train docs and {len(TEST_DOCS)} test docs\")\n",
        "for lab in sorted({d.label for d in TRAIN_DOCS}):\n",
        "    print(\"  – label:\", lab,\n",
        "          \"| train:\", sum(1 for d in TRAIN_DOCS if d.label==lab),\n",
        "          \"| test:\",  sum(1 for d in TEST_DOCS  if d.label==lab))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Vector helpers\n",
        "# -------------------------------------------------\n",
        "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"Cosine similarity between vectors a and b. Returns 0 if either vector has zero norm.\"\"\"\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    if norm_a == 0 or norm_b == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(a, b) / (norm_a * norm_b))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Vocabulary utilities (for Task 2 CBOW)\n",
        "# -------------------------------------------------\n",
        "def build_vocab(docs: List[Doc], min_count: int = 1\n",
        "               ) -> Tuple[Dict[str,int], Dict[int,str], Dict[str,int]]:\n",
        "    \"\"\"\n",
        "    Build word-index mappings based on frequency in a list of Doc objects.\n",
        "\n",
        "    Returns:\n",
        "        w2i: word -> index\n",
        "        i2w: index -> word\n",
        "        freq: Counter of word frequencies\n",
        "    \"\"\"\n",
        "    freq = collections.Counter(t for d in docs for t in d.tokens)\n",
        "    vocab = [w for w, c in freq.items() if c >= min_count]\n",
        "    vocab = sorted(vocab)\n",
        "    w2i = {w:i for i, w in enumerate(vocab)}\n",
        "    i2w = {i:w for w, i in w2i.items()}\n",
        "    return w2i, i2w, freq\n",
        "\n",
        "print(\"✓ Pre-req code ready.\")"
      ],
      "metadata": {
        "id": "SCX-NfpVxjqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c866f0c-b89f-4e83-e278-bcd0fe311dc1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded 400 train docs and 20 test docs\n",
            "  – label: autos | train: 100 | test: 5\n",
            "  – label: hockey | train: 100 | test: 5\n",
            "  – label: mac | train: 100 | test: 5\n",
            "  – label: space | train: 100 | test: 5\n",
            "✓ Pre-req code ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, d in enumerate(TRAIN_DOCS[:10]):\n",
        "    print(f\"\\n------ TRAIN_DOC {i} ------\")\n",
        "    print(\"Label:\", d.label)\n",
        "    print(\"Text:\", d.text)\n",
        "    print(\"Tokens:\", d.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcVK_oPC0P6i",
        "outputId": "6122893c-cabc-4c39-9354-5ae0dfa1e79a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------ TRAIN_DOC 0 ------\n",
            "Label: mac\n",
            "Text: well i just got my centris yesterday it took just over two weeks from placing the order the dealer rutgers computer store appologized because apple made a substitution on my order i ordered the one without ethernet but they substituted one with ethernet he wanted to know if that would\n",
            "Tokens: ['well', 'i', 'just', 'got', 'my', 'centris', 'yesterday', 'it', 'took', 'just', 'over', 'two', 'weeks', 'from', 'placing', 'the', 'order', 'the', 'dealer', 'rutgers', 'computer', 'store', 'appologized', 'because', 'apple', 'made', 'a', 'substitution', 'on', 'my', 'order', 'i', 'ordered', 'the', 'one', 'without', 'ethernet', 'but', 'they', 'substituted', 'one', 'with', 'ethernet', 'he', 'wanted', 'to', 'know', 'if', 'that', 'would']\n",
            "\n",
            "------ TRAIN_DOC 1 ------\n",
            "Label: mac\n",
            "Text: to the best of my knowledge there arent any problems with quadras and blind transfers trouble with blind transfers usually means the programmer screwed up the tibs or didnt test their driver with the device in question well designed tibs poll or loop at every point where delays of µsec\n",
            "Tokens: ['to', 'the', 'best', 'of', 'my', 'knowledge', 'there', 'arent', 'any', 'problems', 'with', 'quadras', 'and', 'blind', 'transfers', 'trouble', 'with', 'blind', 'transfers', 'usually', 'means', 'the', 'programmer', 'screwed', 'up', 'the', 'tibs', 'or', 'didnt', 'test', 'their', 'driver', 'with', 'the', 'device', 'in', 'question', 'well', 'designed', 'tibs', 'poll', 'or', 'loop', 'at', 'every', 'point', 'where', 'delays', 'of', 'µsec']\n",
            "\n",
            "------ TRAIN_DOC 2 ------\n",
            "Label: mac\n",
            "Text: help i have an adb graphicsd tablet which i want to connect to my quadra unfortunately the has only one adb port and it seems i would have to give up my mouse please can someone help me i want to use the tablet as well as the mouse and\n",
            "Tokens: ['help', 'i', 'have', 'an', 'adb', 'graphicsd', 'tablet', 'which', 'i', 'want', 'to', 'connect', 'to', 'my', 'quadra', 'unfortunately', 'the', 'has', 'only', 'one', 'adb', 'port', 'and', 'it', 'seems', 'i', 'would', 'have', 'to', 'give', 'up', 'my', 'mouse', 'please', 'can', 'someone', 'help', 'me', 'i', 'want', 'to', 'use', 'the', 'tablet', 'as', 'well', 'as', 'the', 'mouse', 'and']\n",
            "\n",
            "------ TRAIN_DOC 3 ------\n",
            "Label: mac\n",
            "Text: yes what you are saying is absolutey true but what you fail to mention is the fact that the lciii uses the new pin simms which allow bit access to each simm in the case of the lc iii it only has one simm slot but accesses will be bits\n",
            "Tokens: ['yes', 'what', 'you', 'are', 'saying', 'is', 'absolutey', 'true', 'but', 'what', 'you', 'fail', 'to', 'mention', 'is', 'the', 'fact', 'that', 'the', 'lciii', 'uses', 'the', 'new', 'pin', 'simms', 'which', 'allow', 'bit', 'access', 'to', 'each', 'simm', 'in', 'the', 'case', 'of', 'the', 'lc', 'iii', 'it', 'only', 'has', 'one', 'simm', 'slot', 'but', 'accesses', 'will', 'be', 'bits']\n",
            "\n",
            "------ TRAIN_DOC 4 ------\n",
            "Label: mac\n",
            "Text: if anyone has any information about the upcoming new computers cyclone and tempest i am in need of some info anything would be greatly appreciated thanks\n",
            "Tokens: ['if', 'anyone', 'has', 'any', 'information', 'about', 'the', 'upcoming', 'new', 'computers', 'cyclone', 'and', 'tempest', 'i', 'am', 'in', 'need', 'of', 'some', 'info', 'anything', 'would', 'be', 'greatly', 'appreciated', 'thanks']\n",
            "\n",
            "------ TRAIN_DOC 5 ------\n",
            "Label: mac\n",
            "Text: were all set to buy one of these for the office to use for scanning in color photographs and for optical character recognition weve played with the original grayscale onescanner and were very pleased is the color model comparable in quality also what brand of ocr software would you recommend\n",
            "Tokens: ['were', 'all', 'set', 'to', 'buy', 'one', 'of', 'these', 'for', 'the', 'office', 'to', 'use', 'for', 'scanning', 'in', 'color', 'photographs', 'and', 'for', 'optical', 'character', 'recognition', 'weve', 'played', 'with', 'the', 'original', 'grayscale', 'onescanner', 'and', 'were', 'very', 'pleased', 'is', 'the', 'color', 'model', 'comparable', 'in', 'quality', 'also', 'what', 'brand', 'of', 'ocr', 'software', 'would', 'you', 'recommend']\n",
            "\n",
            "------ TRAIN_DOC 6 ------\n",
            "Label: mac\n",
            "Text: hello world does anyone know of a postscript ppd for a versatec asize plotter which is generally accessed via a zeh postscript interpreter replies by email very gratefully received this is proving to be quite a tricky one andrew d nielsen internet anielsendialixozau advanced systems consultant applelink aust applecentre perth\n",
            "Tokens: ['hello', 'world', 'does', 'anyone', 'know', 'of', 'a', 'postscript', 'ppd', 'for', 'a', 'versatec', 'asize', 'plotter', 'which', 'is', 'generally', 'accessed', 'via', 'a', 'zeh', 'postscript', 'interpreter', 'replies', 'by', 'email', 'very', 'gratefully', 'received', 'this', 'is', 'proving', 'to', 'be', 'quite', 'a', 'tricky', 'one', 'andrew', 'd', 'nielsen', 'internet', 'anielsendialixozau', 'advanced', 'systems', 'consultant', 'applelink', 'aust', 'applecentre', 'perth']\n",
            "\n",
            "------ TRAIN_DOC 7 ------\n",
            "Label: mac\n",
            "Text: i tried the right set and it didnt work im on the phone to their tech support right now and the guys doesnt know what a desktop rebuild is hes got me holding for someone else and holding and holding and holding\n",
            "Tokens: ['i', 'tried', 'the', 'right', 'set', 'and', 'it', 'didnt', 'work', 'im', 'on', 'the', 'phone', 'to', 'their', 'tech', 'support', 'right', 'now', 'and', 'the', 'guys', 'doesnt', 'know', 'what', 'a', 'desktop', 'rebuild', 'is', 'hes', 'got', 'me', 'holding', 'for', 'someone', 'else', 'and', 'holding', 'and', 'holding', 'and', 'holding']\n",
            "\n",
            "------ TRAIN_DOC 8 ------\n",
            "Label: mac\n",
            "Text: could someone tell me how to makefindget the best frontplate for iiviiivxc with internal syquest drive is there one available or do i have to make one from the original or cdrom one or scratch every suggestion welcome\n",
            "Tokens: ['could', 'someone', 'tell', 'me', 'how', 'to', 'makefindget', 'the', 'best', 'frontplate', 'for', 'iiviiivxc', 'with', 'internal', 'syquest', 'drive', 'is', 'there', 'one', 'available', 'or', 'do', 'i', 'have', 'to', 'make', 'one', 'from', 'the', 'original', 'or', 'cdrom', 'one', 'or', 'scratch', 'every', 'suggestion', 'welcome']\n",
            "\n",
            "------ TRAIN_DOC 9 ------\n",
            "Label: mac\n",
            "Text: hi what alternatives to the express modem do duo owners have if they want to go at least baud every place in town says they are back ordered and part of the reason i want a laptop mac is so i can use it as a remote terminal from wherever\n",
            "Tokens: ['hi', 'what', 'alternatives', 'to', 'the', 'express', 'modem', 'do', 'duo', 'owners', 'have', 'if', 'they', 'want', 'to', 'go', 'at', 'least', 'baud', 'every', 'place', 'in', 'town', 'says', 'they', 'are', 'back', 'ordered', 'and', 'part', 'of', 'the', 'reason', 'i', 'want', 'a', 'laptop', 'mac', 'is', 'so', 'i', 'can', 'use', 'it', 'as', 'a', 'remote', 'terminal', 'from', 'wherever']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1 (6 pts): Ranked Retrieval with TF-IDF, BM25, and LSA\n",
        "\n",
        "**Goal.** We’ll build *three* retrieval models on a small subset of the\n",
        "20 Newsgroups dataset and compare how they rank documents:\n",
        "\n",
        "1. **TF-IDF ranked retrieval** - use scikit-learn’s `TfidfVectorizer`\n",
        "2. **BM25 (Okapi BM25)** - use the rank_bm25 library’s `BM25Okapi`\n",
        "3. **LSA (Latent Semantic Analysis via Truncated SVD)** - use scikit-learn’s `TruncatedSVD` on the same TF-IDF features\n",
        "\n",
        "We'll then run the queries in `QUERIES` and see how each method ranks\n",
        "the held-out TEST_DOCS. We’ll also:\n",
        "- check vector dimensionality (TF-IDF / BM25 vocab size vs. LSA k),\n",
        "- compute **Precision@5 (P@5)** for each method (per query and averaged).\n",
        "\n",
        "**Retrieval setup**\n",
        "- Candidate results are the **TEST_DOCS** we built above (5 per topic).\n",
        "- For each query `(query_text, label)`:\n",
        "  - Rank all TEST_DOCS using TF-IDF.\n",
        "  - Rank all TEST_DOCS using BM25.\n",
        "  - Rank all TEST_DOCS using LSA.\n",
        "  - Show the top-5 doc labels.\n",
        "  - Report P@5.\n",
        "\n",
        "**What you implement**\n",
        "\n",
        "**TF-IDF ranked retrieval (2 pts)**  \n",
        "- Fit a `TfidfVectorizer` on the retrieval corpus (TEST_DOCS).\n",
        "- Use our provided tokenizer;\n",
        "- Implement `rank_with_tfidf(query_text, vectorizer, doc_matrix)`:\n",
        "  - Transform the query with the same `TfidfVectorizer`.\n",
        "  - Score each document by aggregating TF-IDF weights over the query terms.\n",
        "  - Return doc IDs sorted by score (descending).\n",
        "\n",
        "**BM25 retrieval (2 pts)**  \n",
        "- Fit a `BM25Okapi` model on the tokenized docs.\n",
        "- Implement `rank_with_bm25(query_text, bm25_model)`:\n",
        "  - Tokenize the query with our tokenizer.\n",
        "  - Score all docs via BM25. Use `get_scores(query_tokens)` from `BM25Okapi` to get aggregated BM25 scores.\n",
        "  - Return doc IDs sorted by score (descending).\n",
        "\n",
        "**LSA retrieval (2 pts)**\n",
        "- Reuse the same TF-IDF vectorizer and its TF-IDF matrix used for TF-IDF ranking, then apply `TruncatedSVD` on that TF-IDF matrix to build a low-dimensional semantic space (LSA).\n",
        "- Implement `rank_with_lsa(query_text, lsa_model)`:\n",
        "  - Project query into that same semantic space (via the same TF-IDF vectorizer + SVD).\n",
        "  - Rank by cosine similarity in LSA space.\n",
        "\n",
        "**Grading (6 pts total)**\n",
        "- **1.1** TF-IDF retrieval implementation — **2 pts**  \n",
        "- **1.2** BM25 retrieval implementation — **2 pts**  \n",
        "- **1.3** LSA retrieval implementation — **2 pts**\n"
      ],
      "metadata": {
        "id": "OwE0TXSkFX89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------\n",
        "# TF-IDF ranked retrieval (scikit-learn)\n",
        "# -----------------\n",
        "def fit_tfidf_ranker(docs: List[Doc]) -> Tuple[TfidfVectorizer, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Fit a TfidfVectorizer on the given docs (retrieval corpus).\n",
        "    Returns:\n",
        "        vectorizer: fitted TfidfVectorizer\n",
        "        doc_matrix: TF-IDF matrix of shape (num_docs, vocab_size)\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    texts_data = [d.text for d in docs]\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        tokenizer=tokenize,\n",
        "        preprocessor=None\n",
        "    )\n",
        "    doc_matrix = vectorizer.fit_transform(texts_data)\n",
        "    return vectorizer, doc_matrix\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def rank_with_tfidf(query_text: str,\n",
        "                    vectorizer: TfidfVectorizer,\n",
        "                    doc_matrix) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs using aggregated TF-IDF weights.\n",
        "    You can use the dot product between the query vector and TF-IDF matrix.\n",
        "    Returns:\n",
        "        order: list of doc indices sorted by score desc\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    query_vec = vectorizer.transform([query_text])\n",
        "    scores = doc_matrix @ query_vec.T\n",
        "    scores = scores.toarray().ravel()\n",
        "    order = np.argsort(-scores)  # minus sign = descending order\n",
        "    return order.tolist()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# BM25 ranked retrieval (rank_bm25)\n",
        "# -----------------\n",
        "def fit_bm25_ranker(docs: List[Doc]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Fit BM25 on the given docs (list of token lists).\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    corpus_tokens = [d.tokens for d in docs]\n",
        "    bm25_model = BM25Okapi(corpus_tokens)\n",
        "    return bm25_model\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def rank_with_bm25(query_text: str,\n",
        "                   bm25_model: BM25Okapi) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs by aggregated BM25 score.\n",
        "    Returns:\n",
        "        order: doc indices (0..num_docs-1) sorted by score desc\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    query_tokens = tokenize(query_text)\n",
        "    scores = bm25_model.get_scores(query_tokens)\n",
        "    order = np.argsort(-scores)\n",
        "    return order.tolist()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# LSA ranked retrieval (reuse TF-IDF + TruncatedSVD)\n",
        "# -----------------\n",
        "@dataclass\n",
        "class LSAModel:\n",
        "    \"\"\"\n",
        "    LSAModel stores:\n",
        "    - vectorizer: the same fitted TfidfVectorizer used for TF-IDF ranking\n",
        "    - svd: fitted TruncatedSVD\n",
        "    - doc_embeddings: dense array of shape (num_docs, k)\n",
        "    - k: number of latent dimensions used\n",
        "    \"\"\"\n",
        "    vectorizer: TfidfVectorizer\n",
        "    svd: TruncatedSVD\n",
        "    doc_embeddings: np.ndarray\n",
        "    k: int\n",
        "\n",
        "\n",
        "def fit_lsa_ranker(vectorizer: TfidfVectorizer,\n",
        "                   doc_matrix,  # TF-IDF matrix constructed before\n",
        "                   k: int=10,\n",
        "                   random_state: int=RANDOM_SEED) -> LSAModel:\n",
        "    \"\"\"\n",
        "    Steps:\n",
        "    1) Reuse the same TF-IDF matrix constructed before\n",
        "    2) Reduce dimensionality with TruncatedSVD\n",
        "    3) Store doc embeddings in latent semantic space\n",
        "    \"\"\"\n",
        "    # clamping k so it doesn't exceed the TF-IDF feature dimension\n",
        "    max_k = max(1, min(k, min(doc_matrix.shape) - 1))\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    svd = TruncatedSVD(\n",
        "        n_components=max_k,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    doc_embeddings = svd.fit_transform(doc_matrix)\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return LSAModel(\n",
        "        vectorizer=vectorizer,\n",
        "        svd=svd,\n",
        "        doc_embeddings=doc_embeddings,\n",
        "        k=max_k\n",
        "    )\n",
        "\n",
        "\n",
        "def rank_with_lsa(query_text: str, lsa_model: LSAModel) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs by cosine similarity in the LSA semantic space.\n",
        "    (Query -> TF-IDF -> SVD; then cosine vs doc embeddings.)\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    query_tfidf = lsa_model.vectorizer.transform([query_text])  # shape: (1, vocab_size)\n",
        "    query_lsa = lsa_model.svd.transform(query_tfidf)[0]  # shape: (k,)\n",
        "    scores = []\n",
        "    for emb in lsa_model.doc_embeddings:\n",
        "        scores.append(cosine(emb, query_lsa))\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    order = np.argsort(-scores)\n",
        "    return order.tolist()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Build all three rankers on the same retrieval corpus (TEST_DOCS)\n",
        "# -------------------------------------------------\n",
        "tfidf_vec, tfidf_matrix = fit_tfidf_ranker(TEST_DOCS)\n",
        "bm25_model             = fit_bm25_ranker(TEST_DOCS)\n",
        "lsa_model              = fit_lsa_ranker(tfidf_vec, tfidf_matrix, k=10, random_state=RANDOM_SEED)\n",
        "\n",
        "# Vector size comparison:\n",
        "tfidf_vocab_size = len(tfidf_vec.vocabulary_)   # dimensionality of TF-IDF term space\n",
        "bm25_vocab_size  = len(bm25_model.idf)          # dimensionality of BM25 term space\n",
        "lsa_k            = lsa_model.k                  # dimensionality of LSA latent semantic space\n",
        "\n",
        "print(\"\\n[TASK1] Vector Size Comparison:\")\n",
        "print(f\"  TF-IDF dim   = {tfidf_vocab_size}\")\n",
        "print(f\"  BM25 dim     = {bm25_vocab_size}\")\n",
        "print(f\"  LSA dim (k)  = {lsa_k}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Evaluation: Precision@5 (per query and averaged)\n",
        "# -------------------------------------------------\n",
        "def precision_at_k(ranked: List[int], relevant: set, k: int = 5) -> float:\n",
        "    hits = sum(i in relevant for i in ranked[:k])\n",
        "    return hits / float(k)\n",
        "\n",
        "TOPK = 5\n",
        "tfidf_p5_scores = []\n",
        "bm25_p5_scores  = []\n",
        "lsa_p5_scores   = []\n",
        "\n",
        "for qtext, qlab in QUERIES:\n",
        "    rel = set(relevant_doc_indices(TEST_DOCS, qlab))\n",
        "\n",
        "    tfidf_order = rank_with_tfidf(qtext, tfidf_vec, tfidf_matrix)\n",
        "    bm25_order  = rank_with_bm25(qtext, bm25_model)\n",
        "    lsa_order   = rank_with_lsa(qtext, lsa_model)\n",
        "\n",
        "    p5_tfidf = precision_at_k(tfidf_order, rel, TOPK)\n",
        "    p5_bm25  = precision_at_k(bm25_order,  rel, TOPK)\n",
        "    p5_lsa   = precision_at_k(lsa_order,   rel, TOPK)\n",
        "\n",
        "    tfidf_p5_scores.append(p5_tfidf)\n",
        "    bm25_p5_scores.append(p5_bm25)\n",
        "    lsa_p5_scores.append(p5_lsa)\n",
        "\n",
        "    print(f\"\\n[TASK1] Query='{qtext}' | (label={qlab})\")\n",
        "    print(\"   TF-IDF top-5 labels:\", [TEST_DOCS[i].label for i in tfidf_order[:TOPK]],\n",
        "          f\"| P@5={p5_tfidf:.3f}\")\n",
        "    print(\"   BM25  top-5 labels:\", [TEST_DOCS[i].label for i in bm25_order[:TOPK]],\n",
        "          f\"| P@5={p5_bm25:.3f}\")\n",
        "    print(\"   LSA   top-5 labels:\", [TEST_DOCS[i].label for i in lsa_order[:TOPK]],\n",
        "          f\"| P@5={p5_lsa:.3f}\")\n",
        "\n",
        "print(\"\\n[TASK1 RESULT_CHECKING_POINT] Precision@5 (average across queries):\")\n",
        "print(f\"  TF-IDF: ({np.mean(tfidf_p5_scores):.3f})\")\n",
        "print(f\"  BM25:   ({np.mean(bm25_p5_scores):.3f})\")\n",
        "print(f\"  LSA:    ({np.mean(lsa_p5_scores):.3f})\")\n"
      ],
      "metadata": {
        "id": "8CAjArbU7xAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d99c73-dd70-4ead-b172-7492874ec8a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TASK1] Vector Size Comparison:\n",
            "  TF-IDF dim   = 441\n",
            "  BM25 dim     = 441\n",
            "  LSA dim (k)  = 10\n",
            "\n",
            "[TASK1] Query='how do i fix problems on my mac like disk failures, monitor issues and software failures' | (label=mac)\n",
            "   TF-IDF top-5 labels: ['space', 'autos', 'mac', 'mac', 'space'] | P@5=0.400\n",
            "   BM25  top-5 labels: ['space', 'mac', 'space', 'mac', 'autos'] | P@5=0.400\n",
            "   LSA   top-5 labels: ['autos', 'mac', 'space', 'mac', 'space'] | P@5=0.400\n",
            "\n",
            "[TASK1] Query='latest updates on nasa missions space shuttle launches and orbital science experiments' | (label=space)\n",
            "   TF-IDF top-5 labels: ['space', 'space', 'autos', 'hockey', 'hockey'] | P@5=0.400\n",
            "   BM25  top-5 labels: ['space', 'space', 'autos', 'space', 'hockey'] | P@5=0.600\n",
            "   LSA   top-5 labels: ['space', 'space', 'autos', 'autos', 'mac'] | P@5=0.400\n",
            "\n",
            "[TASK1] Query='advice on car performance engine reliability and buying a new vehicle' | (label=autos)\n",
            "   TF-IDF top-5 labels: ['mac', 'autos', 'autos', 'space', 'space'] | P@5=0.400\n",
            "   BM25  top-5 labels: ['autos', 'mac', 'space', 'autos', 'space'] | P@5=0.400\n",
            "   LSA   top-5 labels: ['autos', 'mac', 'mac', 'hockey', 'autos'] | P@5=0.400\n",
            "\n",
            "[TASK1] Query='discussion about hockey teams playoffs goalies and physical play on the ice' | (label=hockey)\n",
            "   TF-IDF top-5 labels: ['hockey', 'space', 'hockey', 'autos', 'space'] | P@5=0.400\n",
            "   BM25  top-5 labels: ['hockey', 'space', 'autos', 'hockey', 'space'] | P@5=0.400\n",
            "   LSA   top-5 labels: ['hockey', 'space', 'space', 'autos', 'hockey'] | P@5=0.400\n",
            "\n",
            "[TASK1 RESULT_CHECKING_POINT] Precision@5 (average across queries):\n",
            "  TF-IDF: (0.400)\n",
            "  BM25:   (0.450)\n",
            "  LSA:    (0.400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TESTS & EXPECTED OUTPUT -------------------------\n",
        "# [TASK1] Vector Size Comparison:\n",
        "#   TF-IDF dim   = 441\n",
        "#   BM25 dim     = 441\n",
        "#   LSA dim (k)  = 10\n",
        "#\n",
        "# [TASK1] Query='how do i fix problems on my mac ...' | (label=mac)\n",
        "#    TF-IDF top-5 document labels: ['space', 'autos', ...] | P@5=0.400\n",
        "#    BM25  top-5 document labels: ['space', 'mac', ...] | P@5=0.400\n",
        "#    LSA   top-5 document labels: ['space', 'mac', ...] | P@5=0.400\n",
        "#\n",
        "# ...\n",
        "#\n",
        "# [TASK1] Query='discussion about hockey teams ...' | (label=hockey)\n",
        "#    TF-IDF top-5 document labels: ['hockey', 'space', ...] | P@5=0.400\n",
        "#    BM25  top-5 document labels: ['hockey', 'space', ...] | P@5=0.400\n",
        "#    LSA   top-5 document labels: ['hockey', 'space', ...] | P@5=0.400\n",
        "#\n",
        "# [TASK1 RESULT_CHECKING_POINT] Precision@5 (average across queries):\n",
        "#   TF-IDF: (0.400)\n",
        "#   BM25:   (0.450)\n",
        "#   LSA:    (0.400)\n",
        "# ---------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "HCAb9ksI0zMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 2 (7 pts): Word2Vec (CBOW) with PyTorch\n",
        "\n",
        "## Goal\n",
        "Train a **CBOW** model on the **train subset of 20 Newsgroups** (**exactly 100 docs per topic, truncated**) using **PyTorch** and NumPy utilities.\n",
        "\n",
        "**Do not** use external libraries that already implement CBOW (e.g., *gensim*).  \n",
        "\n",
        "- **Input:** mean of embeddings of the `2W` context tokens around a center word  \n",
        "- **Output:** logits over the whole vocabulary predicting the **center** word  \n",
        "- **Loss:** cross-entropy (`nn.CrossEntropyLoss`)  \n",
        "- **Optimization:** **Adam optimizer** with **mini-batches** (`DataLoader` is used to easily prepare and load mini-batches)\n",
        "\n",
        "We use a **fixed window** of size `W` on each side; contexts are **exactly `2W` tokens**.  \n",
        "Positions that don’t have enough neighbors (near boundaries) are **skipped** — no padding/masking needed.\n",
        "\n",
        "We also use a **naïve softmax** output (complexity **$O(|V|)$** per update) — this is **okay** for our small setup.\n",
        "\n",
        "### Parameter shapes\n",
        "Let $|V|$ be the vocabulary size and $d$ the embedding dimension:\n",
        "- $ W_{\\text{in}} \\in \\mathbb{R}^{|V|\\times d} $\n",
        "- $ W_{\\text{out}} \\in \\mathbb{R}^{|V|\\times d} $\n",
        "- $ b_{\\text{out}} \\in \\mathbb{R}^{|V|} $\n",
        "\n",
        "## What you implement\n",
        "1. **Vocab & data**\n",
        "   - Build `(word → index)` / `(index → word)` from the **train** texts (already implemented for you).\n",
        "   - Create CBOW pairs `(context_indices, target_index)` using a symmetric window of size `W`.\n",
        "   - **Skip** positions that don’t yield a full `2W`-token context (typically happens at edge positions or short sentences).\n",
        "\n",
        "2. **Model (PyTorch)**\n",
        "   - `nn.Embedding(|V|, d)` → **mean over the `2W` context embeddings** → `nn.Linear(d, |V|)` → logits.\n",
        "   - Use `nn.CrossEntropyLoss()` (softmax is automatically calculated in this utility).\n",
        "\n",
        "3. **Training (mini-batches + Adam)**\n",
        "   - Wrap pairs in a `Dataset`/`DataLoader` (`batch_size = B`).\n",
        "   - Train for `E` epochs with `torch.optim.Adam(model.parameters(), lr=lr)`.\n",
        "   - Print average training loss per epoch.\n",
        "\n",
        "4. **Embedding helper**\n",
        "   - `embed_text_mean(tokens, cbow_model)`: average learned **input** embeddings to represent any text (it will be used in Task 3).\n",
        "\n",
        "## Grading (7 pts)\n",
        "- **2.1** Data maker (windowing, indexing) — **2 pts**\n",
        "- **2.2** Training loop (mini-batches + Adam) — **3 pts**\n",
        "- **2.3** Embedding helper (mean of tokens embeddings) — **2 pts**  \n",
        "\n",
        "\n",
        "> **Notes**\n",
        "> - We will compare CBOW vs LSA retrieval in **Task 3**.\n"
      ],
      "metadata": {
        "id": "NG7gjRvSFuYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -----------------------------\n",
        "# Reproducibility\n",
        "# -----------------------------\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Vocab utilities for CBOW, build ing (word -> index) and (index -> word)\n",
        "# -----------------------------\n",
        "def build_w2v_vocab(docs: List[Doc], min_count: int = 1\n",
        "                   ) -> Tuple[Dict[str,int], Dict[int,str]]:\n",
        "    \"\"\"\n",
        "    Build a vocabulary from TRAIN_DOCS.\n",
        "    Returns:\n",
        "        w2i: word -> index  (0..|V|-1)\n",
        "        i2w: index -> word\n",
        "    \"\"\"\n",
        "    w2i, i2w, freq = build_vocab(docs, min_count=min_count)\n",
        "    return w2i, i2w\n",
        "\n",
        "def cbow_training_data_fixed_window(docs: List[Doc],\n",
        "                                    w2i: Dict[str,int],\n",
        "                                    window: int = 2\n",
        "                                   ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Create CBOW pairs with a fixed-size symmetric window of size `window`.\n",
        "    For each token position c, we require exactly `2*window` context tokens.\n",
        "    Positions that don't have enough neighbors are skipped.\n",
        "\n",
        "    Returns:\n",
        "        contexts: np.ndarray of shape (N, 2*window) with word indices\n",
        "        targets : np.ndarray of shape (N,) with target word indices\n",
        "    \"\"\"\n",
        "    contexts, targets = [], []\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    for doc in docs:\n",
        "        tokens = doc.tokens\n",
        "        if len(tokens) < 2 * window + 1:\n",
        "            continue\n",
        "        for c in range(window, len(tokens) - window):\n",
        "            # left context: window tokens before c\n",
        "            left_ctx = tokens[c - window : c]\n",
        "            # right context: window tokens after c\n",
        "            right_ctx = tokens[c + 1 : c + 1 + window]\n",
        "            ctx_tokens = left_ctx + right_ctx  # total length should be 2*window\n",
        "\n",
        "            ctx_indices = []\n",
        "            for t in ctx_tokens:\n",
        "                if t not in w2i:\n",
        "                    break\n",
        "                ctx_indices.append(w2i[t])\n",
        "\n",
        "            # If any token was missing, skip this pair\n",
        "            if len(ctx_indices) != 2 * window:\n",
        "                continue\n",
        "            target_token = tokens[c]\n",
        "            if target_token not in w2i:\n",
        "                continue\n",
        "            target_idx = w2i[target_token]\n",
        "\n",
        "            contexts.append(ctx_indices)\n",
        "            targets.append(target_idx)\n",
        "\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return np.array(contexts, dtype=np.int64), np.array(targets, dtype=np.int64)\n",
        "\n",
        "# -----------------------------\n",
        "# PyTorch Dataset Format\n",
        "# -----------------------------\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, contexts: np.ndarray, targets: np.ndarray):\n",
        "        self.contexts = contexts\n",
        "        self.targets  = targets\n",
        "    def __len__(self) -> int:\n",
        "        return self.targets.shape[0]\n",
        "    def __getitem__(self, idx: int):\n",
        "        x = torch.tensor(self.contexts[idx], dtype=torch.long)\n",
        "        y = torch.tensor(self.targets[idx],  dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# -----------------------------\n",
        "# CBOW Model (Embedding mean -> Linear)\n",
        "# -----------------------------\n",
        "class CBOWTorch(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.in_embed = nn.Embedding(vocab_size, dim) # (|V|, d)\n",
        "        self.out = nn.Linear(dim, vocab_size)         # (d, |V|)\n",
        "        # The following lines are optional but common in word embedding models:\n",
        "        # Small std (0.01) keeps initial logits near zero so softmax isn't saturated.\n",
        "        # This stabilizes early training (gradients stay useful and not too peaky).\n",
        "        nn.init.normal_(self.in_embed.weight, mean=0.0, std=0.01)\n",
        "        nn.init.normal_(self.out.weight,      mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.out.bias)\n",
        "\n",
        "    def forward(self, ctx_idxs: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        ctx_idxs: (B, 2W) tensor of word indices\n",
        "        Returns:\n",
        "            logits: (B, |V|)\n",
        "        \"\"\"\n",
        "        emb = self.in_embed(ctx_idxs)        # (B, 2W, d)\n",
        "        h   = emb.mean(dim=1)                # (B, d)   mean over context tokens\n",
        "        logits = self.out(h)                 # (B, |V|)\n",
        "        return logits\n",
        "\n",
        "@dataclass\n",
        "class CBOWModelTorch:\n",
        "    \"\"\"Light wrapper to carry the trained model and basic metadata.\"\"\"\n",
        "    w2i: Dict[str,int]\n",
        "    i2w: Dict[int,str]\n",
        "    model: nn.Module\n",
        "    dim: int\n",
        "    window: int\n",
        "\n",
        "# -----------------------------\n",
        "# CBOW Model Training Function\n",
        "# -----------------------------\n",
        "def cbow_train_torch(train_docs: List[Doc],\n",
        "                     dim: int        = 100,\n",
        "                     window: int     = 2,\n",
        "                     batch_size: int = 256,\n",
        "                     epochs: int     = 10,\n",
        "                     lr: float       = 1e-3,\n",
        "                     seed: int       = RANDOM_SEED\n",
        "                    ) -> CBOWModelTorch:\n",
        "    \"\"\"\n",
        "    Train CBOW with fixed-size windows using Adam and mini-batches.\n",
        "    \"\"\"\n",
        "    # Build vocab and pairs\n",
        "    w2i, i2w = build_w2v_vocab(train_docs)\n",
        "    contexts, targets = cbow_training_data_fixed_window(train_docs, w2i, window=window)\n",
        "    print(f\"[TASK2] CBOW data: {len(targets)} pairs | V={len(w2i)} | d={dim} | window={window}\")\n",
        "\n",
        "    ds = CBOWDataset(contexts, targets)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    vocab_size = len(w2i)\n",
        "    model = CBOWTorch(vocab_size=vocab_size, dim=dim).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_contexts, batch_targets in dl:\n",
        "            # Move data to DEVICE\n",
        "            batch_contexts = batch_contexts.to(DEVICE)  # shape: (B, 2W)\n",
        "            batch_targets = batch_targets.to(DEVICE)    # shape: (B,)\n",
        "\n",
        "            # Reset gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: get logits for each word in vocabulary\n",
        "            logits = model(batch_contexts)  # (B, |V|)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(logits, batch_targets)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / max(1, num_batches)\n",
        "        print(f\"[TASK2] Epoch {epoch+1}/{epochs} | avg loss = {avg_loss:.4f}\")\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return CBOWModelTorch(w2i=w2i, i2w=i2w, model=model, dim=dim, window=window)\n",
        "\n",
        "# -----------------------------\n",
        "# Embedding helper for downstream tasks\n",
        "# -----------------------------\n",
        "def embed_text_mean(tokens: List[str], cbow: CBOWModelTorch) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Average input embeddings for provided tokens.\n",
        "    Returns a numpy vector of shape (d,).\n",
        "    \"\"\"\n",
        "    vec = np.zeros(cbow.dim, dtype=float)\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    total_embd = np.zeros(cbow.dim, dtype=float) #total embeddings\n",
        "    count = 0\n",
        "    for token in tokens:\n",
        "        if token in cbow.w2i:\n",
        "            token_idx = cbow.w2i[token]\n",
        "            token_embedding = cbow.model.in_embed.weight[token_idx].detach().cpu().numpy()\n",
        "            total_embd += token_embedding\n",
        "            count += 1\n",
        "\n",
        "    if count > 0:\n",
        "        vec = total_embd / count\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return vec\n",
        "\n",
        "# -----------------------------\n",
        "# Train on TRAIN_DOCS (example settings)\n",
        "# -----------------------------\n",
        "cbow = cbow_train_torch(\n",
        "    TRAIN_DOCS,\n",
        "    dim=100,          # embedding dim\n",
        "    window=2,         # context window on each side\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        "    lr=1e-3,\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# Quick sanity: embed the first train doc\n",
        "ex_vec = embed_text_mean(TRAIN_DOCS[0].tokens, cbow)\n",
        "print(\"[TASK2 RESULT_CHECKING_POINT] Embedding shape:\", ex_vec.shape)\n"
      ],
      "metadata": {
        "id": "YoujzX7LBIFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6a5895-f95e-4882-b5a5-023643a18cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TASK2] CBOW data: 15152 pairs | V=4015 | d=100 | window=2\n",
            "[TASK2] Epoch 1/10 | avg loss = 8.2417\n",
            "[TASK2] Epoch 2/10 | avg loss = 7.7855\n",
            "[TASK2] Epoch 3/10 | avg loss = 7.1006\n",
            "[TASK2] Epoch 4/10 | avg loss = 6.8333\n",
            "[TASK2] Epoch 5/10 | avg loss = 6.7604\n",
            "[TASK2] Epoch 6/10 | avg loss = 6.7140\n",
            "[TASK2] Epoch 7/10 | avg loss = 6.6703\n",
            "[TASK2] Epoch 8/10 | avg loss = 6.6275\n",
            "[TASK2] Epoch 9/10 | avg loss = 6.6071\n",
            "[TASK2] Epoch 10/10 | avg loss = 6.5659\n",
            "[TASK2 RESULT_CHECKING_POINT] Embedding shape: (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TESTS & EXPECTED OUTPUT -------------------------\n",
        "# [TASK2] CBOW data: 15152 pairs | V=4015 | d=100 | window=2\n",
        "#   epoch 04/10 | avg_loss6.8..\n",
        "#   ...\n",
        "#   epoch 10/10 | avg_loss=6.5..\n",
        "# [TASK2 RESULT_CHECKING_POINT] Embedding shape: (100,)\n",
        "# ---------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "BwW7L9jZH5mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 3 (7 pts): MAP implementation — compare & evaluate LSA vs CBOW on the test set\n",
        "\n",
        "**Reminder:** You must finish Task 1 (to get `lsa_model`) and Task 2 (to get `cbow`) before starting Task 3. We will use those two models here.\n",
        "\n",
        "## Goal\n",
        "1. Implement **Average Precision (AP)** and **Mean Average Precision (MAP)**.\n",
        "2. Use them to evaluate and compare:\n",
        "   - **LSA retrieval** (from Task 1, using TruncatedSVD on TF-IDF)\n",
        "   - **CBOW retrieval** (from Task 2, using learned word embeddings)\n",
        "3. Report how well each model retrieves relevant documents for each query.\n",
        "4. Compare their **embedding dimensionality**:\n",
        "   - LSA works in a low-rank semantic space of size `k`.\n",
        "   - CBOW uses learned embeddings of size `d`.\n",
        "\n",
        "## Retrieval setup\n",
        "- We have `TEST_DOCS` and `QUERIES`.\n",
        "- For each query `(query_text, label)`:\n",
        "  - A test doc is **relevant** if `doc.label == label`.\n",
        "  - We produce a ranked list of all test docs.\n",
        "\n",
        "**LSA ranking (Task 1 model):**\n",
        "1. Convert the query to TF-IDF with `lsa_model.vectorizer`.\n",
        "2. Project it into the LSA space with `lsa_model.svd`.\n",
        "3. Compute cosine similarity with each document embedding in `lsa_model.doc_embeddings`.\n",
        "4. Sort by similarity (highest first).\n",
        "\n",
        "**CBOW ranking (Task 2 model):**\n",
        "1. Embed the query by averaging the CBOW input embeddings (`W_in`) of its tokens.\n",
        "2. Embed each test doc the same way.\n",
        "3. Rank by cosine similarity.\n",
        "\n",
        "## Evaluation metrics\n",
        "Let the ranked documents for a query be $ d_1, d_2, \\dots, d_n $, and let $ R $ be the set of relevant doc indices.\n",
        "\n",
        "**Precision@k** is:\n",
        "$$\n",
        "\\mathrm{P@}k \\;=\\; \\frac{TP@k}{TP@k + FP@k}.\n",
        "$$\n",
        "\n",
        "Define the **relevance indicator**\n",
        "$$\n",
        "\\mathrm{rel}_k \\;=\\;\n",
        "\\begin{cases}\n",
        "1 & \\text{if } d_k \\in R,\\\\\n",
        "0 & \\text{otherwise.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Average Precision (AP)** for one query is:\n",
        "$$\n",
        "\\mathrm{AP}\n",
        "\\;=\\;\n",
        "\\frac{1}{|R|}\n",
        "\\sum_{k=1}^{n}\n",
        "\\mathrm{P@}k \\cdot \\mathrm{rel}_k.\n",
        "$$\n",
        "\n",
        "**Mean Average Precision (MAP)** over all queries is:\n",
        "$$\n",
        "\\mathrm{MAP} = \\frac{1}{Q} \\sum_{q=1}^{Q} \\mathrm{AP}_q\n",
        "$$\n",
        "\n",
        "We will:\n",
        "- compute AP for each query,\n",
        "- average to get MAP for **LSA** and **CBOW**\n",
        "\n",
        "### Grading (7 pts)\n",
        "- **3.1** `average_precision` and `mean_average_precision` — **3 pts**  \n",
        "- **3.2** LSA ranking with MAP — **2 pts**  \n",
        "- **3.3** CBOW ranking with MAP — **2 pts**\n"
      ],
      "metadata": {
        "id": "57BuaAlg8iLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# AP and MAP implementation\n",
        "# -------------------------------------------------\n",
        "def average_precision(ranking: List[int], relevant: set) -> float:\n",
        "    \"\"\"\n",
        "    Compute Average Precision (AP) for a single query.\n",
        "    ranking: list of doc indices in ranked order (best first).\n",
        "    relevant: set of doc indices considered relevant for this query.\n",
        "\n",
        "    AP = (1/|R|) * sum_k Precision@k * rel_k\n",
        "\n",
        "    In the code, rel_k is handled implicitly:\n",
        "    We only update the sum when rel_k = 1.\n",
        "    When rel_k = 0, we skip adding anything.\n",
        "    \"\"\"\n",
        "    if not ranking or not relevant:\n",
        "        return 0.0\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    num_relevant = len(relevant)\n",
        "    hits = 0\n",
        "    sum_precisions = 0.0\n",
        "\n",
        "    for k, doc_idx in enumerate(ranking, start=1):\n",
        "        if doc_idx in relevant: #rel_k\n",
        "            hits += 1\n",
        "            precision_at_k = hits / k\n",
        "            sum_precisions += precision_at_k\n",
        "\n",
        "    # If there are no relevant docs, AP is defined as 0.0\n",
        "    if num_relevant == 0:\n",
        "        return 0.0\n",
        "\n",
        "    ap = sum_precisions / num_relevant\n",
        "    return ap\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def mean_average_precision(all_rankings: List[List[int]],\n",
        "                           all_relevant: List[set]) -> float:\n",
        "    \"\"\"\n",
        "    Compute MAP across multiple queries.\n",
        "    all_rankings[i] is the ranked doc indices for query i\n",
        "    all_relevant[i] is the set of relevant doc indices for query i\n",
        "\n",
        "    MAP = mean_i AP_i\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    if not all_rankings or not all_relevant:\n",
        "        return 0.0\n",
        "\n",
        "    aps = []\n",
        "    for ranking, rel in zip(all_rankings, all_relevant):\n",
        "        ap = average_precision(ranking, rel)\n",
        "        aps.append(ap)\n",
        "\n",
        "    if len(aps) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return float(np.mean(aps))\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Retrieval with LSA and CBOW for MAP evaluation\n",
        "# -------------------------------------------------\n",
        "\n",
        "# CBOW embeddings for all TEST_DOCS\n",
        "CBOW_TEST_EMBEDS = [embed_text_mean(d.tokens, cbow) for d in TEST_DOCS]  # (num_docs, d)\n",
        "\n",
        "# LSA embeddings for all TEST_DOCS were already computed in lsa_model.doc_embeddings\n",
        "LSA_TEST_EMBEDS = lsa_model.doc_embeddings  # shape: (num_docs, lsa_model.k)\n",
        "\n",
        "\n",
        "def rank_with_lsa_map(query_text: str,\n",
        "                      lsa_model: LSAModel,\n",
        "                      test_embeds: np.ndarray) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank TEST_DOCS for a query using the LSA semantic space.\n",
        "    1) TF-IDF the query with lsa_model.vectorizer\n",
        "    2) Project to LSA with lsa_model.svd\n",
        "    3) Cosine sim vs each doc embedding (test_embeds)\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    query_tfidf = lsa_model.vectorizer.transform([query_text])  # shape: (1, vocab_size)\n",
        "    query_lsa = lsa_model.svd.transform(query_tfidf)[0]  # shape: (k,)\n",
        "\n",
        "    scores = []\n",
        "    for doc_emb in test_embeds:  # each doc_emb: shape (k,)\n",
        "        scores.append(cosine(doc_emb, query_lsa))\n",
        "\n",
        "    scores = np.array(scores)\n",
        "\n",
        "    order = np.argsort(-scores)\n",
        "    return order.tolist()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def rank_with_cbow_map(query_text: str,\n",
        "                       cbow_model: CBOWModelTorch,\n",
        "                       test_embeds: List[np.ndarray]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank TEST_DOCS for a query using CBOW embeddings.\n",
        "    1) Average word vectors for the query text\n",
        "    2) Cosine similarity vs each precomputed doc embedding\n",
        "    3) Sort high -> low\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    query_tokens = tokenize(query_text)\n",
        "    query_vec = embed_text_mean(query_tokens, cbow_model)  # shape: (d,)\n",
        "\n",
        "    scores = []\n",
        "    for doc_vec in test_embeds:  # each doc_vec: shape (d,)\n",
        "        scores.append(cosine(doc_vec, query_vec))\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    order = np.argsort(-scores)\n",
        "    return order.tolist()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Dimensionality comparison\n",
        "# -------------------------------------------------\n",
        "lsa_dim  = lsa_model.k      # LSA latent dimension k\n",
        "cbow_dim = cbow.dim         # CBOW embedding dimension d\n",
        "\n",
        "print(\"[TASK3] Embedding dimensionality:\")\n",
        "print(f\"  LSA  dim (k) = {lsa_dim}\")\n",
        "print(f\"  CBOW dim (d) = {cbow_dim}\")\n",
        "print()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# MAP evaluation + reporting\n",
        "# -------------------------------------------------\n",
        "lsa_rankings  = []\n",
        "cbow_rankings = []\n",
        "rel_sets      = []\n",
        "\n",
        "TOPK = 5\n",
        "\n",
        "for qtext, label in QUERIES:\n",
        "    rel = set(relevant_doc_indices(TEST_DOCS, label))\n",
        "    rel_sets.append(rel)\n",
        "\n",
        "    # get rankings\n",
        "    lsa_order  = rank_with_lsa_map(qtext, lsa_model, LSA_TEST_EMBEDS)\n",
        "    cbow_order = rank_with_cbow_map(qtext, cbow, CBOW_TEST_EMBEDS)\n",
        "\n",
        "    lsa_rankings.append(lsa_order)\n",
        "    cbow_rankings.append(cbow_order)\n",
        "\n",
        "    # inspect per-query behavior\n",
        "    print(f\"[TASK3] Query='{qtext}' | (label={label})\")\n",
        "\n",
        "    print(\"   LSA  top-5 document labels:\", [TEST_DOCS[i].label for i in lsa_order[:TOPK]])\n",
        "\n",
        "    print(\"   CBOW top-5 document labels:\", [TEST_DOCS[i].label for i in cbow_order[:TOPK]])\n",
        "    print()\n",
        "\n",
        "# Compute MAP\n",
        "map_lsa  = mean_average_precision(lsa_rankings,  rel_sets)\n",
        "map_cbow = mean_average_precision(cbow_rankings, rel_sets)\n",
        "\n",
        "print(f\"[TASK3 RESULT_CHECKING_POINT] MAP_LSA={map_lsa:.3f} | MAP_CBOW={map_cbow:.3f}\")"
      ],
      "metadata": {
        "id": "2e4gNNjsIu71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32559126-e818-4b7a-e3b2-ac9211a6c4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TASK3] Embedding dimensionality:\n",
            "  LSA  dim (k) = 10\n",
            "  CBOW dim (d) = 100\n",
            "\n",
            "[TASK3] Query='how do i fix problems on my mac like disk failures, monitor issues and software failures' | (label=mac)\n",
            "   LSA  top-5 document labels: ['autos', 'mac', 'space', 'mac', 'space']\n",
            "   CBOW top-5 document labels: ['mac', 'mac', 'mac', 'mac', 'mac']\n",
            "\n",
            "[TASK3] Query='latest updates on nasa missions space shuttle launches and orbital science experiments' | (label=space)\n",
            "   LSA  top-5 document labels: ['space', 'space', 'autos', 'autos', 'mac']\n",
            "   CBOW top-5 document labels: ['mac', 'mac', 'mac', 'mac', 'mac']\n",
            "\n",
            "[TASK3] Query='advice on car performance engine reliability and buying a new vehicle' | (label=autos)\n",
            "   LSA  top-5 document labels: ['autos', 'mac', 'mac', 'hockey', 'autos']\n",
            "   CBOW top-5 document labels: ['mac', 'mac', 'mac', 'mac', 'mac']\n",
            "\n",
            "[TASK3] Query='discussion about hockey teams playoffs goalies and physical play on the ice' | (label=hockey)\n",
            "   LSA  top-5 document labels: ['hockey', 'space', 'space', 'autos', 'hockey']\n",
            "   CBOW top-5 document labels: ['mac', 'mac', 'mac', 'mac', 'mac']\n",
            "\n",
            "[TASK3 RESULT_CHECKING_POINT] MAP_LSA=0.506 | MAP_CBOW=0.434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TESTS & EXPECTED OUTPUT -------------------------\n",
        "# [TASK3] Embedding dimensionality:\n",
        "#   LSA  dim (k) = 10\n",
        "#   CBOW dim (d) = 100\n",
        "#\n",
        "# [TASK3] Query='how do i fix problems on my mac ...' | (label=mac)\n",
        "#    LSA top-5 document labels: ['autos', 'mac', ...]\n",
        "#    CBOW top-5 document labels: ['mac', 'space', ...]\n",
        "#\n",
        "# ...\n",
        "#\n",
        "# [TASK3] Query='discussion about hockey teams ...'  | (label=hockey)\n",
        "#    LSA top-5 document labels: ['hockey', 'space', ...]\n",
        "#    CBOW top-5 document labels: ['hockey', 'autos', ...]\n",
        "#\n",
        "# [TASK3 RESULT_CHECKING_POINT] MAP_LSA=0.506 | MAP_CBOW=0.411\n",
        "# ---------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "gPv3N2UiJO3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS (2 pts): Solve any task with an LLM\n",
        "\n",
        "**Goal.**  \n",
        "Pick **one** of the homework tasks (Task 1, Task 2, or Task 3) and solve it using an **LLM**. Provide the **LLM** with the **task description** and any **starter/prerequisite code** it depends on, ask it to generate a complete **code solution** first, then run that **generated code** here in the code notebook yourself. Finally, document what you did and **compare** the LLM’s result to your own pipeline.\n",
        "\n",
        "**What to deliver below.**\n",
        "1) **LLM used** (name + version, e.g., “Llama-3-8B-Instruct”, “GPT-x”, “Claude-x”, “Mistral-x”, etc.).  \n",
        "2) **Prompt(s)** you used.  \n",
        "3) **LLM output** — copy and paste the generated code.  \n",
        "4) **Comparison** to your solution: what matches or differs (quantitative or qualitative).  \n",
        "5) **Reflection**: what the LLM was **good at** vs **bad at**, what it got **right** vs **wrong**.\n",
        "\n",
        "> **No code required.** You do **not** need to run, share, or submit any code used for the LLM generation. Provide only the deliverables listed above.\n",
        "> You may use any LLMs through any interface (API, web UI, local inference).\n"
      ],
      "metadata": {
        "id": "t6YRB0GARyHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Google Gemini (Gemini 2.0 / Gemini Pro — Free Version) to generate the solution.\n",
        "\n",
        "**Prompt:**\n",
        "```\n",
        "I need you to solve the following homework task Ranked Retrieval with TF-IDF, BM25, and LSA on 20 Newsgroups\n",
        "\n",
        "Goal:\n",
        "Implement ranked retrieval over a small 20 Newsgroups subset using:\n",
        "TF-IDF (scikit-learn TfidfVectorizer)\n",
        "BM25 (rank_bm25.BM25Okapi)\n",
        "LSA (TruncatedSVD on the TF-IDF matrix)\n",
        "\n",
        "I already have:\n",
        "A Doc dataclass: (text, tokens, label)\n",
        "A tokenizer() function\n",
        "TEST_DOCS: list of 20 Docs\n",
        "QUERIES: list of (query_text, label)\n",
        "cosine() helper\n",
        "relevant_doc_indices() helper\n",
        "\n",
        "I need complete implementations of:\n",
        "\n",
        "    def fit_tfidf_ranker(...)\n",
        "    def rank_with_tfidf(...)\n",
        "    def fit_bm25_ranker(...)\n",
        "    def rank_with_bm25(...)\n",
        "    def fit_lsa_ranker(...)\n",
        "    def rank_with_lsa(...)\n",
        "\n",
        "Requirements:\n",
        "TF-IDF uses tokenizer=tokenize, lowercase=False\n",
        "BM25 uses document.tokens lists\n",
        "LSA:\n",
        "- use same TF-IDF vectorizer\n",
        "- apply TruncatedSVD(k)\n",
        "- convert query → TF-IDF → SVD\n",
        "- rank by cosine similarity\n",
        "\n",
        "Output:\n",
        "Give full Python code that implements these functions cleanly.\n",
        "Do NOT modify function signatures.\n",
        "Do NOT include training Word2Vec or MAP calculations.\n",
        "Include only the code needed for TF-IDF, BM25, and LSA ranking.\n",
        "\n",
        "Produce the final Python code now.\n",
        "```\n",
        "**LLM Output:**\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Callable, Any\n",
        "from dataclasses import dataclass\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# --- MOCKUP CLASSES/FUNCTIONS (Provided/Assumed to exist) ---\n",
        "# NOTE: The actual Doc, tokenizer, cosine, relevant_doc_indices,\n",
        "# TEST_DOCS, QUERIES, and RANDOM_SEED are assumed to be defined\n",
        "# in the user's environment. We define minimal mock-ups for code completeness.\n",
        "\n",
        "@dataclass\n",
        "class Doc:\n",
        "    text: str\n",
        "    tokens: List[str]\n",
        "    label: str\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\"Mock tokenizer function.\"\"\"\n",
        "    # Assuming the tokenizer returns token lists for text\n",
        "    # In a real scenario, this would handle splitting, stemming, etc.\n",
        "    return text.lower().split()\n",
        "\n",
        "def cosine(v1: np.ndarray, v2: np.ndarray) -> float:\n",
        "    \"\"\"Mock cosine similarity function (assumed to be provided).\"\"\"\n",
        "    # Assuming v1 and v2 are 1D arrays\n",
        "    norm_v1 = np.linalg.norm(v1)\n",
        "    norm_v2 = np.linalg.norm(v2)\n",
        "    if norm_v1 == 0 or norm_v2 == 0:\n",
        "        return 0.0\n",
        "    return np.dot(v1, v2) / (norm_v1 * norm_v2)\n",
        "\n",
        "# Global constant assumed to be defined\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "@dataclass\n",
        "class LSAModel:\n",
        "    \"\"\"Container for LSA components.\"\"\"\n",
        "    vectorizer: TfidfVectorizer\n",
        "    svd: TruncatedSVD\n",
        "    doc_embeddings: np.ndarray\n",
        "    k: int\n",
        "\n",
        "\n",
        "# --- TF-IDF Ranked Retrieval ---\n",
        "\n",
        "def fit_tfidf_ranker(docs: List[Doc]) -> Tuple[TfidfVectorizer, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Fit a TfidfVectorizer on the given docs (retrieval corpus).\n",
        "    Returns:\n",
        "        vectorizer: fitted TfidfVectorizer\n",
        "        doc_matrix: TF-IDF matrix of shape (num_docs, vocab_size)\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False # Requirement: lowercase=False\n",
        "    )\n",
        "    # Get the raw text from the Doc objects\n",
        "    corpus_text = [doc.text for doc in docs]\n",
        "    \n",
        "    # Fit the vectorizer on the corpus and transform it\n",
        "    doc_matrix = vectorizer.fit_transform(corpus_text)\n",
        "    \n",
        "    # Convert the sparse matrix to a dense NumPy array for simpler calculation later\n",
        "    doc_matrix = doc_matrix.toarray()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return vectorizer, doc_matrix\n",
        "\n",
        "\n",
        "def rank_with_tfidf(query_text: str,\n",
        "                    vectorizer: TfidfVectorizer,\n",
        "                    doc_matrix: np.ndarray) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs using aggregated TF-IDF weights.\n",
        "    You can use the dot product between the query vector and TF-IDF matrix.\n",
        "    Returns:\n",
        "        order: list of doc indices sorted by score desc\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    # 1. Transform the query text into a TF-IDF vector\n",
        "    # This results in a sparse matrix of shape (1, vocab_size)\n",
        "    query_vector_sparse = vectorizer.transform([query_text])\n",
        "    \n",
        "    # Convert to a dense NumPy array of shape (1, vocab_size)\n",
        "    query_vector = query_vector_sparse.toarray()\n",
        "    \n",
        "    # 2. Calculate scores: Dot product between the query vector and the doc matrix\n",
        "    # doc_matrix is (num_docs, vocab_size).\n",
        "    # query_vector is (1, vocab_size).\n",
        "    # The matrix multiplication (dot product) gives (1, num_docs).\n",
        "    # Since dot product is equivalent to cosine similarity * product of norms,\n",
        "    # and we are not normalizing the documents (which is often done for simpler\n",
        "    # cosine similarity ranking), we use the dot product as the ranking score.\n",
        "    # We transpose the doc_matrix to get (vocab_size, num_docs) for the dot product\n",
        "    # to yield (1, num_docs), or simply use np.dot(doc_matrix, query_vector.T)\n",
        "    # to get (num_docs, 1) scores. Let's use the latter.\n",
        "    \n",
        "    # Scores array of shape (num_docs,)\n",
        "    scores = np.dot(doc_matrix, query_vector.T).flatten()\n",
        "    \n",
        "    # 3. Get the indices that would sort the scores in descending order\n",
        "    # np.argsort returns indices that sort in ascending order,\n",
        "    # so we use [::-1] to reverse the result.\n",
        "    order = np.argsort(scores)[::-1].tolist()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return order\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# BM25 ranked retrieval (rank_bm25)\n",
        "# -----------------\n",
        "def fit_bm25_ranker(docs: List[Doc]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Fit BM25 on the given docs (list of token lists).\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    # Requirement: BM25 uses document.tokens lists\n",
        "    tokenized_corpus = [doc.tokens for doc in docs]\n",
        "    \n",
        "    # Initialize and fit the BM25 model\n",
        "    bm25_model = BM25Okapi(tokenized_corpus)\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return bm25_model\n",
        "\n",
        "\n",
        "def rank_with_bm25(query_text: str,\n",
        "                    bm25_model: BM25Okapi) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs by aggregated BM25 score.\n",
        "    Returns:\n",
        "        order: doc indices (0..num_docs-1) sorted by score desc\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    # 1. Tokenize the query text using the assumed tokenizer\n",
        "    query_tokens = tokenize(query_text)\n",
        "    \n",
        "    # 2. Calculate the scores for all documents\n",
        "    # get_scores returns a NumPy array of shape (num_docs,)\n",
        "    scores = bm25_model.get_scores(query_tokens)\n",
        "    \n",
        "    # 3. Get the indices that would sort the scores in descending order\n",
        "    order = np.argsort(scores)[::-1].tolist()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return order\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# LSA Ranked Retrieval\n",
        "# -----------------\n",
        "def fit_lsa_ranker(vectorizer: TfidfVectorizer,\n",
        "                   doc_matrix: np.ndarray,  # TF-IDF matrix constructed before\n",
        "                   k: int=10,\n",
        "                   random_state: int=RANDOM_SEED) -> LSAModel:\n",
        "    \"\"\"\n",
        "    Steps:\n",
        "    1) Reuse the same TF-IDF matrix constructed before\n",
        "    2) Reduce dimensionality with TruncatedSVD\n",
        "    3) Store doc embeddings in latent semantic space\n",
        "    \"\"\"\n",
        "    # clamping k so it doesn't exceed the TF-IDF feature dimension\n",
        "    max_k = max(1, min(k, min(doc_matrix.shape) - 1))\n",
        "    \n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    # 1. and 2. Reduce dimensionality with TruncatedSVD\n",
        "    # We use the dense doc_matrix from TF-IDF\n",
        "    svd = TruncatedSVD(\n",
        "        n_components=max_k,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # Fit SVD on the TF-IDF matrix and transform it to get doc embeddings\n",
        "    # doc_embeddings shape will be (num_docs, max_k)\n",
        "    doc_embeddings = svd.fit_transform(doc_matrix)\n",
        "    \n",
        "    # Note: LSA is often applied to the sparse matrix directly to save memory.\n",
        "    # Since the input doc_matrix is an np.ndarray (dense), we'll assume it's acceptable.\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return LSAModel(\n",
        "        vectorizer=vectorizer,\n",
        "        svd=svd,\n",
        "        doc_embeddings=doc_embeddings,\n",
        "        k=max_k\n",
        "    )\n",
        "\n",
        "\n",
        "def rank_with_lsa(query_text: str, lsa_model: LSAModel) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs by cosine similarity in the LSA semantic space.\n",
        "    (Query -> TF-IDF -> SVD; then cosine vs doc embeddings.)\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    # 1. Query -> TF-IDF\n",
        "    # Transform the query text using the fitted TfidfVectorizer\n",
        "    # Result is a sparse matrix of shape (1, vocab_size)\n",
        "    query_vector_sparse = lsa_model.vectorizer.transform([query_text])\n",
        "    \n",
        "    # 2. TF-IDF -> SVD (Project into LSA space)\n",
        "    # The query embedding shape will be (1, k)\n",
        "    query_embedding = lsa_model.svd.transform(query_vector_sparse)\n",
        "    \n",
        "    # 3. Calculate scores: Cosine similarity vs doc embeddings\n",
        "    scores = []\n",
        "    # Loop over each document embedding\n",
        "    for doc_embedding in lsa_model.doc_embeddings:\n",
        "        # Calculate cosine similarity. The cosine function is assumed to be provided.\n",
        "        # We ensure inputs are 1D arrays for the mock cosine function.\n",
        "        score = cosine(query_embedding.flatten(), doc_embedding)\n",
        "        scores.append(score)\n",
        "    \n",
        "    scores = np.array(scores)\n",
        "    \n",
        "    # 4. Get the indices that would sort the scores in descending order\n",
        "    order = np.argsort(scores)[::-1].tolist()\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return order\n",
        "```\n"
      ],
      "metadata": {
        "id": "C5ARfLOk-lLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparision to my Solution**\n",
        "\n",
        "1. For TF-IDF: Gemini converts the sparse matrices to dense arrays with .toarray(), which makes the code simpler but less memory-efficient. My version kept the sparse matrix, which is more scalable.\n",
        "\n",
        "**Reflection:**\n",
        "\n",
        "Gemini generated correct code for TF-IDF, BM25, and LSA, and handled vector shapes and ranking logic well. Its main issue was converting TF-IDF sparse matrices into dense NumPy arrays. This works for my tiny dataset but would break on large corpora because dense matrices consume far more memory. My version kept everything sparse, which is more efficient and closer to how real retrieval systems are built. Gemini’s solution was simple and correct, but it ignored scalability concerns, while mine remained more robust for larger settings."
      ],
      "metadata": {
        "id": "MQGiT4Yb-nzP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "eZdNySEk7lRF"
      },
      "source": [
        "# End of HW 02"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}