{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita2002/LLMS/blob/main/Intro2LLM_Homework_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "2Hovllpn7lRB"
      },
      "source": [
        "# HW 02: Retrieval with TF-IDF / BM25 / LSA, Word2Vec (CBOW), and MAP Evaluation\n",
        "\n",
        "### This homework has three tasks:\n",
        "1) **Ranked retrieval with TF-IDF, BM25, and LSA (Truncated SVD)** on a sampled subset of 20 Newsgroups — **6 pts**\n",
        "2) **Word2Vec (CBOW) with PyTorch** trained on the same sampled subset — **7 pts**\n",
        "3) **Mean Average Precision (MAP)** implementation + evaluate LSA vs CBOW on the test split — **7 pts**\n",
        "\n",
        "**BONUS:** Solve any task with an LLM — **2 pts**\n",
        "\n",
        "#### Total: 20 points (+2 bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILeyL6VG7lRB"
      },
      "source": [
        "## Pre-requisite code\n",
        "\n",
        "The code in this section will be used in the tasks.\n",
        "**Do not change these lines in your submission.**\n",
        "\n",
        "We:\n",
        "- load a **small, fixed subset** of the 20 Newsgroups dataset (4 topics),\n",
        "- split it into TRAIN_DOCS and TEST_DOCS,\n",
        "- build queries and relevance sets,\n",
        "- define helpers for tokenization, cosine similarity, etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these on Colab.\n",
        "!pip -q install rank_bm25 torch scikit-learn"
      ],
      "metadata": {
        "id": "FJs7DEekwAu8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random, collections, itertools\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import numpy as np\n",
        "\n",
        "# External libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Tokenization\n",
        "# -------------------------------------------------\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Lowercase, keep alphabetic tokens only, split on whitespace.\n",
        "    We'll also use this tokenizer inside scikit-learn's TfidfVectorizer.\n",
        "    \"\"\"\n",
        "    toks = []\n",
        "    for t in text.lower().strip().split():\n",
        "        # keep only alphabetic chars\n",
        "        t = \"\".join(ch for ch in t if ch.isalpha())\n",
        "        if t:\n",
        "            toks.append(t)\n",
        "    return toks\n",
        "\n",
        "@dataclass\n",
        "class Doc:\n",
        "    text: str\n",
        "    tokens: List[str]\n",
        "    label: str      # coarse topic label (used to derive relevance)\n",
        "\n",
        "def make_docs(lines: List[Tuple[str, str]]) -> List[Doc]:\n",
        "    \"\"\"\n",
        "    lines = [(text, label), ...]\n",
        "    We'll re-tokenize here, even though we already truncated earlier,\n",
        "    to keep the interface clean.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    for txt, lab in lines:\n",
        "        docs.append(Doc(text=txt, tokens=tokenize(txt), label=lab))\n",
        "    return docs\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Mini 20 Newsgroups subset\n",
        "# -------------------------------------------------\n",
        "# We'll sample 4 topical newsgroups as our \"topics\"\n",
        "CATEGORIES = [\n",
        "    \"comp.sys.mac.hardware\",  # Apple / Mac hardware issues\n",
        "    \"sci.space\",              # Space / NASA / missions\n",
        "    \"rec.autos\",              # Cars / engines / performance\n",
        "    \"rec.sport.hockey\",       # Hockey / teams / playoffs\n",
        "]\n",
        "\n",
        "# We'll map each full newsgroup name to a short label\n",
        "LABEL_MAP = {\n",
        "    \"comp.sys.mac.hardware\": \"mac\",\n",
        "    \"sci.space\": \"space\",\n",
        "    \"rec.autos\": \"autos\",\n",
        "    \"rec.sport.hockey\": \"hockey\",\n",
        "}\n",
        "\n",
        "# How many docs we keep per class for train/test,\n",
        "# and how aggressively we truncate each post.\n",
        "TRAIN_PER_CLASS = 100\n",
        "TEST_PER_CLASS  = 5\n",
        "MAX_TOKENS_PER_DOC = 50  # cap each doc to first N tokens for speed\n",
        "\n",
        "def collect_subset(dataset, per_class: int) -> List[Tuple[str,str]]:\n",
        "    \"\"\"\n",
        "    Build a list of (truncated_text, short_label) for up to per_class\n",
        "    examples per category in CATEGORIES.\n",
        "    We remove headers/footers/quotes via fetch_20newsgroups(... remove=...).\n",
        "    \"\"\"\n",
        "    by_class: Dict[str, List[Tuple[str,str]]] = {cat: [] for cat in CATEGORIES}\n",
        "\n",
        "    for text, tgt in zip(dataset.data, dataset.target):\n",
        "        cat = dataset.target_names[tgt]\n",
        "        if cat not in by_class:\n",
        "            continue\n",
        "        if len(by_class[cat]) >= per_class:\n",
        "            continue\n",
        "\n",
        "        toks = tokenize(text)\n",
        "        toks = toks[:MAX_TOKENS_PER_DOC]        # truncate\n",
        "        truncated_text = \" \".join(toks)         # rejoin to text form\n",
        "\n",
        "        by_class[cat].append((truncated_text, LABEL_MAP[cat]))\n",
        "\n",
        "    # flatten in fixed category order for determinism\n",
        "    lines = []\n",
        "    for cat in CATEGORIES:\n",
        "        lines.extend(by_class[cat])\n",
        "    return lines\n",
        "\n",
        "# Load raw 20NG subsets (without headers/footers/quotes to reduce noise)\n",
        "_20ng_train = fetch_20newsgroups(\n",
        "    subset=\"train\",\n",
        "    categories=CATEGORIES,\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "_20ng_test = fetch_20newsgroups(\n",
        "    subset=\"test\",\n",
        "    categories=CATEGORIES,\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# Build our small train/test corpora\n",
        "TRAIN_RAW = collect_subset(_20ng_train, per_class=TRAIN_PER_CLASS)\n",
        "TEST_RAW  = collect_subset(_20ng_test,  per_class=TEST_PER_CLASS)\n",
        "\n",
        "TRAIN_DOCS = make_docs(TRAIN_RAW)\n",
        "TEST_DOCS  = make_docs(TEST_RAW)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Queries & relevance\n",
        "# -------------------------------------------------\n",
        "# We craft one realistic search-style query per topic.\n",
        "# Relevant docs are those in TEST_DOCS whose label matches.\n",
        "QUERIES = [\n",
        "    (\n",
        "        \"how do i fix problems on my mac like disk failures, monitor issues and software failures\",\n",
        "        \"mac\"\n",
        "    ),\n",
        "    (\n",
        "        \"latest updates on nasa missions space shuttle launches and orbital science experiments\",\n",
        "        \"space\"\n",
        "    ),\n",
        "    (\n",
        "        \"advice on car performance engine reliability and buying a new vehicle\",\n",
        "        \"autos\"\n",
        "    ),\n",
        "    (\n",
        "        \"discussion about hockey teams playoffs goalies and physical play on the ice\",\n",
        "        \"hockey\"\n",
        "    )\n",
        "]\n",
        "\n",
        "def relevant_doc_indices(test_docs: List[Doc], label: str) -> List[int]:\n",
        "    \"\"\"Return indices in test_docs whose label matches the given topic label.\"\"\"\n",
        "    return [i for i, d in enumerate(test_docs) if d.label == label]\n",
        "\n",
        "print(f\"✓ Loaded {len(TRAIN_DOCS)} train docs and {len(TEST_DOCS)} test docs\")\n",
        "for lab in sorted({d.label for d in TRAIN_DOCS}):\n",
        "    print(\"  – label:\", lab,\n",
        "          \"| train:\", sum(1 for d in TRAIN_DOCS if d.label==lab),\n",
        "          \"| test:\",  sum(1 for d in TEST_DOCS  if d.label==lab))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Vector helpers\n",
        "# -------------------------------------------------\n",
        "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"Cosine similarity between vectors a and b. Returns 0 if either vector has zero norm.\"\"\"\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    if norm_a == 0 or norm_b == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(a, b) / (norm_a * norm_b))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Vocabulary utilities (for Task 2 CBOW)\n",
        "# -------------------------------------------------\n",
        "def build_vocab(docs: List[Doc], min_count: int = 1\n",
        "               ) -> Tuple[Dict[str,int], Dict[int,str], Dict[str,int]]:\n",
        "    \"\"\"\n",
        "    Build word-index mappings based on frequency in a list of Doc objects.\n",
        "\n",
        "    Returns:\n",
        "        w2i: word -> index\n",
        "        i2w: index -> word\n",
        "        freq: Counter of word frequencies\n",
        "    \"\"\"\n",
        "    freq = collections.Counter(t for d in docs for t in d.tokens)\n",
        "    vocab = [w for w, c in freq.items() if c >= min_count]\n",
        "    vocab = sorted(vocab)\n",
        "    w2i = {w:i for i, w in enumerate(vocab)}\n",
        "    i2w = {i:w for w, i in w2i.items()}\n",
        "    return w2i, i2w, freq\n",
        "\n",
        "print(\"✓ Pre-req code ready.\")"
      ],
      "metadata": {
        "id": "SCX-NfpVxjqr",
        "outputId": "e03c57ab-491f-4a12-8a8b-c60a4d58be79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded 400 train docs and 20 test docs\n",
            "  – label: autos | train: 100 | test: 5\n",
            "  – label: hockey | train: 100 | test: 5\n",
            "  – label: mac | train: 100 | test: 5\n",
            "  – label: space | train: 100 | test: 5\n",
            "✓ Pre-req code ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1 (6 pts): Ranked Retrieval with TF-IDF, BM25, and LSA\n",
        "\n",
        "**Goal.** We’ll build *three* retrieval models on a small subset of the\n",
        "20 Newsgroups dataset and compare how they rank documents:\n",
        "\n",
        "1. **TF-IDF ranked retrieval** - use scikit-learn’s `TfidfVectorizer`\n",
        "2. **BM25 (Okapi BM25)** - use the rank_bm25 library’s `BM25Okapi`\n",
        "3. **LSA (Latent Semantic Analysis via Truncated SVD)** - use scikit-learn’s `TruncatedSVD` on the same TF-IDF features\n",
        "\n",
        "We'll then run the queries in `QUERIES` and see how each method ranks\n",
        "the held-out TEST_DOCS. We’ll also:\n",
        "- check vector dimensionality (TF-IDF / BM25 vocab size vs. LSA k),\n",
        "- compute **Precision@5 (P@5)** for each method (per query and averaged).\n",
        "\n",
        "**Retrieval setup**\n",
        "- Candidate results are the **TEST_DOCS** we built above (5 per topic).\n",
        "- For each query `(query_text, label)`:\n",
        "  - Rank all TEST_DOCS using TF-IDF.\n",
        "  - Rank all TEST_DOCS using BM25.\n",
        "  - Rank all TEST_DOCS using LSA.\n",
        "  - Show the top-5 doc labels.\n",
        "  - Report P@5.\n",
        "\n",
        "**What you implement**\n",
        "\n",
        "**TF-IDF ranked retrieval (2 pts)**  \n",
        "- Fit a `TfidfVectorizer` on the retrieval corpus (TEST_DOCS).\n",
        "- Use our provided tokenizer;\n",
        "- Implement `rank_with_tfidf(query_text, vectorizer, doc_matrix)`:\n",
        "  - Transform the query with the same `TfidfVectorizer`.\n",
        "  - Score each document by aggregating TF-IDF weights over the query terms.\n",
        "  - Return doc IDs sorted by score (descending).\n",
        "\n",
        "**BM25 retrieval (2 pts)**  \n",
        "- Fit a `BM25Okapi` model on the tokenized docs.\n",
        "- Implement `rank_with_bm25(query_text, bm25_model)`:\n",
        "  - Tokenize the query with our tokenizer.\n",
        "  - Score all docs via BM25. Use `get_scores(query_tokens)` from `BM25Okapi` to get aggregated BM25 scores.\n",
        "  - Return doc IDs sorted by score (descending).\n",
        "\n",
        "**LSA retrieval (2 pts)**\n",
        "- Reuse the same TF-IDF vectorizer and its TF-IDF matrix used for TF-IDF ranking, then apply `TruncatedSVD` on that TF-IDF matrix to build a low-dimensional semantic space (LSA).\n",
        "- Implement `rank_with_lsa(query_text, lsa_model)`:\n",
        "  - Project query into that same semantic space (via the same TF-IDF vectorizer + SVD).\n",
        "  - Rank by cosine similarity in LSA space.\n",
        "\n",
        "**Grading (6 pts total)**\n",
        "- **1.1** TF-IDF retrieval implementation — **2 pts**  \n",
        "- **1.2** BM25 retrieval implementation — **2 pts**  \n",
        "- **1.3** LSA retrieval implementation — **2 pts**\n"
      ],
      "metadata": {
        "id": "OwE0TXSkFX89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------\n",
        "# TF-IDF ranked retrieval (scikit-learn)\n",
        "# -----------------\n",
        "def fit_tfidf_ranker(docs: List[Doc]) -> Tuple[TfidfVectorizer, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Fit a TfidfVectorizer on the given docs (retrieval corpus).\n",
        "    Returns:\n",
        "        vectorizer: fitted TfidfVectorizer\n",
        "        doc_matrix: TF-IDF matrix of shape (num_docs, vocab_size)\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def rank_with_tfidf(query_text: str,\n",
        "                    vectorizer: TfidfVectorizer,\n",
        "                    doc_matrix) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs using aggregated TF-IDF weights.\n",
        "    You can use the dot product between the query vector and TF-IDF matrix.\n",
        "    Returns:\n",
        "        order: list of doc indices sorted by score desc\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# BM25 ranked retrieval (rank_bm25)\n",
        "# -----------------\n",
        "def fit_bm25_ranker(docs: List[Doc]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Fit BM25 on the given docs (list of token lists).\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def rank_with_bm25(query_text: str,\n",
        "                   bm25_model: BM25Okapi) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs by aggregated BM25 score.\n",
        "    Returns:\n",
        "        order: doc indices (0..num_docs-1) sorted by score desc\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# LSA ranked retrieval (reuse TF-IDF + TruncatedSVD)\n",
        "# -----------------\n",
        "@dataclass\n",
        "class LSAModel:\n",
        "    \"\"\"\n",
        "    LSAModel stores:\n",
        "    - vectorizer: the same fitted TfidfVectorizer used for TF-IDF ranking\n",
        "    - svd: fitted TruncatedSVD\n",
        "    - doc_embeddings: dense array of shape (num_docs, k)\n",
        "    - k: number of latent dimensions used\n",
        "    \"\"\"\n",
        "    vectorizer: TfidfVectorizer\n",
        "    svd: TruncatedSVD\n",
        "    doc_embeddings: np.ndarray\n",
        "    k: int\n",
        "\n",
        "\n",
        "def fit_lsa_ranker(vectorizer: TfidfVectorizer,\n",
        "                   doc_matrix,  # TF-IDF matrix constructed before\n",
        "                   k: int=10,\n",
        "                   random_state: int=RANDOM_SEED) -> LSAModel:\n",
        "    \"\"\"\n",
        "    Steps:\n",
        "    1) Reuse the same TF-IDF matrix constructed before\n",
        "    2) Reduce dimensionality with TruncatedSVD\n",
        "    3) Store doc embeddings in latent semantic space\n",
        "    \"\"\"\n",
        "    # clamping k so it doesn't exceed the TF-IDF feature dimension\n",
        "    max_k = max(1, min(k, min(doc_matrix.shape) - 1))\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return LSAModel(\n",
        "        vectorizer=vectorizer,\n",
        "        svd=svd,\n",
        "        doc_embeddings=doc_embeddings,\n",
        "        k=max_k\n",
        "    )\n",
        "\n",
        "\n",
        "def rank_with_lsa(query_text: str, lsa_model: LSAModel) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank docs by cosine similarity in the LSA semantic space.\n",
        "    (Query -> TF-IDF -> SVD; then cosine vs doc embeddings.)\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Build all three rankers on the same retrieval corpus (TEST_DOCS)\n",
        "# -------------------------------------------------\n",
        "tfidf_vec, tfidf_matrix = fit_tfidf_ranker(TEST_DOCS)\n",
        "bm25_model             = fit_bm25_ranker(TEST_DOCS)\n",
        "lsa_model              = fit_lsa_ranker(tfidf_vec, tfidf_matrix, k=10, random_state=RANDOM_SEED)\n",
        "\n",
        "# Vector size comparison:\n",
        "tfidf_vocab_size = len(tfidf_vec.vocabulary_)   # dimensionality of TF-IDF term space\n",
        "bm25_vocab_size  = len(bm25_model.idf)          # dimensionality of BM25 term space\n",
        "lsa_k            = lsa_model.k                  # dimensionality of LSA latent semantic space\n",
        "\n",
        "print(\"\\n[TASK1] Vector Size Comparison:\")\n",
        "print(f\"  TF-IDF dim   = {tfidf_vocab_size}\")\n",
        "print(f\"  BM25 dim     = {bm25_vocab_size}\")\n",
        "print(f\"  LSA dim (k)  = {lsa_k}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Evaluation: Precision@5 (per query and averaged)\n",
        "# -------------------------------------------------\n",
        "def precision_at_k(ranked: List[int], relevant: set, k: int = 5) -> float:\n",
        "    hits = sum(i in relevant for i in ranked[:k])\n",
        "    return hits / float(k)\n",
        "\n",
        "TOPK = 5\n",
        "tfidf_p5_scores = []\n",
        "bm25_p5_scores  = []\n",
        "lsa_p5_scores   = []\n",
        "\n",
        "for qtext, qlab in QUERIES:\n",
        "    rel = set(relevant_doc_indices(TEST_DOCS, qlab))\n",
        "\n",
        "    tfidf_order = rank_with_tfidf(qtext, tfidf_vec, tfidf_matrix)\n",
        "    bm25_order  = rank_with_bm25(qtext, bm25_model)\n",
        "    lsa_order   = rank_with_lsa(qtext, lsa_model)\n",
        "\n",
        "    p5_tfidf = precision_at_k(tfidf_order, rel, TOPK)\n",
        "    p5_bm25  = precision_at_k(bm25_order,  rel, TOPK)\n",
        "    p5_lsa   = precision_at_k(lsa_order,   rel, TOPK)\n",
        "\n",
        "    tfidf_p5_scores.append(p5_tfidf)\n",
        "    bm25_p5_scores.append(p5_bm25)\n",
        "    lsa_p5_scores.append(p5_lsa)\n",
        "\n",
        "    print(f\"\\n[TASK1] Query='{qtext}' | (label={qlab})\")\n",
        "    print(\"   TF-IDF top-5 labels:\", [TEST_DOCS[i].label for i in tfidf_order[:TOPK]],\n",
        "          f\"| P@5={p5_tfidf:.3f}\")\n",
        "    print(\"   BM25  top-5 labels:\", [TEST_DOCS[i].label for i in bm25_order[:TOPK]],\n",
        "          f\"| P@5={p5_bm25:.3f}\")\n",
        "    print(\"   LSA   top-5 labels:\", [TEST_DOCS[i].label for i in lsa_order[:TOPK]],\n",
        "          f\"| P@5={p5_lsa:.3f}\")\n",
        "\n",
        "print(\"\\n[TASK1 RESULT_CHECKING_POINT] Precision@5 (average across queries):\")\n",
        "print(f\"  TF-IDF: ({np.mean(tfidf_p5_scores):.3f})\")\n",
        "print(f\"  BM25:   ({np.mean(bm25_p5_scores):.3f})\")\n",
        "print(f\"  LSA:    ({np.mean(lsa_p5_scores):.3f})\")\n"
      ],
      "metadata": {
        "id": "8CAjArbU7xAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TESTS & EXPECTED OUTPUT -------------------------\n",
        "# [TASK1] Vector Size Comparison:\n",
        "#   TF-IDF dim   = 441\n",
        "#   BM25 dim     = 441\n",
        "#   LSA dim (k)  = 10\n",
        "#\n",
        "# [TASK1] Query='how do i fix problems on my mac ...' | (label=mac)\n",
        "#    TF-IDF top-5 document labels: ['space', 'autos', ...] | P@5=0.400\n",
        "#    BM25  top-5 document labels: ['space', 'mac', ...] | P@5=0.400\n",
        "#    LSA   top-5 document labels: ['space', 'mac', ...] | P@5=0.400\n",
        "#\n",
        "# ...\n",
        "#\n",
        "# [TASK1] Query='discussion about hockey teams ...' | (label=hockey)\n",
        "#    TF-IDF top-5 document labels: ['hockey', 'space', ...] | P@5=0.400\n",
        "#    BM25  top-5 document labels: ['hockey', 'space', ...] | P@5=0.400\n",
        "#    LSA   top-5 document labels: ['hockey', 'space', ...] | P@5=0.400\n",
        "#\n",
        "# [TASK1 RESULT_CHECKING_POINT] Precision@5 (average across queries):\n",
        "#   TF-IDF: (0.400)\n",
        "#   BM25:   (0.450)\n",
        "#   LSA:    (0.400)\n",
        "# ---------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "HCAb9ksI0zMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 2 (7 pts): Word2Vec (CBOW) with PyTorch\n",
        "\n",
        "## Goal\n",
        "Train a **CBOW** model on the **train subset of 20 Newsgroups** (**exactly 100 docs per topic, truncated**) using **PyTorch** and NumPy utilities.\n",
        "\n",
        "**Do not** use external libraries that already implement CBOW (e.g., *gensim*).  \n",
        "\n",
        "- **Input:** mean of embeddings of the `2W` context tokens around a center word  \n",
        "- **Output:** logits over the whole vocabulary predicting the **center** word  \n",
        "- **Loss:** cross-entropy (`nn.CrossEntropyLoss`)  \n",
        "- **Optimization:** **Adam optimizer** with **mini-batches** (`DataLoader` is used to easily prepare and load mini-batches)\n",
        "\n",
        "We use a **fixed window** of size `W` on each side; contexts are **exactly `2W` tokens**.  \n",
        "Positions that don’t have enough neighbors (near boundaries) are **skipped** — no padding/masking needed.\n",
        "\n",
        "We also use a **naïve softmax** output (complexity **$O(|V|)$** per update) — this is **okay** for our small setup.\n",
        "\n",
        "### Parameter shapes\n",
        "Let $|V|$ be the vocabulary size and $d$ the embedding dimension:\n",
        "- $ W_{\\text{in}} \\in \\mathbb{R}^{|V|\\times d} $\n",
        "- $ W_{\\text{out}} \\in \\mathbb{R}^{|V|\\times d} $\n",
        "- $ b_{\\text{out}} \\in \\mathbb{R}^{|V|} $\n",
        "\n",
        "## What you implement\n",
        "1. **Vocab & data**\n",
        "   - Build `(word → index)` / `(index → word)` from the **train** texts (already implemented for you).\n",
        "   - Create CBOW pairs `(context_indices, target_index)` using a symmetric window of size `W`.\n",
        "   - **Skip** positions that don’t yield a full `2W`-token context (typically happens at edge positions or short sentences).\n",
        "\n",
        "2. **Model (PyTorch)**\n",
        "   - `nn.Embedding(|V|, d)` → **mean over the `2W` context embeddings** → `nn.Linear(d, |V|)` → logits.\n",
        "   - Use `nn.CrossEntropyLoss()` (softmax is automatically calculated in this utility).\n",
        "\n",
        "3. **Training (mini-batches + Adam)**\n",
        "   - Wrap pairs in a `Dataset`/`DataLoader` (`batch_size = B`).\n",
        "   - Train for `E` epochs with `torch.optim.Adam(model.parameters(), lr=lr)`.\n",
        "   - Print average training loss per epoch.\n",
        "\n",
        "4. **Embedding helper**\n",
        "   - `embed_text_mean(tokens, cbow_model)`: average learned **input** embeddings to represent any text (it will be used in Task 3).\n",
        "\n",
        "## Grading (7 pts)\n",
        "- **2.1** Data maker (windowing, indexing) — **2 pts**\n",
        "- **2.2** Training loop (mini-batches + Adam) — **3 pts**\n",
        "- **2.3** Embedding helper (mean of tokens embeddings) — **2 pts**  \n",
        "\n",
        "\n",
        "> **Notes**\n",
        "> - We will compare CBOW vs LSA retrieval in **Task 3**.\n"
      ],
      "metadata": {
        "id": "NG7gjRvSFuYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -----------------------------\n",
        "# Reproducibility\n",
        "# -----------------------------\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Vocab utilities for CBOW, build ing (word -> index) and (index -> word)\n",
        "# -----------------------------\n",
        "def build_w2v_vocab(docs: List[Doc], min_count: int = 1\n",
        "                   ) -> Tuple[Dict[str,int], Dict[int,str]]:\n",
        "    \"\"\"\n",
        "    Build a vocabulary from TRAIN_DOCS.\n",
        "    Returns:\n",
        "        w2i: word -> index  (0..|V|-1)\n",
        "        i2w: index -> word\n",
        "    \"\"\"\n",
        "    w2i, i2w, freq = build_vocab(docs, min_count=min_count)\n",
        "    return w2i, i2w\n",
        "\n",
        "def cbow_training_data_fixed_window(docs: List[Doc],\n",
        "                                    w2i: Dict[str,int],\n",
        "                                    window: int = 2\n",
        "                                   ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Create CBOW pairs with a fixed-size symmetric window of size `window`.\n",
        "    For each token position c, we require exactly `2*window` context tokens.\n",
        "    Positions that don't have enough neighbors are skipped.\n",
        "\n",
        "    Returns:\n",
        "        contexts: np.ndarray of shape (N, 2*window) with word indices\n",
        "        targets : np.ndarray of shape (N,) with target word indices\n",
        "    \"\"\"\n",
        "    contexts, targets = [], []\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return np.array(contexts, dtype=np.int64), np.array(targets, dtype=np.int64)\n",
        "\n",
        "# -----------------------------\n",
        "# PyTorch Dataset Format\n",
        "# -----------------------------\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, contexts: np.ndarray, targets: np.ndarray):\n",
        "        self.contexts = contexts\n",
        "        self.targets  = targets\n",
        "    def __len__(self) -> int:\n",
        "        return self.targets.shape[0]\n",
        "    def __getitem__(self, idx: int):\n",
        "        x = torch.tensor(self.contexts[idx], dtype=torch.long)\n",
        "        y = torch.tensor(self.targets[idx],  dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# -----------------------------\n",
        "# CBOW Model (Embedding mean -> Linear)\n",
        "# -----------------------------\n",
        "class CBOWTorch(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.in_embed = nn.Embedding(vocab_size, dim) # (|V|, d)\n",
        "        self.out = nn.Linear(dim, vocab_size)         # (d, |V|)\n",
        "        # The following lines are optional but common in word embedding models:\n",
        "        # Small std (0.01) keeps initial logits near zero so softmax isn't saturated.\n",
        "        # This stabilizes early training (gradients stay useful and not too peaky).\n",
        "        nn.init.normal_(self.in_embed.weight, mean=0.0, std=0.01)\n",
        "        nn.init.normal_(self.out.weight,      mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.out.bias)\n",
        "\n",
        "    def forward(self, ctx_idxs: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        ctx_idxs: (B, 2W) tensor of word indices\n",
        "        Returns:\n",
        "            logits: (B, |V|)\n",
        "        \"\"\"\n",
        "        emb = self.in_embed(ctx_idxs)        # (B, 2W, d)\n",
        "        h   = emb.mean(dim=1)                # (B, d)   mean over context tokens\n",
        "        logits = self.out(h)                 # (B, |V|)\n",
        "        return logits\n",
        "\n",
        "@dataclass\n",
        "class CBOWModelTorch:\n",
        "    \"\"\"Light wrapper to carry the trained model and basic metadata.\"\"\"\n",
        "    w2i: Dict[str,int]\n",
        "    i2w: Dict[int,str]\n",
        "    model: nn.Module\n",
        "    dim: int\n",
        "    window: int\n",
        "\n",
        "# -----------------------------\n",
        "# CBOW Model Training Function\n",
        "# -----------------------------\n",
        "def cbow_train_torch(train_docs: List[Doc],\n",
        "                     dim: int        = 100,\n",
        "                     window: int     = 2,\n",
        "                     batch_size: int = 256,\n",
        "                     epochs: int     = 10,\n",
        "                     lr: float       = 1e-3,\n",
        "                     seed: int       = RANDOM_SEED\n",
        "                    ) -> CBOWModelTorch:\n",
        "    \"\"\"\n",
        "    Train CBOW with fixed-size windows using Adam and mini-batches.\n",
        "    \"\"\"\n",
        "    # Build vocab and pairs\n",
        "    w2i, i2w = build_w2v_vocab(train_docs)\n",
        "    contexts, targets = cbow_training_data_fixed_window(train_docs, w2i, window=window)\n",
        "    print(f\"[TASK2] CBOW data: {len(targets)} pairs | V={len(w2i)} | d={dim} | window={window}\")\n",
        "\n",
        "    ds = CBOWDataset(contexts, targets)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return CBOWModelTorch(w2i=w2i, i2w=i2w, model=model, dim=dim, window=window)\n",
        "\n",
        "# -----------------------------\n",
        "# Embedding helper for downstream tasks\n",
        "# -----------------------------\n",
        "def embed_text_mean(tokens: List[str], cbow: CBOWModelTorch) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Average input embeddings for provided tokens.\n",
        "    Returns a numpy vector of shape (d,).\n",
        "    \"\"\"\n",
        "    vec = np.zeros(cbow.dim, dtype=float)\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "    return vec\n",
        "\n",
        "# -----------------------------\n",
        "# Train on TRAIN_DOCS (example settings)\n",
        "# -----------------------------\n",
        "cbow = cbow_train_torch(\n",
        "    TRAIN_DOCS,\n",
        "    dim=100,          # embedding dim\n",
        "    window=2,         # context window on each side\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        "    lr=1e-3,\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# Quick sanity: embed the first train doc\n",
        "ex_vec = embed_text_mean(TRAIN_DOCS[0].tokens, cbow)\n",
        "print(\"[TASK2 RESULT_CHECKING_POINT] Embedding shape:\", ex_vec.shape)\n"
      ],
      "metadata": {
        "id": "YoujzX7LBIFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TESTS & EXPECTED OUTPUT -------------------------\n",
        "# [TASK2] CBOW data: 15152 pairs | V=4015 | d=100 | window=2\n",
        "#   epoch 04/10 | avg_loss6.8..\n",
        "#   ...\n",
        "#   epoch 10/10 | avg_loss=6.5..\n",
        "# [TASK2 RESULT_CHECKING_POINT] Embedding shape: (100,)\n",
        "# ---------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "BwW7L9jZH5mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 3 (7 pts): MAP implementation — compare & evaluate LSA vs CBOW on the test set\n",
        "\n",
        "**Reminder:** You must finish Task 1 (to get `lsa_model`) and Task 2 (to get `cbow`) before starting Task 3. We will use those two models here.\n",
        "\n",
        "## Goal\n",
        "1. Implement **Average Precision (AP)** and **Mean Average Precision (MAP)**.\n",
        "2. Use them to evaluate and compare:\n",
        "   - **LSA retrieval** (from Task 1, using TruncatedSVD on TF-IDF)\n",
        "   - **CBOW retrieval** (from Task 2, using learned word embeddings)\n",
        "3. Report how well each model retrieves relevant documents for each query.\n",
        "4. Compare their **embedding dimensionality**:\n",
        "   - LSA works in a low-rank semantic space of size `k`.\n",
        "   - CBOW uses learned embeddings of size `d`.\n",
        "\n",
        "## Retrieval setup\n",
        "- We have `TEST_DOCS` and `QUERIES`.\n",
        "- For each query `(query_text, label)`:\n",
        "  - A test doc is **relevant** if `doc.label == label`.\n",
        "  - We produce a ranked list of all test docs.\n",
        "\n",
        "**LSA ranking (Task 1 model):**\n",
        "1. Convert the query to TF-IDF with `lsa_model.vectorizer`.\n",
        "2. Project it into the LSA space with `lsa_model.svd`.\n",
        "3. Compute cosine similarity with each document embedding in `lsa_model.doc_embeddings`.\n",
        "4. Sort by similarity (highest first).\n",
        "\n",
        "**CBOW ranking (Task 2 model):**\n",
        "1. Embed the query by averaging the CBOW input embeddings (`W_in`) of its tokens.\n",
        "2. Embed each test doc the same way.\n",
        "3. Rank by cosine similarity.\n",
        "\n",
        "## Evaluation metrics\n",
        "Let the ranked documents for a query be $ d_1, d_2, \\dots, d_n $, and let $ R $ be the set of relevant doc indices.\n",
        "\n",
        "**Precision@k** is:\n",
        "$$\n",
        "\\mathrm{P@}k \\;=\\; \\frac{TP@k}{TP@k + FP@k}.\n",
        "$$\n",
        "\n",
        "Define the **relevance indicator**\n",
        "$$\n",
        "\\mathrm{rel}_k \\;=\\;\n",
        "\\begin{cases}\n",
        "1 & \\text{if } d_k \\in R,\\\\\n",
        "0 & \\text{otherwise.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Average Precision (AP)** for one query is:\n",
        "$$\n",
        "\\mathrm{AP}\n",
        "\\;=\\;\n",
        "\\frac{1}{|R|}\n",
        "\\sum_{k=1}^{n}\n",
        "\\mathrm{P@}k \\cdot \\mathrm{rel}_k.\n",
        "$$\n",
        "\n",
        "**Mean Average Precision (MAP)** over all queries is:\n",
        "$$\n",
        "\\mathrm{MAP} = \\frac{1}{Q} \\sum_{q=1}^{Q} \\mathrm{AP}_q\n",
        "$$\n",
        "\n",
        "We will:\n",
        "- compute AP for each query,\n",
        "- average to get MAP for **LSA** and **CBOW**\n",
        "\n",
        "### Grading (7 pts)\n",
        "- **3.1** `average_precision` and `mean_average_precision` — **3 pts**  \n",
        "- **3.2** LSA ranking with MAP — **2 pts**  \n",
        "- **3.3** CBOW ranking with MAP — **2 pts**\n"
      ],
      "metadata": {
        "id": "57BuaAlg8iLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# AP and MAP implementation\n",
        "# -------------------------------------------------\n",
        "def average_precision(ranking: List[int], relevant: set) -> float:\n",
        "    \"\"\"\n",
        "    Compute Average Precision (AP) for a single query.\n",
        "    ranking: list of doc indices in ranked order (best first).\n",
        "    relevant: set of doc indices considered relevant for this query.\n",
        "\n",
        "    AP = (1/|R|) * sum_k Precision@k * rel_k\n",
        "    \"\"\"\n",
        "    if not ranking or not relevant:\n",
        "        return 0.0\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def mean_average_precision(all_rankings: List[List[int]],\n",
        "                           all_relevant: List[set]) -> float:\n",
        "    \"\"\"\n",
        "    Compute MAP across multiple queries.\n",
        "    all_rankings[i] is the ranked doc indices for query i\n",
        "    all_relevant[i] is the set of relevant doc indices for query i\n",
        "\n",
        "    MAP = mean_i AP_i\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Retrieval with LSA and CBOW for MAP evaluation\n",
        "# -------------------------------------------------\n",
        "\n",
        "# CBOW embeddings for all TEST_DOCS\n",
        "CBOW_TEST_EMBEDS = [embed_text_mean(d.tokens, cbow) for d in TEST_DOCS]  # (num_docs, d)\n",
        "\n",
        "# LSA embeddings for all TEST_DOCS were already computed in lsa_model.doc_embeddings\n",
        "LSA_TEST_EMBEDS = lsa_model.doc_embeddings  # shape: (num_docs, lsa_model.k)\n",
        "\n",
        "\n",
        "def rank_with_lsa_map(query_text: str,\n",
        "                      lsa_model: LSAModel,\n",
        "                      test_embeds: np.ndarray) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank TEST_DOCS for a query using the LSA semantic space.\n",
        "    1) TF-IDF the query with lsa_model.vectorizer\n",
        "    2) Project to LSA with lsa_model.svd\n",
        "    3) Cosine sim vs each doc embedding (test_embeds)\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def rank_with_cbow_map(query_text: str,\n",
        "                       cbow_model: CBOWModelTorch,\n",
        "                       test_embeds: List[np.ndarray]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rank TEST_DOCS for a query using CBOW embeddings.\n",
        "    1) Average word vectors for the query text\n",
        "    2) Cosine similarity vs each precomputed doc embedding\n",
        "    3) Sort high -> low\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Dimensionality comparison\n",
        "# -------------------------------------------------\n",
        "lsa_dim  = lsa_model.k      # LSA latent dimension k\n",
        "cbow_dim = cbow.dim         # CBOW embedding dimension d\n",
        "\n",
        "print(\"[TASK3] Embedding dimensionality:\")\n",
        "print(f\"  LSA  dim (k) = {lsa_dim}\")\n",
        "print(f\"  CBOW dim (d) = {cbow_dim}\")\n",
        "print()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# MAP evaluation + reporting\n",
        "# -------------------------------------------------\n",
        "lsa_rankings  = []\n",
        "cbow_rankings = []\n",
        "rel_sets      = []\n",
        "\n",
        "TOPK = 5\n",
        "\n",
        "for qtext, label in QUERIES:\n",
        "    rel = set(relevant_doc_indices(TEST_DOCS, label))\n",
        "    rel_sets.append(rel)\n",
        "\n",
        "    # get rankings\n",
        "    lsa_order  = rank_with_lsa_map(qtext, lsa_model, LSA_TEST_EMBEDS)\n",
        "    cbow_order = rank_with_cbow_map(qtext, cbow, CBOW_TEST_EMBEDS)\n",
        "\n",
        "    lsa_rankings.append(lsa_order)\n",
        "    cbow_rankings.append(cbow_order)\n",
        "\n",
        "    # inspect per-query behavior\n",
        "    print(f\"[TASK3] Query='{qtext}' | (label={label})\")\n",
        "\n",
        "    print(\"   LSA  top-5 document labels:\", [TEST_DOCS[i].label for i in lsa_order[:TOPK]])\n",
        "\n",
        "    print(\"   CBOW top-5 document labels:\", [TEST_DOCS[i].label for i in cbow_order[:TOPK]])\n",
        "    print()\n",
        "\n",
        "# Compute MAP\n",
        "map_lsa  = mean_average_precision(lsa_rankings,  rel_sets)\n",
        "map_cbow = mean_average_precision(cbow_rankings, rel_sets)\n",
        "\n",
        "print(f\"[TASK3 RESULT_CHECKING_POINT] MAP_LSA={map_lsa:.3f} | MAP_CBOW={map_cbow:.3f}\")"
      ],
      "metadata": {
        "id": "2e4gNNjsIu71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TESTS & EXPECTED OUTPUT -------------------------\n",
        "# [TASK3] Embedding dimensionality:\n",
        "#   LSA  dim (k) = 10\n",
        "#   CBOW dim (d) = 100\n",
        "#\n",
        "# [TASK3] Query='how do i fix problems on my mac ...' | (label=mac)\n",
        "#    LSA top-5 document labels: ['autos', 'mac', ...]\n",
        "#    CBOW top-5 document labels: ['mac', 'space', ...]\n",
        "#\n",
        "# ...\n",
        "#\n",
        "# [TASK3] Query='discussion about hockey teams ...'  | (label=hockey)\n",
        "#    LSA top-5 document labels: ['hockey', 'space', ...]\n",
        "#    CBOW top-5 document labels: ['hockey', 'autos', ...]\n",
        "#\n",
        "# [TASK3 RESULT_CHECKING_POINT] MAP_LSA=0.506 | MAP_CBOW=0.411\n",
        "# ---------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "gPv3N2UiJO3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS (2 pts): Solve any task with an LLM\n",
        "\n",
        "**Goal.**  \n",
        "Pick **one** of the homework tasks (Task 1, Task 2, or Task 3) and solve it using an **LLM**. Provide the **LLM** with the **task description** and any **starter/prerequisite code** it depends on, ask it to generate a complete **code solution** first, then run that **generated code** here in the code notebook yourself. Finally, document what you did and **compare** the LLM’s result to your own pipeline.\n",
        "\n",
        "**What to deliver below.**\n",
        "1) **LLM used** (name + version, e.g., “Llama-3-8B-Instruct”, “GPT-x”, “Claude-x”, “Mistral-x”, etc.).  \n",
        "2) **Prompt(s)** you used.  \n",
        "3) **LLM output** — copy and paste the generated code.  \n",
        "4) **Comparison** to your solution: what matches or differs (quantitative or qualitative).  \n",
        "5) **Reflection**: what the LLM was **good at** vs **bad at**, what it got **right** vs **wrong**.\n",
        "\n",
        "> **No code required.** You do **not** need to run, share, or submit any code used for the LLM generation. Provide only the deliverables listed above.\n",
        "> You may use any LLMs through any interface (API, web UI, local inference).\n"
      ],
      "metadata": {
        "id": "t6YRB0GARyHY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "eZdNySEk7lRF"
      },
      "source": [
        "# End of HW 02"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}