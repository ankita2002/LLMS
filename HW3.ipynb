{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita2002/LLMS/blob/main/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V-5Ms5899a9H",
      "metadata": {
        "id": "V-5Ms5899a9H"
      },
      "source": [
        "# Homework 3: Transformers, T5, Machine Translation\n",
        "\n",
        "This homework guides you through implementing a complete T5 (Text-To-Text Transfer Transformer) encoder-decoder architecture from scratch for machine translation. You will build the encoder stack, decoder stack with cross-attention, and train the model on English-German translation.\n",
        "\n",
        "**Total Points: 20**\n",
        "\n",
        "**Instructions:**\n",
        "1. Complete all tasks in this notebook\n",
        "2. Ensure your code runs without errors\n",
        "3. Submit both this notebook and any additional files created\n",
        "4. Write clear explanations for your approach\n",
        "5. Train your model on the English-German translation dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_5jqzZYb9a9J",
      "metadata": {
        "id": "_5jqzZYb9a9J"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "za4ixerZ9a9K",
      "metadata": {
        "id": "za4ixerZ9a9K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y4X0a4sr9a9L",
      "metadata": {
        "id": "Y4X0a4sr9a9L"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "We'll use the WMT English-German translation dataset. This dataset contains parallel sentences in English and German, perfect for training a sequence-to-sequence translation model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "MI8Fg-6Yy8ZZ"
      },
      "id": "MI8Fg-6Yy8ZZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sS13-kii9a9N",
      "metadata": {
        "id": "sS13-kii9a9N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "import tqdm\n",
        "\n",
        "print(\"Initializing WMT Dataset Loader (No backups, real data)...\")\n",
        "\n",
        "# 1. Load the Real WMT Dataset\n",
        "# We use WMT16 (German-English), a standard benchmark.\n",
        "# We take the first 50,000 examples to keep RAM usage efficient while ensuring variety.\n",
        "# If you want the full dataset (4.5M pairs), remove the [:50000] slice.\n",
        "dataset = load_dataset(\"wmt16\", \"de-en\", split=\"train[:50000]\")\n",
        "\n",
        "print(f\"Successfully loaded {len(dataset)} pairs from WMT16.\")\n",
        "\n",
        "# 2. Extract pairs into memory\n",
        "translation_pairs = []\n",
        "for item in dataset:\n",
        "    # WMT data structure is usually {'translation': {'de': '...', 'en': '...'}}\n",
        "    en_text = item['translation']['en']\n",
        "    de_text = item['translation']['de']\n",
        "    translation_pairs.append((en_text, de_text))\n",
        "\n",
        "# 3. Build Vocabulary (Character-level)\n",
        "# We stick to character-level to match your previous logic.\n",
        "# Note: For SOTA results on WMT, you would usually use BPE (Byte Pair Encoding),\n",
        "# but character-level is robust for learning deep learning fundamentals.\n",
        "print(\"Building vocabulary from dataset...\")\n",
        "\n",
        "counter = Counter()\n",
        "for en, de in translation_pairs:\n",
        "    counter.update(en) # Keep case sensitivity? standard is usually to keep it for WMT\n",
        "    counter.update(de)\n",
        "\n",
        "# Filter rare characters to keep vocab clean (optional, but good for real web data)\n",
        "MIN_FREQ = 5\n",
        "common_chars = [char for char, count in counter.items() if count >= MIN_FREQ]\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "chars = special_tokens + sorted(common_chars)\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f'Shared vocabulary size: {vocab_size}')\n",
        "\n",
        "# 4. Mappings\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "PAD_IDX = stoi['<pad>']\n",
        "SOS_IDX = stoi['<sos>']\n",
        "EOS_IDX = stoi['<eos>']\n",
        "UNK_IDX = stoi['<unk>']\n",
        "\n",
        "# 5. Encoding/Decoding Functions\n",
        "def encode(s):\n",
        "    \"\"\"Encode string to token IDs (Character level).\"\"\"\n",
        "    # We do NOT lower() here to preserve information, unlike the toy example\n",
        "    return [stoi.get(c, UNK_IDX) for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    \"\"\"Decode token IDs to string.\"\"\"\n",
        "    return ''.join([itos[i] for i in l if i != PAD_IDX])\n",
        "\n",
        "print(f\"Special tokens: PAD={PAD_IDX}, SOS={SOS_IDX}, EOS={EOS_IDX}, UNK={UNK_IDX}\")\n",
        "\n",
        "# 6. Train/Val Split (80/20)\n",
        "n_train = int(0.8 * len(translation_pairs))\n",
        "train_pairs = translation_pairs[:n_train]\n",
        "val_pairs = translation_pairs[n_train:]\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "print(f\"Val pairs:   {len(val_pairs)}\")\n",
        "print(\"-\" * 40)\n",
        "print(\"Sample Data (Real WMT):\")\n",
        "print(f\"  EN: {train_pairs[5][0]}\") # Random index to show real sentences\n",
        "print(f\"  DE: {train_pairs[5][1]}\")\n",
        "print(f\"  Encoded: {encode(train_pairs[5][0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZfPZQJP_9a9O",
      "metadata": {
        "id": "ZfPZQJP_9a9O"
      },
      "source": [
        "## TASK 1: T5 Encoder Implementation (6 points)\n",
        "\n",
        "Build the encoder stack that processes source sequences with bidirectional attention. The encoder is similar to BERT but designed for the T5 architecture.\n",
        "\n",
        "**1.1**: *Encoder Block* — Implement T5 encoder block with bidirectional self-attention, feed-forward network, layer normalization, and residual connections\n",
        "\n",
        "**1.2**: *Encoder Stack* — Stack multiple encoder blocks with positional encoding and token embeddings\n",
        "\n",
        "**1.3**: *Encoder Testing* — Test encoder implementation and verify bidirectional attention works correctly\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function: Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:x.size(1), :].unsqueeze(0)"
      ],
      "metadata": {
        "id": "9WrahFO_zWIn"
      },
      "id": "9WrahFO_zWIn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, hidden_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)"
      ],
      "metadata": {
        "id": "MQOwi67NzVO5"
      },
      "id": "MQOwi67NzVO5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3G4xE39a9O",
      "metadata": {
        "id": "2e3G4xE39a9O"
      },
      "outputs": [],
      "source": [
        "# Helper function: Scaled Dot-Product Attention\n",
        "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        query: [batch_size, seq_len, d_k] or [batch_size, n_heads, seq_len, d_k]\n",
        "        key: [batch_size, seq_len, d_k] or [batch_size, n_heads, seq_len, d_k]\n",
        "        value: [batch_size, seq_len, d_v] or [batch_size, n_heads, seq_len, d_v]\n",
        "        mask: [batch_size, seq_len, seq_len] or None (0s for masked positions)\n",
        "        dropout: Dropout layer or None\n",
        "    Returns:\n",
        "        output: [batch_size, seq_len, d_v] or [batch_size, n_heads, seq_len, d_v]\n",
        "        attention_weights: [batch_size, seq_len, seq_len] or [batch_size, n_heads, seq_len, seq_len]\n",
        "    \"\"\"\n",
        "    d_k = query.size(-1)\n",
        "\n",
        "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    # Apply mask if provided (set masked positions to -inf before softmax)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # Apply dropout if provided\n",
        "    if dropout is not None:\n",
        "        attention_weights = dropout(attention_weights)\n",
        "\n",
        "    # Multiply attention weights by values\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = n_embd // n_head\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.query = nn.Linear(n_embd, n_embd)\n",
        "        self.key = nn.Linear(n_embd, n_embd)\n",
        "        self.value = nn.Linear(n_embd, n_embd)\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        B, T, C = q.shape\n",
        "        q = self.query(q).view(B, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = self.key(k).view(B, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = self.value(v).view(B, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        scores = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
        "        out = (attn @ v).transpose(1, 2).contiguous().view(B, -1, C)\n",
        "        return self.proj(out), attn"
      ],
      "metadata": {
        "id": "QGOuf4XfzT2E"
      },
      "id": "QGOuf4XfzT2E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "qLeTriMq9a9P",
      "metadata": {
        "id": "qLeTriMq9a9P"
      },
      "source": [
        "### Task 1.1 Implement T5 encoder block\n",
        "\n",
        "Implement a T5 encoder block with:\n",
        "- Bidirectional multi-head self-attention (all positions can attend to all positions)\n",
        "- Feed-forward network\n",
        "- Layer normalization (pre-norm architecture: normalize before sub-layers)\n",
        "- Residual connections around each sub-layer\n",
        "\n",
        "The encoder block should follow this structure:\n",
        "1. Self-attention: `x = x + dropout(attention(layer_norm(x)))`\n",
        "2. Feed-forward: `x = x + dropout(ffn(layer_norm(x)))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BlFf55Sp9a9Q",
      "metadata": {
        "id": "BlFf55Sp9a9Q"
      },
      "outputs": [],
      "source": [
        "class T5EncoderBlock(nn.Module):\n",
        "    \"\"\"T5 Encoder Block with bidirectional self-attention.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # TODO: Initialize:\n",
        "        # - Multi-head self-attention (bidirectional, no masking)\n",
        "        # - Feed-forward network\n",
        "        # - Two layer normalization layers (one for attention, one for FFN)\n",
        "        # - Dropout layer\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # TODO: Implement encoder block with residual connections (pre-norm)\n",
        "        # 1. Self-attention: x = x + dropout(attention(layer_norm(x)))\n",
        "        # 2. Feed-forward: x = x + dropout(ffn(layer_norm(x)))\n",
        "        # Note: Apply layer norm BEFORE the sub-layer (pre-norm architecture)\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "encoder_block = T5EncoderBlock(d_model=512, n_heads=8, d_ff=2048)\n",
        "x = torch.randn(2, 10, 512)\n",
        "output = encoder_block(x)\n",
        "print(f\"Encoder block output shape: {output.shape}\")\n",
        "## RESULT_CHECKING_POINT -> torch.Size([2, 10, 512])"
      ],
      "metadata": {
        "id": "hd9_vWK8ze6V"
      },
      "id": "hd9_vWK8ze6V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "CGRy3jtN9a9R",
      "metadata": {
        "id": "CGRy3jtN9a9R"
      },
      "source": [
        "### Task 1.2 Build encoder stack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rulPLwVs9a9R",
      "metadata": {
        "id": "rulPLwVs9a9R"
      },
      "outputs": [],
      "source": [
        "class T5Encoder(nn.Module):\n",
        "    \"\"\"T5 Encoder: Stack of encoder blocks with embeddings and positional encoding.\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # TODO: Initialize:\n",
        "        # - Token embedding layer\n",
        "        # - Positional encoding\n",
        "        # - Stack of encoder blocks (n_layers)\n",
        "        # - Final layer normalization (optional, but common in T5)\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # TODO: Implement encoder forward pass\n",
        "        # 1. Token embeddings\n",
        "        # 2. Add positional encoding\n",
        "        # 3. Apply dropout\n",
        "        # 4. Pass through encoder blocks\n",
        "        # 5. Apply final layer norm\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "        return x\n",
        "\n",
        "# Test your implementation\n",
        "encoder = T5Encoder(vocab_size=1000, d_model=512, n_heads=8, n_layers=6, d_ff=2048)\n",
        "x = torch.randint(0, 1000, (2, 10))\n",
        "output = encoder(x)\n",
        "print(f\"Encoder output shape: {output.shape}\")\n",
        "## RESULT_CHECKING_POINT -> torch.Size([2, 10, 512])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iBEnibTD9a9S",
      "metadata": {
        "id": "iBEnibTD9a9S"
      },
      "source": [
        "### Task 1.3 Test encoder implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y777s7JV9a9S",
      "metadata": {
        "id": "y777s7JV9a9S"
      },
      "outputs": [],
      "source": [
        "def test_encoder(encoder, sample_text, max_len=50):\n",
        "    \"\"\"Test encoder and visualize attention patterns.\"\"\"\n",
        "    encoder.eval()\n",
        "\n",
        "    # Encode text → ensure tensors go to the correct device\n",
        "    encoded = torch.tensor(encode(sample_text[:max_len]), device=device).unsqueeze(0)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = encoder(encoded)\n",
        "\n",
        "    print(f\"Input text: {sample_text[:max_len]}\")\n",
        "    print(f\"Input shape: {encoded.shape}\")\n",
        "    print(f\"Encoder output shape: {output.shape}\")\n",
        "    print(f\"Encoder output mean: {output.mean().item():.4f}\")\n",
        "    print(f\"Encoder output std: {output.std().item():.4f}\")\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dnhEc-0O9a9S",
      "metadata": {
        "id": "dnhEc-0O9a9S"
      },
      "source": [
        "## TASK 2: T5 Decoder with Cross-Attention (7 points)\n",
        "\n",
        "Build the decoder stack with causal self-attention and cross-attention to encoder outputs. The decoder uses masked self-attention (causal) for autoregressive generation and cross-attention to attend to the encoder's output.\n",
        "\n",
        "**2.1**: *Cross-Attention Mechanism* — Implement cross-attention where queries come from decoder states, and keys/values come from encoder outputs\n",
        "\n",
        "**2.2**: *T5 Decoder Block* — Implement decoder block with masked self-attention, cross-attention, and feed-forward network\n",
        "\n",
        "**2.3**: *Decoder Stack* — Stack multiple decoder blocks with positional encoding and token embeddings\n",
        "\n",
        "### Task 2.1 Implement cross-attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wn6EKaNz9a9S",
      "metadata": {
        "id": "wn6EKaNz9a9S"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionHead(nn.Module):\n",
        "    \"\"\"Single head of cross-attention (decoder attends to encoder).\"\"\"\n",
        "    def __init__(self, head_size, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # Query comes from decoder, Key and Value come from encoder\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, decoder_states, encoder_states):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossMultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head cross-attention for T5.\"\"\"\n",
        "    def __init__(self, n_head, head_size, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, decoder_states, encoder_states):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "        return out"
      ],
      "metadata": {
        "id": "n1iL1E4f0juf"
      },
      "id": "n1iL1E4f0juf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "cross_attn = CrossMultiHeadAttention(n_head=8, head_size=64, n_embd=512, dropout=0.1)\n",
        "decoder_states = torch.randn(2, 10, 512)  # (batch, decoder_seq_len, d_model)\n",
        "encoder_states = torch.randn(2, 15, 512)  # (batch, encoder_seq_len, d_model)\n",
        "output = cross_attn(decoder_states, encoder_states)\n",
        "print(f\"Cross-attention output shape: {output.shape}\")\n",
        "## RESULT_CHECKING_POINT -> torch.Size([2, 10, 512])"
      ],
      "metadata": {
        "id": "7sqBlHHl0lNg"
      },
      "id": "7sqBlHHl0lNg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "98016e25",
      "metadata": {
        "id": "98016e25"
      },
      "source": [
        "### Task 2.2 Implement T5 decoder block\n",
        "\n",
        "Implement the decoder block with:\n",
        "- Masked self-attention (causal, for autoregressive generation)\n",
        "- Cross-attention (decoder attends to encoder)\n",
        "- Feed-forward network\n",
        "- Three layer normalization layers (pre-norm)\n",
        "- Residual connections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_causal_mask(seq_len):\n",
        "    return torch.tril(torch.ones(seq_len, seq_len))"
      ],
      "metadata": {
        "id": "mhAlzMrL2dWw"
      },
      "id": "mhAlzMrL2dWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6b2757",
      "metadata": {
        "id": "4f6b2757"
      },
      "outputs": [],
      "source": [
        "class T5DecoderBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a96190a",
      "metadata": {
        "id": "7a96190a"
      },
      "source": [
        "### Task 2.3 Build decoder stack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "401c2484",
      "metadata": {
        "id": "401c2484"
      },
      "outputs": [],
      "source": [
        "class T5Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, block_size, max_len, dropout):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing T5DecoderBlock...\")\n",
        "decoder_block = T5DecoderBlock(n_embd=512, n_head=8, block_size=128, dropout=0.1)\n",
        "x = torch.randn(2, 10, 512)\n",
        "encoder_output = torch.randn(2, 15, 512)\n",
        "output = decoder_block(x, encoder_output)\n",
        "print(f\"Decoder block output shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "RLDAn5VgHcJn"
      },
      "id": "RLDAn5VgHcJn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f41025bc",
      "metadata": {
        "id": "f41025bc"
      },
      "source": [
        "## TASK 3: Full T5 Model and Machine Translation (7 points)\n",
        "\n",
        "Combine encoder and decoder into a complete T5 model and train it on English-German machine translation.\n",
        "\n",
        "**3.1**: *Complete T5 Model* — Combine encoder and decoder stacks with shared token embeddings and language modeling head\n",
        "\n",
        "**3.2**: *Translation Dataset Preparation* — Prepare English-German translation dataset with proper batching and padding\n",
        "\n",
        "**3.3**: *Training and Testing* — Train model on translation task and evaluate performance\n",
        "\n",
        "### Task 3.1 Build complete T5 model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18fe5485",
      "metadata": {
        "id": "18fe5485"
      },
      "outputs": [],
      "source": [
        "class T5Model(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, block_size, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc11ec90",
      "metadata": {
        "id": "bc11ec90"
      },
      "source": [
        "### Task 3.2 Prepare translation dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translation_batch(split, batch_size=4):\n",
        "    \"\"\"Get a batch of English-German translation pairs.\"\"\"\n",
        "    pairs = train_pairs if split == 'train' else val_pairs\n",
        "\n",
        "    # Random sampling\n",
        "    indices = torch.randint(0, len(pairs), (batch_size,))\n",
        "\n",
        "    src_batch = []\n",
        "    tgt_batch = []\n",
        "\n",
        "    for idx in indices:\n",
        "        en_text, de_text = pairs[idx]\n",
        "        src_ids = encode(en_text)\n",
        "        # German: Add SOS at start, EOS at end\n",
        "        tgt_ids = [SOS_IDX] + encode(de_text) + [EOS_IDX]\n",
        "\n",
        "        src_batch.append(torch.tensor(src_ids, dtype=torch.long))\n",
        "        tgt_batch.append(torch.tensor(tgt_ids, dtype=torch.long))\n",
        "\n",
        "    # Pad\n",
        "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    tgt_batch = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # Teacher forcing split\n",
        "    tgt_input = tgt_batch[:, :-1] # Input: SOS ... Token\n",
        "    tgt_output = tgt_batch[:, 1:] # Target: Token ... EOS\n",
        "\n",
        "    return src_batch.to(device), tgt_input.to(device), tgt_output.to(device)"
      ],
      "metadata": {
        "id": "RNSHLo3jRnL3"
      },
      "id": "RNSHLo3jRnL3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 512 # Increased from 64 to 512 to handle sentences > 64 chars\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "model = T5Model(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    n_layers=2,\n",
        "    d_ff=512,\n",
        "    block_size=BLOCK_SIZE\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model initialized with block_size={BLOCK_SIZE}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Updated Batch Function with Truncation\n",
        "def get_safe_batch(split, batch_size=4, max_len=BLOCK_SIZE):\n",
        "    src_batch, tgt_in, tgt_out = get_translation_batch(split, batch_size)\n",
        "\n",
        "    # Truncate if they exceed block_size\n",
        "    if src_batch.size(1) > max_len:\n",
        "        src_batch = src_batch[:, :max_len]\n",
        "    if tgt_in.size(1) > max_len:\n",
        "        tgt_in = tgt_in[:, :max_len]\n",
        "        tgt_out = tgt_out[:, :max_len] # Output must match input length\n",
        "\n",
        "    return src_batch, tgt_in, tgt_out\n",
        "\n",
        "print(\"Starting training...\")\n",
        "model.train()"
      ],
      "metadata": {
        "id": "HVegxzwEKV6V"
      },
      "id": "HVegxzwEKV6V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "acc5916c",
      "metadata": {
        "id": "acc5916c"
      },
      "source": [
        "### Task 3.3 Train and test translation model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PRINT_EVERY = 50\n",
        "\n",
        "for epoch in range(5):   # or more epochs later\n",
        "    for step in range(len(train_pairs)//BATCH_SIZE):\n",
        "        src, tgt_in, tgt_out = get_safe_batch('train', batch_size=BATCH_SIZE)\n",
        "\n",
        "        logits = model(src, tgt_in)\n",
        "        loss = criterion(logits.reshape(-1, vocab_size), tgt_out.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print every few iterations\n",
        "        if step % PRINT_EVERY == 0:\n",
        "            print(f\"Epoch {epoch} | Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(\"✓ Training completed successfully!\")\n"
      ],
      "metadata": {
        "id": "owQWdJzhIh4R"
      },
      "id": "owQWdJzhIh4R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode (disables dropout)\n",
        "model.eval()\n",
        "\n",
        "def translate_sentence(sentence, model, max_length=100):\n",
        "    \"\"\"\n",
        "    Translates an English sentence to German using the trained T5Model.\n",
        "    Uses greedy decoding.\n",
        "    \"\"\"\n",
        "    # 1. Prepare Source (English)\n",
        "    # Encode and add batch dimension [1, seq_len]\n",
        "    src_ids = encode(sentence)\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # 2. Encoder Pass\n",
        "    # We encode the source sequence once\n",
        "    with torch.no_grad():\n",
        "        encoder_output = model.encoder(src_tensor)\n",
        "\n",
        "    # 3. Decoder Loop (Autoregressive)\n",
        "    # Start with the Start-Of-Sequence token\n",
        "    decoder_input = torch.tensor([[SOS_IDX]], dtype=torch.long).to(device)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            # Pass current sequence and encoder output to decoder\n",
        "            # decoder_input shape: [1, current_seq_len]\n",
        "            decoder_output = model.decoder(decoder_input, encoder_output)\n",
        "\n",
        "            # Project to vocabulary\n",
        "            logits = model.lm_head(decoder_output)\n",
        "\n",
        "            # Get the token with highest probability for the last position\n",
        "            # logits shape: [1, current_seq_len, vocab_size]\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n",
        "\n",
        "        # Stop if End-Of-Sequence token is generated\n",
        "        if next_token_id == EOS_IDX:\n",
        "            break\n",
        "\n",
        "        generated_tokens.append(next_token_id)\n",
        "\n",
        "        # Append prediction to decoder input for next step\n",
        "        next_token_tensor = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
        "        decoder_input = torch.cat([decoder_input, next_token_tensor], dim=1)\n",
        "\n",
        "    # 4. Decode to string\n",
        "    translated_text = decode(generated_tokens)\n",
        "    return translated_text\n",
        "\n",
        "# --- Test Translations ---\n",
        "print(\"--- Translation Demo ---\")\n",
        "test_sentences = [\n",
        "    \"Hello\",\n",
        "    \"Good morning\",\n",
        "    \"I love you\",\n",
        "    \"Where is the bathroom?\",\n",
        "    \"This is difficult\"\n",
        "]\n",
        "\n",
        "for s in test_sentences:\n",
        "    trans = translate_sentence(s, model)\n",
        "    print(f\"EN: {s:25} -> DE: {trans}\")\n",
        "\n",
        "# Try a custom one\n",
        "custom = \"I am hungry\"\n",
        "print(f\"\\nCustom: {custom} -> {translate_sentence(custom, model)}\")"
      ],
      "metadata": {
        "id": "ykI_wrHXMtd1"
      },
      "id": "ykI_wrHXMtd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS (2 points): Solve any task with an LLM\n",
        "\n",
        "**Goal.**  \n",
        "Pick **one** of the homework tasks (Task 1, Task 2, or Task 3) and solve it using an **LLM**. Provide the **LLM** with the **task description** and any **starter/prerequisite code** it depends on, ask it to generate a complete **code solution** first, then run that **generated code** here in the code notebook yourself. Finally, document what you did and **compare** the LLM’s result to your own pipeline.\n",
        "\n",
        "**What to deliver below.**\n",
        "1) **LLM used** (name + version, e.g., “Llama-3-8B-Instruct”, “GPT-x”, “Claude-x”, “Mistral-x”, etc.).  \n",
        "2) **Prompt(s)** you used.  \n",
        "3) **LLM output** — copy and paste the generated code.  \n",
        "4) **Comparison** to your solution: what matches or differs (quantitative or qualitative).  \n",
        "5) **Reflection**: what the LLM was **good at** vs **bad at**, what it got **right** vs **wrong**.\n",
        "\n",
        "> **No code required.** You do **not** need to run, share, or submit any code used for the LLM generation. Provide only the deliverables listed above.\n",
        "> You may use any LLMs through any interface (API, web UI, local inference).\n"
      ],
      "metadata": {
        "id": "w8rDo9crziNZ"
      },
      "id": "w8rDo9crziNZ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgjqVReHzpb0"
      },
      "id": "BgjqVReHzpb0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "_5jqzZYb9a9J",
        "Y4X0a4sr9a9L",
        "ZfPZQJP_9a9O",
        "dnhEc-0O9a9S"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}