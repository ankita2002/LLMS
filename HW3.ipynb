{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita2002/LLMS/blob/main/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V-5Ms5899a9H",
      "metadata": {
        "id": "V-5Ms5899a9H"
      },
      "source": [
        "# Homework 3: Transformers, T5, Machine Translation\n",
        "\n",
        "This homework guides you through implementing a complete T5 (Text-To-Text Transfer Transformer) encoder-decoder architecture from scratch for machine translation. You will build the encoder stack, decoder stack with cross-attention, and train the model on English-German translation.\n",
        "\n",
        "**Total Points: 20**\n",
        "\n",
        "**Instructions:**\n",
        "1. Complete all tasks in this notebook\n",
        "2. Ensure your code runs without errors\n",
        "3. Submit both this notebook and any additional files created\n",
        "4. Write clear explanations for your approach\n",
        "5. Train your model on the English-German translation dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_5jqzZYb9a9J",
      "metadata": {
        "id": "_5jqzZYb9a9J"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "za4ixerZ9a9K",
      "metadata": {
        "id": "za4ixerZ9a9K",
        "outputId": "d54a539a-1c28-46dc-f5ab-bd83380fa39d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y4X0a4sr9a9L",
      "metadata": {
        "id": "Y4X0a4sr9a9L"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "We'll use the WMT English-German translation dataset. This dataset contains parallel sentences in English and German, perfect for training a sequence-to-sequence translation model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "MI8Fg-6Yy8ZZ"
      },
      "id": "MI8Fg-6Yy8ZZ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "sS13-kii9a9N",
      "metadata": {
        "id": "sS13-kii9a9N",
        "outputId": "ca93c6cc-ceaf-4712-8897-8446f4f1edca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing WMT Dataset Loader (No backups, real data)...\n",
            "Successfully loaded 50000 pairs from WMT16.\n",
            "Building vocabulary from dataset...\n",
            "Shared vocabulary size: 113\n",
            "Special tokens: PAD=0, SOS=1, EOS=2, UNK=3\n",
            "----------------------------------------\n",
            "Train pairs: 40000\n",
            "Val pairs:   10000\n",
            "----------------------------------------\n",
            "Sample Data (Real WMT):\n",
            "  EN: Please rise, then, for this minute' s silence.\n",
            "  DE: Ich bitte Sie, sich zu einer Schweigeminute zu erheben.\n",
            "  Encoded: [45, 69, 62, 58, 76, 62, 4, 75, 66, 76, 62, 13, 4, 77, 65, 62, 71, 13, 4, 63, 72, 75, 4, 77, 65, 66, 76, 4, 70, 66, 71, 78, 77, 62, 9, 4, 76, 4, 76, 66, 69, 62, 71, 60, 62, 15]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "import tqdm\n",
        "\n",
        "print(\"Initializing WMT Dataset Loader (No backups, real data)...\")\n",
        "\n",
        "# 1. Load the Real WMT Dataset\n",
        "# We use WMT16 (German-English), a standard benchmark.\n",
        "# We take the first 50,000 examples to keep RAM usage efficient while ensuring variety.\n",
        "# If you want the full dataset (4.5M pairs), remove the [:50000] slice.\n",
        "dataset = load_dataset(\"wmt16\", \"de-en\", split=\"train[:50000]\")\n",
        "\n",
        "print(f\"Successfully loaded {len(dataset)} pairs from WMT16.\")\n",
        "\n",
        "# 2. Extract pairs into memory\n",
        "translation_pairs = []\n",
        "for item in dataset:\n",
        "    # WMT data structure is usually {'translation': {'de': '...', 'en': '...'}}\n",
        "    en_text = item['translation']['en']\n",
        "    de_text = item['translation']['de']\n",
        "    translation_pairs.append((en_text, de_text))\n",
        "\n",
        "# 3. Build Vocabulary (Character-level)\n",
        "# We stick to character-level to match your previous logic.\n",
        "# Note: For SOTA results on WMT, you would usually use BPE (Byte Pair Encoding),\n",
        "# but character-level is robust for learning deep learning fundamentals.\n",
        "print(\"Building vocabulary from dataset...\")\n",
        "\n",
        "counter = Counter()\n",
        "for en, de in translation_pairs:\n",
        "    counter.update(en) # Keep case sensitivity? standard is usually to keep it for WMT\n",
        "    counter.update(de)\n",
        "\n",
        "# Filter rare characters to keep vocab clean (optional, but good for real web data)\n",
        "MIN_FREQ = 5\n",
        "common_chars = [char for char, count in counter.items() if count >= MIN_FREQ]\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "chars = special_tokens + sorted(common_chars)\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f'Shared vocabulary size: {vocab_size}')\n",
        "\n",
        "# 4. Mappings\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "PAD_IDX = stoi['<pad>']\n",
        "SOS_IDX = stoi['<sos>']\n",
        "EOS_IDX = stoi['<eos>']\n",
        "UNK_IDX = stoi['<unk>']\n",
        "\n",
        "# 5. Encoding/Decoding Functions\n",
        "def encode(s):\n",
        "    \"\"\"Encode string to token IDs (Character level).\"\"\"\n",
        "    # We do NOT lower() here to preserve information, unlike the toy example\n",
        "    return [stoi.get(c, UNK_IDX) for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    \"\"\"Decode token IDs to string.\"\"\"\n",
        "    return ''.join([itos[i] for i in l if i != PAD_IDX])\n",
        "\n",
        "print(f\"Special tokens: PAD={PAD_IDX}, SOS={SOS_IDX}, EOS={EOS_IDX}, UNK={UNK_IDX}\")\n",
        "\n",
        "# 6. Train/Val Split (80/20)\n",
        "n_train = int(0.8 * len(translation_pairs))\n",
        "train_pairs = translation_pairs[:n_train]\n",
        "val_pairs = translation_pairs[n_train:]\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "print(f\"Val pairs:   {len(val_pairs)}\")\n",
        "print(\"-\" * 40)\n",
        "print(\"Sample Data (Real WMT):\")\n",
        "print(f\"  EN: {train_pairs[5][0]}\") # Random index to show real sentences\n",
        "print(f\"  DE: {train_pairs[5][1]}\")\n",
        "print(f\"  Encoded: {encode(train_pairs[5][0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZfPZQJP_9a9O",
      "metadata": {
        "id": "ZfPZQJP_9a9O"
      },
      "source": [
        "## TASK 1: T5 Encoder Implementation (6 points)\n",
        "\n",
        "Build the encoder stack that processes source sequences with bidirectional attention. The encoder is similar to BERT but designed for the T5 architecture.\n",
        "\n",
        "**1.1**: *Encoder Block* — Implement T5 encoder block with bidirectional self-attention, feed-forward network, layer normalization, and residual connections\n",
        "\n",
        "**1.2**: *Encoder Stack* — Stack multiple encoder blocks with positional encoding and token embeddings\n",
        "\n",
        "**1.3**: *Encoder Testing* — Test encoder implementation and verify bidirectional attention works correctly\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function: Positional Encoding\n",
        "#This code builds a fixed table of sine/cos position vectors, then adds the right row to each token embedding so the Transformer knows token order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:x.size(1), :].unsqueeze(0)"
      ],
      "metadata": {
        "id": "9WrahFO_zWIn"
      },
      "id": "9WrahFO_zWIn",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, hidden_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, hidden_size), #expands the vector\n",
        "            nn.GELU(), # A smooth activation function, Adds non-linearity\n",
        "            nn.Dropout(dropout), #Randomly zeros some values during training, helps reduce overfitting\n",
        "            nn.Linear(hidden_size, n_embd), #reduces the expanded size back to original\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)"
      ],
      "metadata": {
        "id": "MQOwi67NzVO5"
      },
      "id": "MQOwi67NzVO5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2e3G4xE39a9O",
      "metadata": {
        "id": "2e3G4xE39a9O"
      },
      "outputs": [],
      "source": [
        "# Helper function: Scaled Dot-Product Attention\n",
        "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        query: [batch_size, seq_len, d_k] or [batch_size, n_heads, seq_len, d_k]\n",
        "        key: [batch_size, seq_len, d_k] or [batch_size, n_heads, seq_len, d_k]\n",
        "        value: [batch_size, seq_len, d_v] or [batch_size, n_heads, seq_len, d_v]\n",
        "        mask: [batch_size, seq_len, seq_len] or None (0s for masked positions)\n",
        "        dropout: Dropout layer or None\n",
        "    Returns:\n",
        "        output: [batch_size, seq_len, d_v] or [batch_size, n_heads, seq_len, d_v]\n",
        "        attention_weights: [batch_size, seq_len, seq_len] or [batch_size, n_heads, seq_len, seq_len]\n",
        "    \"\"\"\n",
        "    d_k = query.size(-1)\n",
        "\n",
        "    # Compute attention scores: Q @ K^T / sqrt(d_k) using dot product\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    #it id divided by sqrt(d_k) to keep numbers stable\n",
        "    #Without this scaling, scores can get too large and softmax becomes unstable.\n",
        "\n",
        "    # Apply mask if provided (set masked positions to -inf before softmax)\n",
        "    # Mask is used to block attention to some positions.\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # Apply dropout if provided\n",
        "    if dropout is not None:\n",
        "        attention_weights = dropout(attention_weights)\n",
        "\n",
        "    # Multiply attention weights by values\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = n_embd // n_head\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.query = nn.Linear(n_embd, n_embd)\n",
        "        self.key = nn.Linear(n_embd, n_embd)\n",
        "        self.value = nn.Linear(n_embd, n_embd)\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        B, T, C = q.shape\n",
        "        q = self.query(q).view(B, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = self.key(k).view(B, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = self.value(v).view(B, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        scores = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
        "        out = (attn @ v).transpose(1, 2).contiguous().view(B, -1, C)\n",
        "        return self.proj(out), attn"
      ],
      "metadata": {
        "id": "QGOuf4XfzT2E"
      },
      "id": "QGOuf4XfzT2E",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "qLeTriMq9a9P",
      "metadata": {
        "id": "qLeTriMq9a9P"
      },
      "source": [
        "### Task 1.1 Implement T5 encoder block\n",
        "\n",
        "Implement a T5 encoder block with:\n",
        "- Bidirectional multi-head self-attention (all positions can attend to all positions)\n",
        "- Feed-forward network\n",
        "- Layer normalization (pre-norm architecture: normalize before sub-layers)\n",
        "- Residual connections around each sub-layer\n",
        "\n",
        "The encoder block should follow this structure:\n",
        "1. Self-attention: `x = x + dropout(attention(layer_norm(x)))`\n",
        "2. Feed-forward: `x = x + dropout(ffn(layer_norm(x)))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "BlFf55Sp9a9Q",
      "metadata": {
        "id": "BlFf55Sp9a9Q"
      },
      "outputs": [],
      "source": [
        "class T5EncoderBlock(nn.Module):\n",
        "    \"\"\"T5 Encoder Block with bidirectional self-attention.\"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # TODO: Initialize:\n",
        "\n",
        "        # - Multi-head self-attention (bidirectional, no masking)\n",
        "        self.self_attn = MultiHeadAttention(n_embd=d_model, n_head=n_heads, dropout=dropout)\n",
        "        # - Feed-forward network\n",
        "        self.ffn = FeedForward(n_embd=d_model, hidden_size=d_ff, dropout=dropout)\n",
        "        # - Two layer normalization layers (one for attention, one for FFN)\n",
        "        self.ln_attn = nn.LayerNorm(d_model)\n",
        "        self.ln_ffn = nn.LayerNorm(d_model)\n",
        "        # - Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # TODO: Implement encoder block with residual connections (pre-norm)\n",
        "        # 1. Self-attention: x = x + dropout(attention(layer_norm(x)))\n",
        "        x_norm = self.ln_attn(x) #pre-norm: normalize before attention\n",
        "        attn_out, _ = self.self_attn(x_norm, x_norm, x_norm, mask=mask) #(Q,K,V,mask) self attention\n",
        "        # mask is optional. In encoder you usually use it to ignore <pad> tokens.\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        # 2. Feed-forward: x = x + dropout(ffn(layer_norm(x)))\n",
        "        # Note: Apply layer norm BEFORE the sub-layer (pre-norm architecture)\n",
        "        #FFN improves each token’s representation on its own.\n",
        "        #It’s a small neural net: expand -> nonlinearity -> shrink back.\n",
        "        x_norm = self.ln_ffn(x)\n",
        "        ffn_out = self.ffn(x_norm)\n",
        "        x = x + self.dropout(ffn_out)\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "        return x\n",
        "\n",
        "#normalize -> self-attention -> add back (residual) -> normalize -> feed-forward -> add back (residual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "encoder_block = T5EncoderBlock(d_model=512, n_heads=8, d_ff=2048)\n",
        "x = torch.randn(2, 10, 512)\n",
        "output = encoder_block(x)\n",
        "print(f\"Encoder block output shape: {output.shape}\")\n",
        "## RESULT_CHECKING_POINT -> torch.Size([2, 10, 512])"
      ],
      "metadata": {
        "id": "hd9_vWK8ze6V",
        "outputId": "5e3461b7-3281-4d2c-e950-c03d189c6c36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hd9_vWK8ze6V",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder block output shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CGRy3jtN9a9R",
      "metadata": {
        "id": "CGRy3jtN9a9R"
      },
      "source": [
        "### Task 1.2 Build encoder stack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "rulPLwVs9a9R",
      "metadata": {
        "id": "rulPLwVs9a9R",
        "outputId": "5ba10eec-61a7-4b43-c4e0-b63de56e9bb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ],
      "source": [
        "class T5Encoder(nn.Module):\n",
        "    \"\"\"T5 Encoder: Stack of encoder blocks with embeddings and positional encoding.\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # TODO: Initialize:\n",
        "        # - Token embedding layer (ID-> vector)\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        # - Positional encoding (add order info)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len)\n",
        "        # Dropout after embeddings + positional encoding\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # - Stack of encoder blocks (n_layers)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            T5EncoderBlock(d_model=d_model, n_heads=n_heads, d_ff=d_ff, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        # - Final layer normalization (optional, but common in T5)\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # TODO: Implement encoder forward pass\n",
        "        # 1. Token embeddings\n",
        "        x = self.token_emb(x)  # (B, T) -> (B, T, d_model)\n",
        "        # 2. Add positional encoding\n",
        "        x = self.pos_enc(x)    # (B, T, d_model)\n",
        "        # 3. Apply dropout\n",
        "        x = self.dropout(x)\n",
        "        # 4. Pass through encoder blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask=mask)\n",
        "        # 5. Apply final layer norm\n",
        "        x = self.ln_f(x)\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "        return x\n",
        "\n",
        "# Test your implementation\n",
        "encoder = T5Encoder(vocab_size=1000, d_model=512, n_heads=8, n_layers=6, d_ff=2048)\n",
        "x = torch.randint(0, 1000, (2, 10))\n",
        "output = encoder(x)\n",
        "print(f\"Encoder output shape: {output.shape}\")\n",
        "## RESULT_CHECKING_POINT -> torch.Size([2, 10, 512])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iBEnibTD9a9S",
      "metadata": {
        "id": "iBEnibTD9a9S"
      },
      "source": [
        "### Task 1.3 Test encoder implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "y777s7JV9a9S",
      "metadata": {
        "id": "y777s7JV9a9S"
      },
      "outputs": [],
      "source": [
        "def test_encoder(encoder, sample_text, max_len=50):\n",
        "    \"\"\"Test encoder and visualize attention patterns.\"\"\"\n",
        "    encoder.eval()\n",
        "\n",
        "    # Encode text → ensure tensors go to the correct device\n",
        "    encoded = torch.tensor(encode(sample_text[:max_len]), device=device).unsqueeze(0)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = encoder(encoded)\n",
        "\n",
        "    print(f\"Input text: {sample_text[:max_len]}\")\n",
        "    print(f\"Input shape: {encoded.shape}\")\n",
        "    print(f\"Encoder output shape: {output.shape}\")\n",
        "    print(f\"Encoder output mean: {output.mean().item():.4f}\")\n",
        "    print(f\"Encoder output std: {output.std().item():.4f}\")\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dnhEc-0O9a9S",
      "metadata": {
        "id": "dnhEc-0O9a9S"
      },
      "source": [
        "## TASK 2: T5 Decoder with Cross-Attention (7 points)\n",
        "\n",
        "Build the decoder stack with causal self-attention and cross-attention to encoder outputs. The decoder uses masked self-attention (causal) for autoregressive generation and cross-attention to attend to the encoder's output.\n",
        "\n",
        "**2.1**: *Cross-Attention Mechanism* — Implement cross-attention where queries come from decoder states, and keys/values come from encoder outputs\n",
        "\n",
        "**2.2**: *T5 Decoder Block* — Implement decoder block with masked self-attention, cross-attention, and feed-forward network\n",
        "\n",
        "**2.3**: *Decoder Stack* — Stack multiple decoder blocks with positional encoding and token embeddings\n",
        "\n",
        "### Task 2.1 Implement cross-attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "wn6EKaNz9a9S",
      "metadata": {
        "id": "wn6EKaNz9a9S"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionHead(nn.Module):\n",
        "    \"\"\"Single head of cross-attention (decoder attends to encoder).\"\"\"\n",
        "    def __init__(self, head_size, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # Query comes from decoder, Key and Value come from encoder\n",
        "        self.q = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.k = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.v = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        self.scale = head_size ** -0.5\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, decoder_states, encoder_states):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # decoder_states: (B, T_dec, n_embd)\n",
        "        # encoder_states: (B, T_enc, n_embd)\n",
        "        Q = self.q(decoder_states)\n",
        "        K = self.k(encoder_states)\n",
        "        V = self.v(encoder_states)\n",
        "\n",
        "        # scores: (B, T_dec, T_enc)\n",
        "        scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)     # (B, T_dec, T_enc)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = attn @ V                       # (B, T_dec, head_size)\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossMultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head cross-attention for T5.\"\"\"\n",
        "    def __init__(self, n_head, head_size, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        self.heads = nn.ModuleList([\n",
        "            CrossAttentionHead(head_size=head_size, n_embd=n_embd, dropout=dropout)\n",
        "            for _ in range(n_head)\n",
        "        ])\n",
        "        self.proj = nn.Linear(n_head * head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, decoder_states, encoder_states):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        out = torch.cat([h(decoder_states, encoder_states) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))   # (B, T_dec, n_embd)\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "        return out"
      ],
      "metadata": {
        "id": "n1iL1E4f0juf"
      },
      "id": "n1iL1E4f0juf",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "cross_attn = CrossMultiHeadAttention(n_head=8, head_size=64, n_embd=512, dropout=0.1)\n",
        "decoder_states = torch.randn(2, 10, 512)  # (batch, decoder_seq_len, d_model)\n",
        "encoder_states = torch.randn(2, 15, 512)  # (batch, encoder_seq_len, d_model)\n",
        "output = cross_attn(decoder_states, encoder_states)\n",
        "print(f\"Cross-attention output shape: {output.shape}\")\n",
        "## RESULT_CHECKING_POINT -> torch.Size([2, 10, 512])"
      ],
      "metadata": {
        "id": "7sqBlHHl0lNg",
        "outputId": "a0e12ff7-49af-4b78-a686-8aa32f7a2800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7sqBlHHl0lNg",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-attention output shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98016e25",
      "metadata": {
        "id": "98016e25"
      },
      "source": [
        "### Task 2.2 Implement T5 decoder block\n",
        "\n",
        "Implement the decoder block with:\n",
        "- Masked self-attention (causal, for autoregressive generation)\n",
        "- Cross-attention (decoder attends to encoder)\n",
        "- Feed-forward network\n",
        "- Three layer normalization layers (pre-norm)\n",
        "- Residual connections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_causal_mask(seq_len):\n",
        "    return torch.tril(torch.ones(seq_len, seq_len))"
      ],
      "metadata": {
        "id": "mhAlzMrL2dWw"
      },
      "id": "mhAlzMrL2dWw",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4f6b2757",
      "metadata": {
        "id": "4f6b2757"
      },
      "outputs": [],
      "source": [
        "class T5DecoderBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # Masked self-attention\n",
        "        self.self_attn = MultiHeadAttention(n_embd=n_embd, n_head=n_head, dropout=dropout)\n",
        "        #Cross-attention (decoder -> encoder)\n",
        "        self.cross_attn = CrossMultiHeadAttention(\n",
        "            n_head=n_head, head_size=head_size, n_embd=n_embd, dropout=dropout\n",
        "        )\n",
        "        #Feed-forward\n",
        "        self.ffn = FeedForward(n_embd=n_embd, hidden_size=4 * n_embd, dropout=dropout)\n",
        "\n",
        "        # Pre-norm LayerNorms (one before each sub-layer)\n",
        "        self.ln_self = nn.LayerNorm(n_embd)\n",
        "        self.ln_cross = nn.LayerNorm(n_embd)\n",
        "        self.ln_ffn = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Dropout around each sub-layer output\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Causal mask: allow attending to past + self only\n",
        "        causal = torch.tril(torch.ones(block_size, block_size))\n",
        "        self.register_buffer(\"causal_mask\", causal)\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        B, T, C = x.shape\n",
        "        # mask self attention\n",
        "        mask = self.causal_mask[:T, :T].unsqueeze(0).unsqueeze(0)  # (1,1,T,T)\n",
        "        x_norm = self.ln_self(x)\n",
        "        self_out, _ = self.self_attn(x_norm, x_norm, x_norm, mask=mask)\n",
        "        x = x + self.dropout(self_out)\n",
        "\n",
        "        #cross-attention\n",
        "        x_norm = self.ln_cross(x)\n",
        "        cross_out = self.cross_attn(x_norm, encoder_output)\n",
        "        x = x + self.dropout(cross_out)\n",
        "\n",
        "        #Feed-Forward\n",
        "        x_norm = self.ln_ffn(x)\n",
        "        ffn_out = self.ffn(x_norm)\n",
        "        x = x + self.dropout(ffn_out)\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a96190a",
      "metadata": {
        "id": "7a96190a"
      },
      "source": [
        "### Task 2.3 Build decoder stack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "401c2484",
      "metadata": {
        "id": "401c2484"
      },
      "outputs": [],
      "source": [
        "class T5Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, block_size, max_len, dropout):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # Token embedding + positional encoding\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Stack of decoder blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            T5DecoderBlock(n_embd=d_model, n_head=n_heads, block_size=block_size, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        # x: (B, T_dec) token ids\n",
        "        # encoder_output: (B, T_enc, d_model)\n",
        "\n",
        "        #Token embeddings\n",
        "        x = self.token_emb(x)          # (B, T_dec, d_model)\n",
        "        #Positional encoding\n",
        "        x = self.pos_enc(x)            # (B, T_dec, d_model)\n",
        "        #Dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #Decoder blocks (masked self-attn + cross-attn + FFN)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, encoder_output)\n",
        "\n",
        "        #Final layer norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        return x\n",
        "        ## YOUR_CODE_ENDS_HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing T5DecoderBlock...\")\n",
        "decoder_block = T5DecoderBlock(n_embd=512, n_head=8, block_size=128, dropout=0.1)\n",
        "x = torch.randn(2, 10, 512)\n",
        "encoder_output = torch.randn(2, 15, 512)\n",
        "output = decoder_block(x, encoder_output)\n",
        "print(f\"Decoder block output shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "RLDAn5VgHcJn",
        "outputId": "299269aa-6f71-4bfa-c2b9-972ca0ae20bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RLDAn5VgHcJn",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing T5DecoderBlock...\n",
            "Decoder block output shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f41025bc",
      "metadata": {
        "id": "f41025bc"
      },
      "source": [
        "## TASK 3: Full T5 Model and Machine Translation (7 points)\n",
        "\n",
        "Combine encoder and decoder into a complete T5 model and train it on English-German machine translation.\n",
        "\n",
        "**3.1**: *Complete T5 Model* — Combine encoder and decoder stacks with shared token embeddings and language modeling head\n",
        "\n",
        "**3.2**: *Translation Dataset Preparation* — Prepare English-German translation dataset with proper batching and padding\n",
        "\n",
        "**3.3**: *Training and Testing* — Train model on translation task and evaluate performance\n",
        "\n",
        "### Task 3.1 Build complete T5 model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "18fe5485",
      "metadata": {
        "id": "18fe5485"
      },
      "outputs": [],
      "source": [
        "class T5Model(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, block_size, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        self.shared_emb = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Encoder + Decoder stacks\n",
        "        self.encoder = T5Encoder(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            d_ff=d_ff,\n",
        "            max_len=max_len,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.decoder = T5Decoder(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            d_ff=d_ff,\n",
        "            block_size=block_size,\n",
        "            max_len=max_len,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        # Replace their embeddings with the shared one\n",
        "        self.encoder.token_emb = self.shared_emb\n",
        "        self.decoder.token_emb = self.shared_emb\n",
        "\n",
        "        # Language modeling head (decoder states -> vocab logits)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Tie lm_head weights to the shared embedding (common trick)\n",
        "        self.lm_head.weight = self.shared_emb.weight\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        pad_idx = globals().get(\"PAD_IDX\", None)\n",
        "        if pad_idx is not None:\n",
        "            # (B, 1, 1, T_src) mask for encoder self-attn (1 = keep, 0 = pad)\n",
        "            src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        else:\n",
        "            src_mask = None\n",
        "\n",
        "        encoder_output = self.encoder(src, mask=src_mask)          # (B, T_src, d_model)\n",
        "        decoder_output = self.decoder(tgt, encoder_output)         # (B, T_tgt, d_model)\n",
        "        logits = self.lm_head(decoder_output)                      # (B, T_tgt, vocab_size)\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc11ec90",
      "metadata": {
        "id": "bc11ec90"
      },
      "source": [
        "### Task 3.2 Prepare translation dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translation_batch(split, batch_size=4):\n",
        "    \"\"\"Get a batch of English-German translation pairs.\"\"\"\n",
        "    pairs = train_pairs if split == 'train' else val_pairs\n",
        "\n",
        "    # Random sampling\n",
        "    indices = torch.randint(0, len(pairs), (batch_size,))\n",
        "\n",
        "    src_batch = []\n",
        "    tgt_batch = []\n",
        "\n",
        "    for idx in indices:\n",
        "        en_text, de_text = pairs[idx]\n",
        "        src_ids = encode(en_text)\n",
        "        # German: Add SOS at start, EOS at end\n",
        "        tgt_ids = [SOS_IDX] + encode(de_text) + [EOS_IDX]\n",
        "\n",
        "        src_batch.append(torch.tensor(src_ids, dtype=torch.long))\n",
        "        tgt_batch.append(torch.tensor(tgt_ids, dtype=torch.long))\n",
        "\n",
        "    # Pad\n",
        "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    tgt_batch = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # Teacher forcing split\n",
        "    tgt_input = tgt_batch[:, :-1] # Input: SOS ... Token\n",
        "    tgt_output = tgt_batch[:, 1:] # Target: Token ... EOS\n",
        "\n",
        "    return src_batch.to(device), tgt_input.to(device), tgt_output.to(device)"
      ],
      "metadata": {
        "id": "RNSHLo3jRnL3"
      },
      "id": "RNSHLo3jRnL3",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 512 # Increased from 64 to 512 to handle sentences > 64 chars\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "model = T5Model(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    n_layers=2,\n",
        "    d_ff=512,\n",
        "    block_size=BLOCK_SIZE\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model initialized with block_size={BLOCK_SIZE}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Updated Batch Function with Truncation\n",
        "def get_safe_batch(split, batch_size=4, max_len=BLOCK_SIZE):\n",
        "    src_batch, tgt_in, tgt_out = get_translation_batch(split, batch_size)\n",
        "\n",
        "    # Truncate if they exceed block_size\n",
        "    if src_batch.size(1) > max_len:\n",
        "        src_batch = src_batch[:, :max_len]\n",
        "    if tgt_in.size(1) > max_len:\n",
        "        tgt_in = tgt_in[:, :max_len]\n",
        "        tgt_out = tgt_out[:, :max_len] # Output must match input length\n",
        "\n",
        "    return src_batch, tgt_in, tgt_out\n",
        "\n",
        "print(\"Starting training...\")\n",
        "model.train()"
      ],
      "metadata": {
        "id": "HVegxzwEKV6V",
        "outputId": "a9e4716b-0ef2-4f5d-a632-7dd309cf7274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HVegxzwEKV6V",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with block_size=512\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5Model(\n",
              "  (shared_emb): Embedding(113, 128)\n",
              "  (encoder): T5Encoder(\n",
              "    (token_emb): Embedding(113, 128)\n",
              "    (pos_enc): PositionalEncoding()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0-1): 2 x T5EncoderBlock(\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): FeedForward(\n",
              "          (net): Sequential(\n",
              "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.1, inplace=False)\n",
              "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "            (4): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (ln_attn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (ln_ffn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): T5Decoder(\n",
              "    (token_emb): Embedding(113, 128)\n",
              "    (pos_enc): PositionalEncoding()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0-1): 2 x T5DecoderBlock(\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (cross_attn): CrossMultiHeadAttention(\n",
              "          (heads): ModuleList(\n",
              "            (0-3): 4 x CrossAttentionHead(\n",
              "              (q): Linear(in_features=128, out_features=32, bias=False)\n",
              "              (k): Linear(in_features=128, out_features=32, bias=False)\n",
              "              (v): Linear(in_features=128, out_features=32, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ffn): FeedForward(\n",
              "          (net): Sequential(\n",
              "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.1, inplace=False)\n",
              "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "            (4): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (ln_self): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (ln_cross): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (ln_ffn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=128, out_features=113, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc5916c",
      "metadata": {
        "id": "acc5916c"
      },
      "source": [
        "### Task 3.3 Train and test translation model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PRINT_EVERY = 50\n",
        "\n",
        "for epoch in range(5):   # or more epochs later\n",
        "    for step in range(len(train_pairs)//BATCH_SIZE):\n",
        "        src, tgt_in, tgt_out = get_safe_batch('train', batch_size=BATCH_SIZE)\n",
        "\n",
        "        logits = model(src, tgt_in)\n",
        "        loss = criterion(logits.reshape(-1, vocab_size), tgt_out.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print every few iterations\n",
        "        if step % PRINT_EVERY == 0:\n",
        "            print(f\"Epoch {epoch} | Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(\"✓ Training completed successfully!\")\n"
      ],
      "metadata": {
        "id": "owQWdJzhIh4R",
        "outputId": "df14dda1-fe67-45a1-e6e7-2c078c8748b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "owQWdJzhIh4R",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Step 0: Loss = 89.3607\n",
            "Epoch 0 | Step 50: Loss = 5.1698\n",
            "Epoch 0 | Step 100: Loss = 4.1441\n",
            "Epoch 0 | Step 150: Loss = 3.7162\n",
            "Epoch 0 | Step 200: Loss = 3.4283\n",
            "Epoch 0 | Step 250: Loss = 3.2717\n",
            "Epoch 0 | Step 300: Loss = 3.0852\n",
            "Epoch 0 | Step 350: Loss = 2.9160\n",
            "Epoch 0 | Step 400: Loss = 2.8818\n",
            "Epoch 0 | Step 450: Loss = 2.7830\n",
            "Epoch 0 | Step 500: Loss = 2.7026\n",
            "Epoch 0 | Step 550: Loss = 2.7062\n",
            "Epoch 0 | Step 600: Loss = 2.6180\n",
            "Epoch 0 | Step 650: Loss = 2.6384\n",
            "Epoch 0 | Step 700: Loss = 2.5355\n",
            "Epoch 0 | Step 750: Loss = 2.6297\n",
            "Epoch 0 | Step 800: Loss = 2.4999\n",
            "Epoch 0 | Step 850: Loss = 2.5242\n",
            "Epoch 0 | Step 900: Loss = 2.5133\n",
            "Epoch 0 | Step 950: Loss = 2.5112\n",
            "Epoch 0 | Step 1000: Loss = 2.5093\n",
            "Epoch 0 | Step 1050: Loss = 2.5554\n",
            "Epoch 0 | Step 1100: Loss = 2.4445\n",
            "Epoch 0 | Step 1150: Loss = 2.5198\n",
            "Epoch 0 | Step 1200: Loss = 2.4293\n",
            "Epoch 0 | Step 1250: Loss = 2.3989\n",
            "Epoch 0 | Step 1300: Loss = 2.4592\n",
            "Epoch 0 | Step 1350: Loss = 2.4719\n",
            "Epoch 0 | Step 1400: Loss = 2.4656\n",
            "Epoch 0 | Step 1450: Loss = 2.4237\n",
            "Epoch 0 | Step 1500: Loss = 2.4744\n",
            "Epoch 0 | Step 1550: Loss = 2.4311\n",
            "Epoch 0 | Step 1600: Loss = 2.4132\n",
            "Epoch 0 | Step 1650: Loss = 2.3721\n",
            "Epoch 0 | Step 1700: Loss = 2.4040\n",
            "Epoch 0 | Step 1750: Loss = 2.3415\n",
            "Epoch 0 | Step 1800: Loss = 2.3265\n",
            "Epoch 0 | Step 1850: Loss = 2.3641\n",
            "Epoch 0 | Step 1900: Loss = 2.3335\n",
            "Epoch 0 | Step 1950: Loss = 2.3751\n",
            "Epoch 0 | Step 2000: Loss = 2.3433\n",
            "Epoch 0 | Step 2050: Loss = 2.3524\n",
            "Epoch 0 | Step 2100: Loss = 2.2989\n",
            "Epoch 0 | Step 2150: Loss = 2.3666\n",
            "Epoch 0 | Step 2200: Loss = 2.2631\n",
            "Epoch 0 | Step 2250: Loss = 2.2890\n",
            "Epoch 0 | Step 2300: Loss = 2.2648\n",
            "Epoch 0 | Step 2350: Loss = 2.3209\n",
            "Epoch 0 | Step 2400: Loss = 2.2543\n",
            "Epoch 0 | Step 2450: Loss = 2.1703\n",
            "Epoch 1 | Step 0: Loss = 2.2537\n",
            "Epoch 1 | Step 50: Loss = 2.2354\n",
            "Epoch 1 | Step 100: Loss = 2.1620\n",
            "Epoch 1 | Step 150: Loss = 2.1937\n",
            "Epoch 1 | Step 200: Loss = 2.1359\n",
            "Epoch 1 | Step 250: Loss = 2.2758\n",
            "Epoch 1 | Step 300: Loss = 2.2554\n",
            "Epoch 1 | Step 350: Loss = 2.2016\n",
            "Epoch 1 | Step 400: Loss = 2.0615\n",
            "Epoch 1 | Step 450: Loss = 2.1444\n",
            "Epoch 1 | Step 500: Loss = 2.1130\n",
            "Epoch 1 | Step 550: Loss = 2.1051\n",
            "Epoch 1 | Step 600: Loss = 2.0481\n",
            "Epoch 1 | Step 650: Loss = 2.0620\n",
            "Epoch 1 | Step 700: Loss = 2.0673\n",
            "Epoch 1 | Step 750: Loss = 2.0614\n",
            "Epoch 1 | Step 800: Loss = 2.0797\n",
            "Epoch 1 | Step 850: Loss = 2.0548\n",
            "Epoch 1 | Step 900: Loss = 2.0832\n",
            "Epoch 1 | Step 950: Loss = 2.0958\n",
            "Epoch 1 | Step 1000: Loss = 1.9945\n",
            "Epoch 1 | Step 1050: Loss = 1.9850\n",
            "Epoch 1 | Step 1100: Loss = 2.0129\n",
            "Epoch 1 | Step 1150: Loss = 2.0050\n",
            "Epoch 1 | Step 1200: Loss = 2.0650\n",
            "Epoch 1 | Step 1250: Loss = 2.0764\n",
            "Epoch 1 | Step 1300: Loss = 1.9490\n",
            "Epoch 1 | Step 1350: Loss = 1.9356\n",
            "Epoch 1 | Step 1400: Loss = 2.0025\n",
            "Epoch 1 | Step 1450: Loss = 1.9710\n",
            "Epoch 1 | Step 1500: Loss = 1.9564\n",
            "Epoch 1 | Step 1550: Loss = 1.9169\n",
            "Epoch 1 | Step 1600: Loss = 1.9757\n",
            "Epoch 1 | Step 1650: Loss = 1.8262\n",
            "Epoch 1 | Step 1700: Loss = 1.8315\n",
            "Epoch 1 | Step 1750: Loss = 1.9680\n",
            "Epoch 1 | Step 1800: Loss = 1.9574\n",
            "Epoch 1 | Step 1850: Loss = 1.9188\n",
            "Epoch 1 | Step 1900: Loss = 1.8407\n",
            "Epoch 1 | Step 1950: Loss = 1.9151\n",
            "Epoch 1 | Step 2000: Loss = 1.8987\n",
            "Epoch 1 | Step 2050: Loss = 1.9297\n",
            "Epoch 1 | Step 2100: Loss = 1.9322\n",
            "Epoch 1 | Step 2150: Loss = 1.8331\n",
            "Epoch 1 | Step 2200: Loss = 1.9191\n",
            "Epoch 1 | Step 2250: Loss = 1.8370\n",
            "Epoch 1 | Step 2300: Loss = 1.8203\n",
            "Epoch 1 | Step 2350: Loss = 1.7870\n",
            "Epoch 1 | Step 2400: Loss = 1.7958\n",
            "Epoch 1 | Step 2450: Loss = 1.8820\n",
            "Epoch 2 | Step 0: Loss = 1.8766\n",
            "Epoch 2 | Step 50: Loss = 1.8046\n",
            "Epoch 2 | Step 100: Loss = 1.8249\n",
            "Epoch 2 | Step 150: Loss = 1.7736\n",
            "Epoch 2 | Step 200: Loss = 1.8114\n",
            "Epoch 2 | Step 250: Loss = 1.8864\n",
            "Epoch 2 | Step 300: Loss = 1.8303\n",
            "Epoch 2 | Step 350: Loss = 1.8317\n",
            "Epoch 2 | Step 400: Loss = 1.7862\n",
            "Epoch 2 | Step 450: Loss = 1.8332\n",
            "Epoch 2 | Step 500: Loss = 1.7526\n",
            "Epoch 2 | Step 550: Loss = 1.7583\n",
            "Epoch 2 | Step 600: Loss = 1.7758\n",
            "Epoch 2 | Step 650: Loss = 1.7540\n",
            "Epoch 2 | Step 700: Loss = 1.7550\n",
            "Epoch 2 | Step 750: Loss = 1.8206\n",
            "Epoch 2 | Step 800: Loss = 1.7778\n",
            "Epoch 2 | Step 850: Loss = 1.7624\n",
            "Epoch 2 | Step 900: Loss = 1.7321\n",
            "Epoch 2 | Step 950: Loss = 1.7104\n",
            "Epoch 2 | Step 1000: Loss = 1.7525\n",
            "Epoch 2 | Step 1050: Loss = 1.7479\n",
            "Epoch 2 | Step 1100: Loss = 1.7452\n",
            "Epoch 2 | Step 1150: Loss = 1.7145\n",
            "Epoch 2 | Step 1200: Loss = 1.6854\n",
            "Epoch 2 | Step 1250: Loss = 1.5981\n",
            "Epoch 2 | Step 1300: Loss = 1.6653\n",
            "Epoch 2 | Step 1350: Loss = 1.7725\n",
            "Epoch 2 | Step 1400: Loss = 1.8653\n",
            "Epoch 2 | Step 1450: Loss = 1.7232\n",
            "Epoch 2 | Step 1500: Loss = 1.6927\n",
            "Epoch 2 | Step 1550: Loss = 1.6957\n",
            "Epoch 2 | Step 1600: Loss = 1.6580\n",
            "Epoch 2 | Step 1650: Loss = 1.7515\n",
            "Epoch 2 | Step 1700: Loss = 1.6970\n",
            "Epoch 2 | Step 1750: Loss = 1.6476\n",
            "Epoch 2 | Step 1800: Loss = 1.7080\n",
            "Epoch 2 | Step 1850: Loss = 1.6841\n",
            "Epoch 2 | Step 1900: Loss = 1.6332\n",
            "Epoch 2 | Step 1950: Loss = 1.5960\n",
            "Epoch 2 | Step 2000: Loss = 1.6281\n",
            "Epoch 2 | Step 2050: Loss = 1.6523\n",
            "Epoch 2 | Step 2100: Loss = 1.5631\n",
            "Epoch 2 | Step 2150: Loss = 1.6251\n",
            "Epoch 2 | Step 2200: Loss = 1.6476\n",
            "Epoch 2 | Step 2250: Loss = 1.6492\n",
            "Epoch 2 | Step 2300: Loss = 1.6705\n",
            "Epoch 2 | Step 2350: Loss = 1.6788\n",
            "Epoch 2 | Step 2400: Loss = 1.6389\n",
            "Epoch 2 | Step 2450: Loss = 1.6363\n",
            "Epoch 3 | Step 0: Loss = 1.6323\n",
            "Epoch 3 | Step 50: Loss = 1.6161\n",
            "Epoch 3 | Step 100: Loss = 1.6193\n",
            "Epoch 3 | Step 150: Loss = 1.7202\n",
            "Epoch 3 | Step 200: Loss = 1.6219\n",
            "Epoch 3 | Step 250: Loss = 1.6091\n",
            "Epoch 3 | Step 300: Loss = 1.6352\n",
            "Epoch 3 | Step 350: Loss = 1.5818\n",
            "Epoch 3 | Step 400: Loss = 1.5945\n",
            "Epoch 3 | Step 450: Loss = 1.5851\n",
            "Epoch 3 | Step 500: Loss = 1.6456\n",
            "Epoch 3 | Step 550: Loss = 1.6421\n",
            "Epoch 3 | Step 600: Loss = 1.5905\n",
            "Epoch 3 | Step 650: Loss = 1.5596\n",
            "Epoch 3 | Step 700: Loss = 1.6263\n",
            "Epoch 3 | Step 750: Loss = 1.6017\n",
            "Epoch 3 | Step 800: Loss = 1.6019\n",
            "Epoch 3 | Step 850: Loss = 1.5804\n",
            "Epoch 3 | Step 900: Loss = 1.5634\n",
            "Epoch 3 | Step 950: Loss = 1.5879\n",
            "Epoch 3 | Step 1000: Loss = 1.6487\n",
            "Epoch 3 | Step 1050: Loss = 1.5872\n",
            "Epoch 3 | Step 1100: Loss = 1.6730\n",
            "Epoch 3 | Step 1150: Loss = 1.5568\n",
            "Epoch 3 | Step 1200: Loss = 1.6656\n",
            "Epoch 3 | Step 1250: Loss = 1.5221\n",
            "Epoch 3 | Step 1300: Loss = 1.6784\n",
            "Epoch 3 | Step 1350: Loss = 1.5440\n",
            "Epoch 3 | Step 1400: Loss = 1.5466\n",
            "Epoch 3 | Step 1450: Loss = 1.4731\n",
            "Epoch 3 | Step 1500: Loss = 1.5052\n",
            "Epoch 3 | Step 1550: Loss = 1.4984\n",
            "Epoch 3 | Step 1600: Loss = 1.5790\n",
            "Epoch 3 | Step 1650: Loss = 1.5823\n",
            "Epoch 3 | Step 1700: Loss = 1.5589\n",
            "Epoch 3 | Step 1750: Loss = 1.5628\n",
            "Epoch 3 | Step 1800: Loss = 1.4838\n",
            "Epoch 3 | Step 1850: Loss = 1.4616\n",
            "Epoch 3 | Step 1900: Loss = 1.5284\n",
            "Epoch 3 | Step 1950: Loss = 1.4299\n",
            "Epoch 3 | Step 2000: Loss = 1.5134\n",
            "Epoch 3 | Step 2050: Loss = 1.5758\n",
            "Epoch 3 | Step 2100: Loss = 1.5665\n",
            "Epoch 3 | Step 2150: Loss = 1.5367\n",
            "Epoch 3 | Step 2200: Loss = 1.5030\n",
            "Epoch 3 | Step 2250: Loss = 1.6100\n",
            "Epoch 3 | Step 2300: Loss = 1.5439\n",
            "Epoch 3 | Step 2350: Loss = 1.6222\n",
            "Epoch 3 | Step 2400: Loss = 1.4915\n",
            "Epoch 3 | Step 2450: Loss = 1.5915\n",
            "Epoch 4 | Step 0: Loss = 1.6176\n",
            "Epoch 4 | Step 50: Loss = 1.4327\n",
            "Epoch 4 | Step 100: Loss = 1.4572\n",
            "Epoch 4 | Step 150: Loss = 1.5450\n",
            "Epoch 4 | Step 200: Loss = 1.5040\n",
            "Epoch 4 | Step 250: Loss = 1.5727\n",
            "Epoch 4 | Step 300: Loss = 1.4203\n",
            "Epoch 4 | Step 350: Loss = 1.2926\n",
            "Epoch 4 | Step 400: Loss = 1.4118\n",
            "Epoch 4 | Step 450: Loss = 1.4991\n",
            "Epoch 4 | Step 500: Loss = 1.4835\n",
            "Epoch 4 | Step 550: Loss = 1.4249\n",
            "Epoch 4 | Step 600: Loss = 1.5032\n",
            "Epoch 4 | Step 650: Loss = 1.4705\n",
            "Epoch 4 | Step 700: Loss = 1.4954\n",
            "Epoch 4 | Step 750: Loss = 1.4996\n",
            "Epoch 4 | Step 800: Loss = 1.5023\n",
            "Epoch 4 | Step 850: Loss = 1.5261\n",
            "Epoch 4 | Step 900: Loss = 1.5364\n",
            "Epoch 4 | Step 950: Loss = 1.5284\n",
            "Epoch 4 | Step 1000: Loss = 1.4586\n",
            "Epoch 4 | Step 1050: Loss = 1.4900\n",
            "Epoch 4 | Step 1100: Loss = 1.4768\n",
            "Epoch 4 | Step 1150: Loss = 1.5087\n",
            "Epoch 4 | Step 1200: Loss = 1.4822\n",
            "Epoch 4 | Step 1250: Loss = 1.4744\n",
            "Epoch 4 | Step 1300: Loss = 1.4265\n",
            "Epoch 4 | Step 1350: Loss = 1.4682\n",
            "Epoch 4 | Step 1400: Loss = 1.3713\n",
            "Epoch 4 | Step 1450: Loss = 1.5088\n",
            "Epoch 4 | Step 1500: Loss = 1.4494\n",
            "Epoch 4 | Step 1550: Loss = 1.4067\n",
            "Epoch 4 | Step 1600: Loss = 1.4704\n",
            "Epoch 4 | Step 1650: Loss = 1.4444\n",
            "Epoch 4 | Step 1700: Loss = 1.5022\n",
            "Epoch 4 | Step 1750: Loss = 1.4448\n",
            "Epoch 4 | Step 1800: Loss = 1.3729\n",
            "Epoch 4 | Step 1850: Loss = 1.5232\n",
            "Epoch 4 | Step 1900: Loss = 1.4392\n",
            "Epoch 4 | Step 1950: Loss = 1.4102\n",
            "Epoch 4 | Step 2000: Loss = 1.4180\n",
            "Epoch 4 | Step 2050: Loss = 1.4922\n",
            "Epoch 4 | Step 2100: Loss = 1.3959\n",
            "Epoch 4 | Step 2150: Loss = 1.4112\n",
            "Epoch 4 | Step 2200: Loss = 1.3998\n",
            "Epoch 4 | Step 2250: Loss = 1.4588\n",
            "Epoch 4 | Step 2300: Loss = 1.3875\n",
            "Epoch 4 | Step 2350: Loss = 1.4616\n",
            "Epoch 4 | Step 2400: Loss = 1.4484\n",
            "Epoch 4 | Step 2450: Loss = 1.4523\n",
            "✓ Training completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode (disables dropout)\n",
        "model.eval()\n",
        "\n",
        "def translate_sentence(sentence, model, max_length=100):\n",
        "    \"\"\"\n",
        "    Translates an English sentence to German using the trained T5Model.\n",
        "    Uses greedy decoding.\n",
        "    \"\"\"\n",
        "    # 1. Prepare Source (English)\n",
        "    # Encode and add batch dimension [1, seq_len]\n",
        "    src_ids = encode(sentence)\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # 2. Encoder Pass\n",
        "    # We encode the source sequence once\n",
        "    with torch.no_grad():\n",
        "        encoder_output = model.encoder(src_tensor)\n",
        "\n",
        "    # 3. Decoder Loop (Autoregressive)\n",
        "    # Start with the Start-Of-Sequence token\n",
        "    decoder_input = torch.tensor([[SOS_IDX]], dtype=torch.long).to(device)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            # Pass current sequence and encoder output to decoder\n",
        "            # decoder_input shape: [1, current_seq_len]\n",
        "            decoder_output = model.decoder(decoder_input, encoder_output)\n",
        "\n",
        "            # Project to vocabulary\n",
        "            logits = model.lm_head(decoder_output)\n",
        "\n",
        "            # Get the token with highest probability for the last position\n",
        "            # logits shape: [1, current_seq_len, vocab_size]\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n",
        "\n",
        "        # Stop if End-Of-Sequence token is generated\n",
        "        if next_token_id == EOS_IDX:\n",
        "            break\n",
        "\n",
        "        generated_tokens.append(next_token_id)\n",
        "\n",
        "        # Append prediction to decoder input for next step\n",
        "        next_token_tensor = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
        "        decoder_input = torch.cat([decoder_input, next_token_tensor], dim=1)\n",
        "\n",
        "    # 4. Decode to string\n",
        "    translated_text = decode(generated_tokens)\n",
        "    return translated_text\n",
        "\n",
        "# --- Test Translations ---\n",
        "print(\"--- Translation Demo ---\")\n",
        "test_sentences = [\n",
        "    \"Hello\",\n",
        "    \"Good morning\",\n",
        "    \"I love you\",\n",
        "    \"Where is the bathroom?\",\n",
        "    \"This is difficult\"\n",
        "]\n",
        "\n",
        "for s in test_sentences:\n",
        "    trans = translate_sentence(s, model)\n",
        "    print(f\"EN: {s:25} -> DE: {trans}\")\n",
        "\n",
        "# Try a custom one\n",
        "custom = \"I am hungry\"\n",
        "print(f\"\\nCustom: {custom} -> {translate_sentence(custom, model)}\")"
      ],
      "metadata": {
        "id": "ykI_wrHXMtd1",
        "outputId": "a22e09cd-8725-4472-b831-cc818dbf3873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ykI_wrHXMtd1",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Translation Demo ---\n",
            "EN: Hello                     -> DE: Wie sollten der Bereichen zu erwährleichen Bericht eine lobalen letzten  ollen Bereichen zu erwältig\n",
            "EN: Good morning              -> DE: Bereich Bericht in der Frage in der Frage der Frage angenen  erreichen Berichterstützung von Fragen \n",
            "EN: I love you                -> DE: Ich möchte  möchte   eine  bestehen, wenn  eine  bestehen, wenn  ich möchte   eine   eine  bestehen,\n",
            "EN: Where is the bathroom?    -> DE: Wir sollten der  eine  sind so hat eine  solchen  in einem  auch eine  solchen  eine  sich nicht nic\n",
            "EN: This is difficult         -> DE: Diese Statschaft die Statelinie der Statschaftsprechen der Staten der Staten der Staten der Staten d\n",
            "\n",
            "Custom: I am hungry -> Ich möchte mir gegen Sie eine seine Sitzung nach eine  sich nicht nicht nicht nicht nicht nicht nich\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS (2 points): Solve any task with an LLM\n",
        "\n",
        "**Goal.**  \n",
        "Pick **one** of the homework tasks (Task 1, Task 2, or Task 3) and solve it using an **LLM**. Provide the **LLM** with the **task description** and any **starter/prerequisite code** it depends on, ask it to generate a complete **code solution** first, then run that **generated code** here in the code notebook yourself. Finally, document what you did and **compare** the LLM’s result to your own pipeline.\n",
        "\n",
        "**What to deliver below.**\n",
        "1) **LLM used** (name + version, e.g., “Llama-3-8B-Instruct”, “GPT-x”, “Claude-x”, “Mistral-x”, etc.).  \n",
        "2) **Prompt(s)** you used.  \n",
        "3) **LLM output** — copy and paste the generated code.  \n",
        "4) **Comparison** to your solution: what matches or differs (quantitative or qualitative).  \n",
        "5) **Reflection**: what the LLM was **good at** vs **bad at**, what it got **right** vs **wrong**.\n",
        "\n",
        "> **No code required.** You do **not** need to run, share, or submit any code used for the LLM generation. Provide only the deliverables listed above.\n",
        "> You may use any LLMs through any interface (API, web UI, local inference).\n"
      ],
      "metadata": {
        "id": "w8rDo9crziNZ"
      },
      "id": "w8rDo9crziNZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LLM used:\n",
        "1. Google Gemini (“Flash 2.5”)"
      ],
      "metadata": {
        "id": "tph74FOlm6rX"
      },
      "id": "tph74FOlm6rX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt\n",
        "\n",
        "```\n",
        "You are helping me implement TASK 3.1 of a homework: Build a complete T5 model for machine translation.\n",
        "Combine encoder and decoder into a complete T5 model and train it on English-German machine translation.\n",
        "\n",
        "3.1: Complete T5 Model — Combine encoder and decoder stacks with shared token embeddings and language modeling head\n",
        "3.2: Translation Dataset Preparation — Prepare English-German translation dataset with proper batching and padding\n",
        "3.3: Training and Testing — Train model on translation task and evaluate performance\n",
        "\n",
        "Please do this:\n",
        "TASK 3.1: Complete T5 Model — Combine encoder and decoder stacks with shared token embeddings and language modeling head.\n",
        "\n",
        "Constraints:\n",
        "- Use PyTorch.\n",
        "- I already implemented T5Encoder and T5Decoder classes that return hidden states of shape:\n",
        "  encoder(src) -> (B, T_src, d_model)\n",
        "  decoder(tgt, encoder_output) -> (B, T_tgt, d_model)\n",
        "- I have PAD_IDX defined globally.\n",
        "- I want shared token embeddings between encoder and decoder.\n",
        "- I want a language modeling head that maps decoder hidden states to vocab logits.\n",
        "- Please implement class T5Model(nn.Module) with __init__ and forward(self, src, tgt) returning logits of shape (B, T_tgt, vocab_size).\n",
        "- Use weight tying: lm_head weight tied to shared embedding.\n",
        "\n",
        "Starter signature to fill:\n",
        "class T5Model(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, block_size, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # TODO\n",
        "        return logits\n",
        "```"
      ],
      "metadata": {
        "id": "nnCmB10snEI7"
      },
      "id": "nnCmB10snEI7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google Gemini Output / LLM Output:\n",
        "```\n",
        "This is a great task for building a complete T5-style model\\! The core idea is to share the input token embeddings between the encoder and the decoder and then use the same weight matrix for the final linear layer (the language modeling head) that maps the decoder's output to the vocabulary logits (a technique called **weight tying**).\n",
        "\n",
        "Here is the implementation of the `T5Model` class in PyTorch, assuming you have already defined `T5Encoder` and `T5Decoder` that handle the token embedding and positional encoding *internally* based on a shared embedding layer you pass them.\n",
        "\n",
        "### 💻 T5Model Implementation\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Assuming you have these classes defined elsewhere\n",
        "# from your_module import T5Encoder, T5Decoder\n",
        "\n",
        "# Mock T5Encoder and T5Decoder for self-contained example\n",
        "# Replace these with your actual implementations\n",
        "class T5Encoder(nn.Module):\n",
        "    def __init__(self, embedding_layer, d_model, n_heads, n_layers, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        # In a real implementation, this would include the encoder stack (layers)\n",
        "        self.embedding = embedding_layer\n",
        "        self.d_model = d_model\n",
        "    \n",
        "    def forward(self, src):\n",
        "        # Apply embedding and get hidden states\n",
        "        # The embedding layer is assumed to handle the token-to-vector conversion\n",
        "        B, T_src = src.shape\n",
        "        # Mock encoder logic: simple lookup and a dummy projection\n",
        "        embedded = self.embedding(src)\n",
        "        # Assuming the embedding layer returns (B, T_src, d_model)\n",
        "        return embedded\n",
        "\n",
        "class T5Decoder(nn.Module):\n",
        "    def __init__(self, embedding_layer, d_model, n_heads, n_layers, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        # In a real implementation, this would include the decoder stack (layers)\n",
        "        self.embedding = embedding_layer\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, tgt, encoder_output):\n",
        "        # Apply embedding and get hidden states\n",
        "        # The embedding layer is assumed to handle the token-to-vector conversion\n",
        "        B, T_tgt = tgt.shape\n",
        "        # Mock decoder logic: simple lookup and a dummy projection\n",
        "        embedded = self.embedding(tgt)\n",
        "        # Assuming the embedding layer returns (B, T_tgt, d_model)\n",
        "        # In the real decoder, it would use encoder_output for cross-attention\n",
        "        return embedded\n",
        "\n",
        "\n",
        "# --- Your requested implementation starts here ---\n",
        "\n",
        "class T5Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete T5 Model combining encoder and decoder with shared token embeddings\n",
        "    and a weight-tied language modeling head.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, block_size, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 1. Shared Token Embeddings\n",
        "        # A T5-style model uses the same embedding matrix for both input and output.\n",
        "        # This layer maps tokens (indices) to vectors (d_model).\n",
        "        self.shared_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # 2. Encoder and Decoder Stacks\n",
        "        # Pass the shared embedding layer to both the encoder and decoder.\n",
        "        # Your T5Encoder/T5Decoder classes should handle the rest (Positional Encoding,\n",
        "        # Transformer Blocks, Layer Norms, etc.) internally.\n",
        "        self.encoder = T5Encoder(\n",
        "            embedding_layer=self.shared_embedding,\n",
        "            d_model=d_model,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            d_ff=d_ff,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.decoder = T5Decoder(\n",
        "            embedding_layer=self.shared_embedding,\n",
        "            d_model=d_model,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            d_ff=d_ff,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # 3. Language Modeling Head (Output Layer)\n",
        "        # This is a simple linear layer that maps the decoder's hidden states\n",
        "        # (B, T_tgt, d_model) back to vocabulary logits (B, T_tgt, vocab_size).\n",
        "        # We initialize it as a standard Linear layer first.\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        \n",
        "        # 4. Weight Tying\n",
        "        # The weight matrix of the output linear layer (`lm_head.weight`) is\n",
        "        # tied to the weight matrix of the shared token embedding layer\n",
        "        # (`shared_embedding.weight`).\n",
        "        self.lm_head.weight = self.shared_embedding.weight\n",
        "        \n",
        "        # Initialize parameters (often done with a specific scheme like Xavier/Kaiming)\n",
        "        # For simplicity, we just initialize the lm_head for demonstration.\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # A common practice in T5 is to scale the embeddings/outputs\n",
        "        nn.init.normal_(self.shared_embedding.weight, mean=0.0, std=0.02)\n",
        "        # No need to initialize lm_head.weight since it's tied to shared_embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Processes source and target sequences through the T5 model.\n",
        "\n",
        "        Args:\n",
        "            src (Tensor): Source sequence (e.g., English), shape (B, T_src).\n",
        "            tgt (Tensor): Target sequence (e.g., German, shifted right), shape (B, T_tgt).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Logits over the vocabulary for the next target token, shape (B, T_tgt, vocab_size).\n",
        "        \"\"\"\n",
        "        \n",
        "        # 1. Encoder Forward Pass\n",
        "        # Maps the source sequence to contextualized hidden states.\n",
        "        # encoder_output shape: (B, T_src, d_model)\n",
        "        encoder_output = self.encoder(src)\n",
        "        \n",
        "        # 2. Decoder Forward Pass\n",
        "        # Processes the target sequence and attends to the encoder output.\n",
        "        # decoder_output shape: (B, T_tgt, d_model)\n",
        "        decoder_output = self.decoder(tgt, encoder_output)\n",
        "        \n",
        "        # 3. Language Modeling Head\n",
        "        # Maps the decoder's hidden states to logits over the vocabulary.\n",
        "        # logits shape: (B, T_tgt, vocab_size)\n",
        "        logits = self.lm_head(decoder_output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# --- End of implementation ---\n",
        "```\n",
        "\n",
        "### 💡 Key Design Points Explained\n",
        "\n",
        "1.  **Shared Token Embedding (`self.shared_embedding`)**:\n",
        "\n",
        "      * This single `nn.Embedding` layer is created once.\n",
        "      * It is passed into the constructors of both `self.encoder` and `self.decoder`. This ensures both components use the **identical weight matrix** for looking up token representations, which is a standard T5/Transformer design feature.\n",
        "\n",
        "2.  **Encoder-Decoder Flow**:\n",
        "\n",
        "      * The `forward` method first computes `encoder_output = self.encoder(src)`.\n",
        "      * This output is then passed to the decoder as the context for cross-attention: `decoder_output = self.decoder(tgt, encoder_output)`.\n",
        "\n",
        "3.  **Language Modeling Head (`self.lm_head`)**:\n",
        "\n",
        "      * This is the final linear layer that converts the high-dimensional decoder output back to a vector of scores (logits) for every word in the vocabulary.\n",
        "\n",
        "4.  **Weight Tying (The Crucial Step)**:\n",
        "\n",
        "      * The line `self.lm_head.weight = self.shared_embedding.weight` is what ties the weights.\n",
        "      * The weight matrix used for *decoding* (mapping hidden state to vocabulary scores) is set to be the **exact same matrix** used for *encoding* (mapping input tokens to hidden states). This is an effective form of regularization that reduces the number of parameters and often improves performance.\n",
        "\n",
        "The structure of the T5 model you're building is a classic **Encoder-Decoder Transformer** architecture.\n",
        "\n",
        "Would you like me to proceed with **TASK 3.2: Translation Dataset Preparation** using PyTorch and the common libraries like HuggingFace's `datasets`?\n",
        "```"
      ],
      "metadata": {
        "id": "DcfaQvVMo_pM"
      },
      "id": "DcfaQvVMo_pM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparision:\n",
        "\n",
        "**What matches (same idea):**\n",
        "\n",
        "1. Shared embeddings: Both approaches use one embedding matrix for encoder + decoder.\n",
        "2. LM head: Both map decoder hidden states (B, T_tgt, d_model) to logits (B, T_tgt, vocab_size).\n",
        "3. Weight tying: Both tie lm_head.weight to the shared embedding weights.\n",
        "4. Forward flow: Both do encoder(src) → decoder(tgt, encoder_output) → lm_head\n",
        "\n",
        "**Differences:**\n",
        "1. Gemini did not use PAD_IDX to build a padding mask. In my pipeline, I have  pass a padding mask into the encoder so attention ignores <pad> tokens.\n",
        "2. Gemini includes explicit initialization, My pipeline relies on PyTorch default init.\n",
        "3. Gemini output includes mock encoder/decoder classes (demo code) in the response.\n",
        "4. Gemini shares embeddings by passing them in, I shared embeddings by replacing the embedding layer after construction."
      ],
      "metadata": {
        "id": "WsL95YN4qQq4"
      },
      "id": "WsL95YN4qQq4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection:\n",
        "**Good at:**\n",
        "1. Got the overall architecture right.\n",
        "2. Remembered weight tying and implemented it correctly.\n",
        "3. Produced readable, structured code.\n",
        "\n",
        "\n",
        "**Bad at / got wrong:**\n",
        "1. Assumed a different T5Encoder/T5Decoder API than the notebook’s actual implementation.\n",
        "2. Included extra mock code and explanations.\n",
        "3. Did not use PAD_IDX masking, which matters for correct training with padded batches."
      ],
      "metadata": {
        "id": "JA4OAOGlqRAc"
      },
      "id": "JA4OAOGlqRAc"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgjqVReHzpb0"
      },
      "id": "BgjqVReHzpb0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "_5jqzZYb9a9J",
        "Y4X0a4sr9a9L",
        "ZfPZQJP_9a9O",
        "dnhEc-0O9a9S"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}